## 1984

Hinton, G. (1984, January 01). Boltzmann machines: Constraint satisfaction networks that learn. Retrieved from https://www.cs.utoronto.ca/~hinton/absps/bmtr.pdf

    - We present a parallel constrain satisfactory network called “Boltzmann Machine”
    - We describe a method based on statistical mechanics to learn from examples
    - It creates internal representations

## 1985

Ackley, D. H., Hinton, G. E., & Sejnowski, T. J. (1985). A learning algorithm for boltzmann machines. Cognitive Science, 9(1), 147–169. Retrieved from https://www.cs.toronto.edu/~hinton/absps/cogscibm.pdf

    - We present a Boltzmann Machine
    - The machine is composed of 0-1 units connected by bidirectional links that can take on real values
    - We describe a learning method to create internal representations based on statistical mechanics
    - When shown a partial example, the network can complete it: the system will then find the minimum energy configuration that is compatible with that input
    - However, the current version of the learning algorithm is very slow

## 1986

Hinton, G. E., & Sejnowski, T. J. (1986). Learning and relearning in Boltzmann machines. Parallel Distributed Processing, 1. Retrieved from https://www.cs.toronto.edu/~hinton/absps/pdp7.pdf

    - We need to find a way for a Boltzmann Machine to escape from local minima during a relaxation search
    - We found that this can be done by using a stochastic decision rule
    - An information needed to do credit assignment is propagated, and the network reaches thermal equilibrium
    - The network constructs distributed representations which are resistant to minor damage
    - They exhibit rapid relearning after major damage

Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986). Learning representations by back-propagating errors. Nature, 323, 533–536. Retrieved from https://www.iro.umontreal.ca/~vincentp/ift3395/lectures/backprop_old.pdf

    - We describe a back-propagation learning procedure for neural networks
    - Internal hidden units are able to learn important features of the task domain

## 1992

Neal, R. M. (1992). Connectionist Learning of Belief Networks. Artif. Intell. Retrieved from https://www.semanticscholar.org/paper/Connectionist-Learning-of-Belief-Networks-Neal/a120c05ad7cd4ce2eb8fb9697e16c7c4877208a5

    - Belief networks usually represent knowledge derived from experts
    - We describe how to learn "sigmoid" and "noisy-OR" belief networks using Gibbs sampling
    - Except for the lack of a negative phase, learning is similar to that in a Boltzmann machine
    - These metworks are naturally applicable to classification or unsupervised learning problems
    - They provide a link between connectionist learning and expert knowledge

## 1993

Hinton, G. E., & Zemel, R. (1993). Autoencoders, Minimum Description Length and Helmholtz Free Energy. Advances in Neural Information Processing Systems, 6. Retrieved from https://proceedings.neurips.cc/paper/1993/hash/9e3cfc48eccf81a0d57663e129aef3cb-Abstract.html

    - We derive an objective for training autoencoders based on the Minimum Description Length principle (MDL)
    - We aim to minimize the information required to describe both the code vector and the reconstruction error
    - This is minimized by choosing code vectors stochastically according to a Boltzmann distribution
    - The recognition weights approximate the Boltzmann distribution giving an upper bound on MDL
    - The generative weights define the energy of each possible code vector given the input vector

## 1994

Bengio, Y., Simard, P., & Frasconi, P. (1994). Learning long-term dependencies with gradient descent is difficult. IEEE Trans. Neural Networks, 5(2), 157–166. Retrieved from https://www.comp.hkbu.edu.hk/~markus/teaching/comp7650/tnn-94-gradient.pdf

    - It is experimentally known that RNNs poorly learn long-term dependencies (we also show this)
    - Theoretic result: either a dynamic system is resistant to noise, or efficiently trainable by GD
    - The gradient exponentially vanishes or explodes in RNNs
    - We propose alternatives to standard GD (time-weighted pseudo-Newton algorithm)

Zemel, R. (1994). A minimum description length framework for unsupervised learning. Retrieved from https://www.semanticscholar.org/paper/A-minimum-description-length-framework-for-learning-Zemel/b7b2bffdf5b62305bec4c0f1ea7e3c1ba66fccb5

    - A PhD thesis of R. Zemel
    - We describe unsupervised learning based on the Minimum Description Length (MDL) principle
    - It says to minimize the summed description length of the model and the data with respect to the model
    - It makes explicit a tradeoff between the accuracy of a representation and the succinctness
    - We derive objectives for self-supervised NNs according to MDL

## 1995

Bishop, C. M. (1995). Training with Noise is Equivalent to Tikhonov Regularization. Neural Comput., 7(1), 108–116. Retrieved from https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/bishop-tikhonov-nc-95.pdf

Dayan, P., Hinton, G. E., Neal, R. M., & Zemel, R. S. (1995). The Helmholtz Machine. Retrieved from https://www.cs.toronto.edu/~hinton/absps/helmholtz.pdf

    - A problem: For most of generative models, each pattern can be generated in exponentially many ways, so it is intractable to adjust the parameters to maximize the probability of the observed patterns
    - We overcome this by maximizing an easily computed lower bound on the probability of the observations
    - A Helmholtz Machine consists of multiple layers of binary stochastic units connected by two sets of weights
    - Top-down connections is a generative model, bottom-up connections is a recognition model

Hinton, G. E., Dayan, P., Frey, B., & Neal, R. (1995). The "wake-sleep" algorithm for unsupervised neural networks. Science. Retrieved from https://www.cs.toronto.edu/~hinton/absps/ws.pdf

    - We describe an unsupervised learning algorithm to fit a multilayer neural generative model
    - In the “wake” phase, neurons are driven by recognition connections and generative connections are adapted
    - In the “sleep” phase, neurons are driven by generative connections and recognition connections are adapted

Zemel, R. S., & Hinton, G. E. (1995). Learning population codes by minimizing description length. Neural Comput., 7(3), 549–564. Retrieved from https://dl.acm.org/doi/10.1162/neco.1995.7.3.549

    - We show how the minimum description length principle can be used to develop redundant population codes

## 1996

Saul, L. K., Jaakkola, T., & Jordan, M. I. (1996). Mean Field Theory for Sigmoid Belief Networks. arXiv, cs/9603102. Retrieved from https://arxiv.org/abs/cs/9603102v1

    - Problem: in large belief networks computing likelihoods is intractable
    - We provide a tractable approximation to the true probability distribution in sigmoid belief networks
    - We demonstrate the utility of this framework on handwritten digit recognition

Williams, C. (1996). Computing with Infinite Networks. Advances in Neural Information Processing Systems, 9. Retrieved from https://papers.nips.cc/paper_files/paper/1996/hash/ae5e3ce40e0404a45ecacaaf05e5f735-Abstract.html

## 1997

Bell, A. J., & Sejnowski, T. J. (1997). The “independent components” of natural scenes are edge filters. Vision Res., 37(23), 3327–3338. Retrieved from https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2882863/pdf/nihms-197142.pdf

    - We propose the infomax network: a new unsupervised algorithm based on information maximization
    - Being trained on images, it produces sets of visual filters that are localized and oriented
    - The outputs of these filters are as independent as possible, since an algorithm performs ICA
    - Compared to PCA and ZCA, the ICA filters have more sparsely distributed outputs on natural scenes

Hinton, G. E., & Ghahramani, Z. (1997). Generative models for discovering sparse distributed representations. Philos. Trans. R. Soc. London, Ser. B. Retrieved from https://www.cs.toronto.edu/~hinton/absps/rgbn.pdf

    - We propose the Rectied Gaussian Belief Net: a hierarchical generative neural network
    - It uses bottom-up, top-down and lateral connections
    - It seems to work much better than the wake-sleep algorithm
    - It is very effective at discovering hierarchical sparse distributed representations
    - Its main disadvantage is that the recognition process involves Gibbs sampling

Hochreiter, S., & Schmidhuber, J. (1997). Flat Minima. Neural Comput., 9(1), 1–42. Retrieved from https://www.bioinf.jku.at/publications/older/3304.pdf

    - Bayesian argument suggests that flat minima correspond to "simple" networks and low expected overfitting
    - Our algorithm requires the computation of second order derivatives
    - But it has backprop's order of complexity
    - In stock market prediction, it outperforms backprop, weight decay and optimal brain surgeon

Rao, R. P. N., & Ballard, D. (1997). Dynamic Model of Visual Recognition Predicts Neural Response Properties in the Visual Cortex. Neural Comput. Retrieved from https://www.semanticscholar.org/paper/Dynamic-Model-of-Visual-Recognition-Predicts-Neural-Rao-Ballard/e3a83c2ed3af29a23ab342212d1ae9650a0c64a1

    -  We describe a hierarchical network model of visual recognition
    -  It dynamically combines input-driven bottom-up signals with expectation-driven top-down signals

## 1998

Prechelt, L. (1998). Early Stopping — But When? Neural Networks: Tricks of the Trade. Springer. Retrieved from https://page.mi.fu-berlin.de/prechelt/Biblio/stop_tricks1997.pdf

    - We perform experiments with early stopping on 12 problems and 24 different network architectures 
    - We find that slower stopping criteria allow for small improvements in generalization (about 4% on average)
    - However, it costs much more training time (about factor 4 longer on average)

Lecun, Y., Bottou, L., Orr, G. B., & Muller, K.-r. (2000). Efficient BackProp. ResearchGate. Retrieved from https://cseweb.ucsd.edu/classes/wi08/cse253/Handouts/lecun-98b.pdf

    - Stochastic learning is faster than batch learning, often improves quality
    - Stochastic learning allow to track changes in online learning
    - However, many acceleration techniques (like conjugate gradient) only operate in batch learning
    - Shuffle the training set so that successive examples rarely belong to the same class
    - Focus hard examples (but not for data containing outliers)
    - Normalize the inputs and uncorrelate them if possible
    - Hyperbolic tangent often converge faster than sigmoid, also tanh(x) + ax may be good
    - Choose target values at the point of maximum second derivative of the sigmoid
    - Some recommendations for initialization are given (IMO see later works for this)
    - Some recommendations for separate learning rate for each weight
    - Radial basis functions (RBF) vs sigmoid units is discussed
    - SGD convergence and second-order methods are discussed in details
    - It is shown that classical second-order algoritms are impractical for NNs, modifications are proposed

Neal, R. M., & Hinton, G. E. (1998). A View of the EM Algorithm that Justifies Incremental, Sparse, and other Variants. Learning in Graphical Models. Springer. Retrieved from https://www.cs.toronto.edu/~hinton/absps/emk.pdf

    - We show that EM algorithm maximizes a function that resembles negative free energy
    - From this perspective, we can justify an incremental, sparse and other EM variants

Jordan, M. I., Ghahramani, Z., Jaakkola, T. S., & Saul, L. K. (1998). An Introduction to Variational Methods for Graphical Models. Learning in Graphical Models. Springer. Retrieved from https://people.eecs.berkeley.edu/~jordan/papers/variational-intro.pdf

## 1999

Rao, R. P. N., & Ballard, D. (1999). Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects. Nat. Neurosci. Retrieved from https://www.semanticscholar.org/paper/Predictive-coding-in-the-visual-cortex%3A-a-of-some-Rao-Ballard/a424ec3b8846f57b8ffdb566d272e28d5a525909

## 2000

Tishby, N., Pereira, F. C., & Bialek, W. (2000). The information bottleneck method. arXiv, physics/0004057. Retrieved from https://arxiv.org/abs/physics/0004057v1

    - To find which features of X play a role in the prediction, we may want to find a short code for X that preserves the maximum information about Y
    - We squeeze the information that X provides about Y through a "bottleneck" formed by a limited set of codewords
    - We derive equations and an iterative algorithm for finding representations of the signal that capture its relevant structure

## 2001

Blei, D. M., Ng, A. Y., & Jordan, M. I. (2001). Latent Dirichlet Allocation. Journal of Machine Learning Research, 3(Jan), 601–608. Retrieved from https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf

Ng, A., & Jordan, M. (2001). On Discriminative vs. Generative Classifiers: A comparison of logistic regression and naive Bayes. Advances in Neural Information Processing Systems, 14. Retrieved from https://papers.nips.cc/paper_files/paper/2001/hash/7b7a53e239400a13bd6be6c91c4f6c4e-Abstract.html

## 2002

Hinton, H. (2002) Training Products of Experts by Minimizing Contrastive Divergence. Neural Computation, 14, 1771-1800. - References - Scientific Research Publishing. (2024, March 10). Retrieved from https://www.cs.toronto.edu/~hinton/absps/tr00-004.pdf

## 2003

Bengio, Y., Paiement, J.-f., Vincent, P., Delalleau, O., Roux, N., & Ouimet, M. (2003). Out-of-Sample Extensions for LLE, Isomap, MDS, Eigenmaps, and Spectral Clustering. Advances in Neural Information Processing Systems, 16. Retrieved from https://papers.nips.cc/paper_files/paper/2003/hash/cf05968255451bdefe3c5bc64d550517-Abstract.html

    - We consider five types of unsupervised learning algorithms based on a spectral embedding
    - They are MDS, spectral clustering, Laplacian eigenmaps, Isomap and LLE
    - We present an extension for these methods
    - It allows one to apply a trained model to out-of-sample points without having to recompute eigenvectors
    - It introduces a notion of function induction and generalization error for these algorithms
    - We experiment on real high-dimensional data

Friston, K. J. (2003). Learning and inference in the brain. Neural Networks. Retrieved from https://www.semanticscholar.org/paper/Learning-and-inference-in-the-brain-Friston/0ea24ffe3bee9faeeead947c6c7ab00c99f5ccf2

Lee, T., & Mumford, D. (2003). Hierarchical Bayesian inference in the visual cortex. Journal of The Optical Society of America A-optics Image Science and Vision. Retrieved from https://www.semanticscholar.org/paper/Hierarchical-Bayesian-inference-in-the-visual-Lee-Mumford/5042cb5efad30f443adef472b8748e1b7bb0452f

Teh, Y. W., Welling, M., Osindero, S., & Hinton, G. E. (2004). Energy-based models for sparse overcomplete representations. Journal of Machine Learning Research, 4(7-8), 1235–1260. Retrieved from https://www.jmlr.org/papers/volume4/teh03a/teh03a.pdf

Welling, M., & Teh, Y. W. (2003). Approximate inference in Boltzmann machines. Artif. Intell., 143(1), 19–50. Retrieved from https://core.ac.uk/download/pdf/82455875.pdf

## 2004

Grandvalet, Y., & Bengio, Y. (2004). Semi-supervised Learning by Entropy Minimization. Advances in Neural Information Processing Systems, 17. Retrieved from https://papers.nips.cc/paper_files/paper/2004/hash/96f2b50b5d3613adf9c27049b2a888c7-Abstract.html

    - We present minimum entropy regularization for semi-supervised learning for any probabilistic classifier
    - Unlabeled examples are mostly beneficial when classes have small overlap
    - Experiments suggest that the minimum entropy regularization may be a serious contender to generative models

Knill, D., & Pouget, A. (2004). The Bayesian brain: the role of uncertainty in neural coding and computation. Trends Neurosci. Retrieved from https://www.semanticscholar.org/paper/The-Bayesian-brain%3A-the-role-of-uncertainty-in-and-Knill-Pouget/2a3146db2f3cb39ef37105d428eef964a253daf2

## 2005

Friston, K. J. (2005). A theory of cortical responses. Philosophical Transactions of the Royal Society B: Biological Sciences. Retrieved from https://www.semanticscholar.org/paper/A-theory-of-cortical-responses-Friston/8fcbc38e7196b0dc8748f04cd6101e71f92c158e

## 2006

Crammer, K., Dekel, O., Keshet, J., Shalev-Shwartz, S., & Singer, Y. (2006). Online Passive-Aggressive Algorithms. Journal of Machine Learning Research, 7(19), 551–585. Retrieved from https://jmlr.csail.mit.edu/papers/v7/crammer06a.html

Friston, K. J., Kilner, J., & Harrison, L. (2006). A free energy principle for the brain. J. Physiol.-Paris. Retrieved from https://www.semanticscholar.org/paper/A-free-energy-principle-for-the-brain-Friston-Kilner/641a9d87b9636d4ef2e353a569ddede68bd29131

Hinton, G. E., & Salakhutdinov, R. R. (2006). Reducing the Dimensionality of Data with Neural Networks. Science, 313(5786), 504–507. Retrieved from https://www.cs.toronto.edu/~hinton/absps/science.pdf

    - High-dimensional data can be converted to low-dimensional codes by training a multilayer NN autoencoder
    - Problem: this works well only if the initial weights are close to a good solution
    - We describe an effective way of initializing the weights
    - Such autoencoder learn codes that work much better than PCA as a tool to reduce the dimensionality of data

Hinton, G. E., Osindero, S., & Teh, Y.-W. (2006). A fast learning algorithm for deep belief nets. Neural Comput., 16764513. Retrieved from https://www.cs.toronto.edu/~hinton/absps/ncfast.pdf

    - Problem: the explaining-away effects make inference difficult in Deep Belief Networks (DBN)
    - We propose an algorithm to learn DBNs one layer at a time
    - The top two layers form an undirected associative memory
    - Our 3-layer network forms a good generative model of the joint distribution of MNIST
    - The low-dimensional data manifolds are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines 

## 2007

Bengio, Y., Lamblin, P., Popovici, D., Larochelle, H., & Montreal, U. (2007). Greedy layer-wise training of deep networks. Advances in Neural Information Processing Systems, 19. Retrieved from https://proceedings.neurips.cc/paper_files/paper/2006/file/5da713a690c067105aeb2fae32403405-Paper.pdf

    - Problem: gradient-based optimization of DNNs from random initialization often get stuck in poor solutions
    - We explore variants of Deep Belief Networks layer-wise learning algorithm to better understand its success
    - We found that layer-wise unsupervised training strategy helps the optimization by initializing weights in a region near a good local minimum, giving rise to internal distributed representations that are high-level abstractions

Bengio, Y., & LeCun, Y. (2007). Scaling learning algorithms towards AI. Retrieved from https://www.iro.umontreal.ca/~lisa/pointeurs/bengio+lecun_chapter2007.pdf

    - Kernel methods are fundamentally limited in their ability to learn complex high-dimensional functions
    - Kernel machines are shallow architectures, they can be very sample-inefficient
    - We analyze a limitation of kernel machines with a local kernel
    - We argue that deep architectures have the potential to generalize beyond immediate neighbors

Friston, K. J., & Stephan, K. (2007). Free-energy and the brain. Synthese. Retrieved from https://www.semanticscholar.org/paper/Free-energy-and-the-brain-Friston-Stephan/bf15128e48db7aaa9146328d965fc81dbdf4db3f

Ranzato, M., Poultney, C., Chopra, S., & Cun, Y. (2006). Efficient Learning of Sparse Representations with an Energy-Based Model. Advances in Neural Information Processing Systems, 19. Retrieved from https://proceedings.neurips.cc/paper/2006/hash/87f4d79e36d68c3031ccf6c55e9bbd39-Abstract.html

    - We train a linear encoder-decoder with sparsifying non-linearity in unsupervised way
    - The non-linearity turns a code vector into a quasi-binary sparse code vector
    - Learning proceeds in a two-phase EM-like fashion
    - We use the proposed method to initialize the first CNN layer to achieve SOTA on MNIST

## 2008

Friston, K. J. (2008). Hierarchical Models in the Brain. PLoS Comput. Biol. Retrieved from https://www.semanticscholar.org/paper/Hierarchical-Models-in-the-Brain-Friston/1a014a076cac3c7f5d81a084e296c095f9230437

Lee, H., Ekanadham, C., & Ng, A. (2007). Sparse deep belief net model for visual area V2. Advances in Neural Information Processing Systems, 20. Retrieved from https://papers.nips.cc/paper_files/paper/2007/hash/4daa3db355ef2b0e64b472968cb70f0d-Abstract.html

    - We develop a sparse variant of the deep belief networks
    - The first layer, similar to prior work on sparse coding and ICA, results in edge filters
    - The second layer learns "corner" features that mimic properties of visual cortical area V2

Le Roux, N., & Bengio, Y. (2008). Representational power of restricted boltzmann machines and deep belief networks. arXiv, 18254699. Retrieved from https://www.cl.uni-heidelberg.de/courses/ws14/deepl/LeRouxBengio07.pdf

    - We show that Restricted Boltzmann Machines are universal approximators of discrete distributions
    - Do we need a lot of layers in Deep Belief Networks?
    - Maybe the answer lies in the ability of a DBN to generalize better by having a more compact representation
    - The analysis suggests to investigate KL as an alternative to Contrastive Divergence for training each layer
    - Because it would take into account that more layers will be added

Vincent, P., Larochelle, H., Bengio, Y., & Manzagol, P.-A. (2008). Extracting and composing robust features with denoising autoencoders. Proceedings of the 25th International Conference on Machine Learning, 1096–1103. Retrieved from https://www.cs.toronto.edu/~larocheh/publications/icml-2008-denoising-autoencoders.pdf

    - We propose a recipe for unsupervised learning based on the robustness to partial corruption of the input
    - Denoising autoencoders can be stacked to initialize deep architectures
    - We motivate this from a manifold learning, information theoretic and generative model perspectives
    - Experiments clearly show the surprising advantage of corrupting the input of autoencoders

Wainwright, M. J., & Jordan, M. I. (2008). Graphical Models, Exponential Families, and Variational Inference. MAL, 1(1-2), 1–305. Retrieved from https://www.nowpublishers.com/article/Details/MAL-001

## 2009

Bengio, Y. (2009). Learning Deep Architectures for AI. Foundations, 2(1), 1–55. Retrieved from https://www.iro.umontreal.ca/~lisa/pointeurs/TR1312.pdf

    - We discuss the motivations and principles regarding learning algorithms for deep architectures
    - We discuss multiple levels of distributed representations of the data
    - We focus on the Deep Belief Networks, and their component elements, the Restricted Boltzmann Machine

Erhan, D., Manzagol, P.-A., Bengio, Y., Bengio, S., & Vincent, P. (2009). The Difficulty of Training Deep Architectures and the Effect of Unsupervised Pre-Training. Artificial Intelligence and Statistics. PMLR. Retrieved from https://proceedings.mlr.press/v5/erhan09a.html

    - Our experiments the positive effect of unsupervised pre-training and its role as a regularizer

Friston, K. J., & Kiebel, S. (2009). Predictive coding under the free-energy principle. Philosophical Transactions of the Royal Society B: Biological Sciences. Retrieved from https://www.semanticscholar.org/paper/Predictive-coding-under-the-free-energy-principle-Friston-Kiebel/6927ea92b0d759a75f0f696fa028155d6d9ee2ca

Friston, K. J., & Kiebel, S. (2009). Cortical circuits for perceptual inference. Neural Networks. Retrieved from https://www.semanticscholar.org/paper/Cortical-circuits-for-perceptual-inference-Friston-Kiebel/73881ecac61ef6a05847e0f110b4f3c6477a0d57

Friston, K. J. (2009). The free-energy principle: a rough guide to the brain? Trends in Cognitive Sciences. Retrieved from https://www.semanticscholar.org/paper/The-free-energy-principle%3A-a-rough-guide-to-the-Friston/a878886efacc6a5d742bf98bfc25c0734ce502b1

Friston, K. J., Daunizeau, J., & Kiebel, S. (2009). Reinforcement Learning or Active Inference? PLoS One. Retrieved from https://www.semanticscholar.org/paper/Reinforcement-Learning-or-Active-Inference-Friston-Daunizeau/5e07da2914783a21980ecc1cea688d1333e6b6e4

Salakhutdinov, R., & Hinton, G. (2009). Deep Boltzmann Machines. Artificial Intelligence and Statistics. PMLR. Retrieved from https://proceedings.mlr.press/v5/salakhutdinov09a.html

    - We propose a learning algorithm for Boltzmann machines with many layers of hidden variables
    - Data-dependent expectations are estimated using a variational approximation
    - Data-independent expectations are approximated using persistent Markov chains
    - The learning can be made more efficient by using a layer-by-layer "pre-training" phase
    - On MNIST and NORB deep Boltzmann machines learn good generative models

Larochelle, H., Bengio, Y., Louradour, J., & Lamblin, P. (2009). Exploring Strategies for Training Deep Neural Networks. Journal of Machine Learning Research, 1, 140. Retrieved from https://jmlr.org/papers/volume10/larochelle09a/larochelle09a.pdf

    - We confirmthat the greedy layer-wise unsupervised training strategy helps the optimization by
    - 1) Initializing weights in a region near a good local minimum
    - 2) Implicitly acts as a sort of regularization for high-level abstractions
    - We present a series of experiments demonstrating, for example, where the addition of more depth helps
    - We empirically explore simple variants of training algorithms
    - Highle resembles the paper "Greedy layer-wise training of deep networks"

Salakhutdinov, R. (2009). Learning Deep Generative Models (Dissertation). Retrieved from https://tspace.library.utoronto.ca/bitstream/1807/19226/3/Salakhutdinov_Ruslan_R_200910_PhD_thesis.pdf

    - A PhD thesis of Ruslan Salakhutdinov
    - In pt. 1 we describe Deep Belief Networks
    - In pt. 2 we describe Deep Boltzmann Machines

## 2010

Bottou, L. (2010). Large-Scale Machine Learning with Stochastic Gradient Descent. Proceedings of COMPSTAT'2010. Physica-Verlag HD. Retrieved from https://link.springer.com/chapter/10.1007/978-3-7908-2604-3_16

Dekel, O., Gilad-Bachrach, R., Shamir, O., & Xiao, L. (2010). Optimal Distributed Online Prediction using Mini-Batches. arXiv, 1012.1367. Retrieved from https://arxiv.org/abs/1012.1367v2

Friston, K. J. (2010). The free-energy principle: a unified brain theory? Nat. Rev. Neurosci. Retrieved from https://www.semanticscholar.org/paper/The-free-energy-principle%3A-a-unified-brain-theory-Friston/1ed6a4a10589618d4f26350f1a296ee767ceff6b

Friston, K. J. (2010). Embodied Inference: or “I think therefore I am, if I am what I think”. Retrieved from https://www.semanticscholar.org/paper/Embodied-Inference-%3A-or-%E2%80%9C-I-think-therefore-I-am-%2C-Friston/c5b1e58ba8a5fde9607108e18c0b8ab2158cb1ba

Glorot, X., & Bengio, Y. (2010). Understanding the difficulty of training deep feedforward neural networks. Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics. JMLR Workshop and Conference Proceedings. Retrieved from https://proceedings.mlr.press/v9/glorot10a.html

    - Problem: standard GD from random initialization is doing poorly with deep neural networks
    - The logistic sigmoid activation is unsuited for DNNs, because it drives top hidden layer into saturation
    - Saturated units can slowly move out of saturation by themselves, explaining the plateaus seen when training NNs
    - We propose the hyperbolic tangent non-linearity that saturates less
    - For efficient training the singular values of the Jacobian associated with each layer should be close to 1
    - Based on this, we propose a new initialization scheme that brings substantially faster convergence

Gutmann, M., & Hyvärinen, A. (2010). Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics. JMLR Workshop and Conference Proceedings. Retrieved from https://proceedings.mlr.press/v9/gutmann10a.html

    - We consider a task of learning unnormalized generative models of data (e.g. energy-based models or Markov random fields). Such a model outputs an unnormalized probability density given a data sample, that is, the density function does not integrate to one. Such models are easier to learn that normalized ones, since the normalization coefficient is often intractable.
    - We present noise-contrastive estimation (NCE) for unnormalized models which shows advantages over score matching, contrastive divergence, and maximum-likelihood where the normalization constant is estimated with importance sampling.
    - In NCE, the model is trained to discern between the real data and a predefined noise distribution
    - Our model offers the best trade-off between computational and statistical efficiency

Kingma, D. P., & Cun, Y. (2010). Regularized estimation of image statistics by Score Matching. Advances in Neural Information Processing Systems, 23. Retrieved from https://papers.nips.cc/paper_files/paper/2010/hash/6f3e29a35278d71c7f65495871231324-Abstract.html

    - We propose a version of the double-backpropagation algorithm for training high-dimensional density models
    - In addition, we introduce a regularization term for the Score Matching loss
    - Results are reported for image denoising and super-resolution

Martens, J. (2010). Deep learning via Hessian-free optimization. ICML'10: Proceedings of the 27th International Conference on International Conference on Machine Learning. Omnipress. Retrieved from https://www.cs.toronto.edu/~jmartens/docs/Deep_HessianFree.pdf

Nair, V., & Hinton, G. E. (2010). Rectified linear units improve restricted boltzmann machines. ICML'10: Proceedings of the 27th International Conference on International Conference on Machine Learning. Omnipress. Retrieved from https://www.cs.toronto.edu/%7Efritz/absps/reluICML.pdf

    - Noisy, rectified linear units approximate an infinite number of stochastic hidden units in RBM
    - These uints preserve information about relative intensities

Salakhutdinov, R., & Larochelle, H. (2010). Efficient Learning of Deep Boltzmann Machines. Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics. JMLR Workshop and Conference Proceedings. Retrieved from https://proceedings.mlr.press/v9/salakhutdinov10a/salakhutdinov10a.pdf

    - We present a new approximate inference algorithm for Deep Boltzmann Machines
    - It learns a separate "recognition" model to initialize latent variables in a single bottom-up pass
    - It performs well on visual recognition tasks: MNIST, OCR and NORB

Salakhutdinov, R. (2010). Learning Deep Boltzmann Machines using adaptive MCMC. ICML 2010 - Proceedings, 27th International Conference on Machine Learning, 943–950. Retrieved from https://www.cs.cmu.edu/~rsalakhu/papers/adapt.pdf

    - Deep Boltzmann Machine has an energy landscape with many local minima separated by high energy barriers
    - Gibbs sampler tends to get trapped in one local mode, which often results in unstable learning dynamics
    - We show a close connection between Fast PCD and adaptive MCMC
    - We propose a Coupled Adaptive Simulated Tempering algorithm to better explore a multimodal energy landscape
    - This improves learning of large-scale DBM’s

Vincent, P., Larochelle, H., Lajoie, I., Bengio, Y., & Manzagol, P.-A. (2010). Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion. J. Mach. Learn. Res., 11, 3371–3408. Retrieved from https://www.jmlr.org/papers/volume11/vincent10a/vincent10a.pdf

    - We propose to stack layers of denoising autoencoders which are trained locally
    - It bridges the performance gap between autoencoders and deep belief networks
    - Denoising autoencoders are able to learn Gabor-like edge detectors from natural image patches
    - Thus we establish the value of using a denoising criterion as a tractable unsupervised objective

Zinkevich, M., Weimer, M., Li, L., & Smola, A. (2010). Parallelized Stochastic Gradient Descent. Advances in Neural Information Processing Systems, 23. Retrieved from https://papers.nips.cc/paper_files/paper/2010/hash/abea47ba24142ed16b7d8fbf2c740e0d-Abstract.html

    - We present the first parallel SGD algorithm that is perfectly suited to MapReduce settings

## 2011

Carandini, M., & Heeger, D. J. (2011). Normalization as a canonical neural computation. Nat. Rev. Neurosci., 22108672. Retrieved from https://pubmed.ncbi.nlm.nih.gov/22108672

Friston, K. J. (2011). What Is Optimal about Motor Control? Neuron. Retrieved from https://www.semanticscholar.org/paper/What-Is-Optimal-about-Motor-Control-Friston/f3ba5b8e81eb4fe8fe5b381a65fd1003cd25d451

Glorot, X., Bordes, A., & Bengio, Y. (2011). Deep Sparse Rectifier Neural Networks. Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics. JMLR Workshop and Conference Proceedings. Retrieved from https://proceedings.mlr.press/v15/glorot11a.html

    - This work extends the results of Nair and Hinton (2010) for the case of denoising autoencoders
    - Rectifying neurons yield equal or better performance than hyperbolic tangent networks
    - They create sparse representations with true zeros, which seem suitable for naturally sparse data
    - They can reach their best performance without requiring any unsupervised pre-training
    - They are also a better model of biological neurons

Huang, Y., & Rao, R. P. N. (2011). Predictive coding. Wiley Interdiscip. Rev. Cognit. Sci. Retrieved from https://www.semanticscholar.org/paper/Predictive-coding.-Huang-Rao/f1eb4088e2a1d433abb5627e18cd4d40886dc2fa

Larochelle, H., & Murray, I. (2011). The Neural Autoregressive Distribution Estimator. Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics. JMLR Workshop and Conference Proceedings. Retrieved from https://proceedings.mlr.press/v15/larochelle11a.html

Ngiam, J., Chen, Z., Bhaskar, S., Koh, P., & Ng, A. (2011). Sparse Filtering. Advances in Neural Information Processing Systems, 24. Retrieved from https://papers.nips.cc/paper_files/paper/2011/hash/192fc044e74dffea144f9ac5dc9f3395-Abstract.html

    - We present Sparse filtering: an unsupervised feature learning method
    - It has only one hyperparemeter - the number of features to learn
    - Given a feature matrix, it involves first L2-normalizing it by rows, then by columns and finally summing up the absolute value of all entries (so, we optimize them for sparseness using the L1-penalty)
    - This method is faster than sparse coding, ICA, and sparse autoencoders
    - The sparse filtering objective can be viewed as a normalized version of the ICA objective

Oseledets, I. V. (2011). Tensor-Train Decomposition. SIAM J. Sci. Comput. Retrieved from https://epubs.siam.org/doi/abs/10.1137/090752286?journalCode=sjoce3

    - A new way to approximate tensor with a product of simpler tensors (eq. 1.2)

Rifai, S., Vincent, P., Muller, X., Glorot, X., & Bengio, Y. (2011). Contractive Auto-Encoders: Explicit Invariance During Feature Extraction. International Conference on Machine Learning. Retrieved from https://icml.cc/2011/papers/455_icmlpaper.pdf

    - We add a well chosen penalty term to the classical reconstruction cost function in deterministic auto-encoders
    - This penalty is a Frobenius norm of the Jacobian matrix of the encoder activations
    - It achieves equal results or outperforms denoising auto-encoders
    - It can be seen as a link between deterministic and non-deterministic auto-encoders
    - It forms better representation that corresponds to lower-dimensional non-linear manifold

Saxe, A. M., Koh, P. W., Chen, Z., Bhand, M., & Ng, A. Y. (2011). On Random Weights and Unsupervised Feature Learning. Proceedings of the 28th International Conference on Machine Learning, 1089–1096. Retrieved from https://icml.cc/2011/papers/551_icmlpaper.pdf

    - Question: while it has been known that certain feature learning architectures can yield useful features for object recognition tasks even with untrained, random weights, why do random weights sometimes do so well?
    - We find that some CNNs can be frequency selective and translation invariant with random weights
    - We demonstrate the viability of using random weights to quickly evaluate candidate architectures
    - A large fraction of the SOTA methods performance can be attributed to the architecture alone

Torralba, A., & Efros, A. A. . Unbiased look at dataset bias. CVPR 2011. IEEE. doi: 10.1109/CVPR.2011.5995347. Retrieved from https://people.csail.mit.edu/torralba/publications/datasets_cvpr11.pdf

    - The authors complain that the community usually focus on beating some benchmarks, which have become closed worlds unto themselves. Have we perhaps lost sight of the original purpose of visual recognition?
    - Let’s play a game we call Name That Dataset! Most of these datasets were collected with the expressed goal of being as varied and rich as possible, aiming to sample the visual world “in the wild”.Yet this task turns out to be easy.
    - We trained a classifier to play Name That Dataset. It performs rather well at 39% (chance is 8%) being trained on 1000 samples per dataset, and even better with more data. So, the datasets appear to have a strong build-in bias.
    - Is it to be expected that when training on one dataset and testing on another there is a big drop in performance? If a dataset defines a “car” to be the rear view of a race-car, then there is no reasonable algorithm that will say that a side view of a family sedan is also a “car”.
    - For example, in cross-dataset testing, for the ”car” classification task the average performance obtained when training and testing on the same dataset is 53.4% which drops to 27.5%. So, little generalization appears to be happening beyond the given dataset.
    - 1) Selection Bias: getting images from the Internet does not in itself guarantee a fair sampling, since keyword-based searches will return only particular types of images. It might be better to start with a large collection of unannotated images and label them by crowd-sourcing.
    - 2) Capture Bias: the object is almost always in the center of the image. Almost all the mugs has a right-facing handle, etc. One way to deal with this is to perform various data transformations to reduce this bias.
    - 3) Category or label bias: this comes from the fact that semantic categories are often poorly defined, and different labellers may assign differing labels to the same type of object (e.g. “grass” vs. “lawn”, “painting” vs. “picture”).
    - 4) Negative Set Bias: having a rich and unbiased negative set is important to classifier performance. One remedy is to add negatives from other datasets. Another approach is to actively mine hard negatives as part of dataset construction.
    - A good first step would be to run any new dataset on the battery of tests that have been outlined in this paper. It might help finding the main problematic issues quickly and early, not years after the dataset has been released.

Welling, M., & Teh, Y. W. (2011). Bayesian learning via stochastic gradient langevin dynamics. ICML'11: Proceedings of the 28th International Conference on International Conference on Machine Learning. Omnipress. Retrieved from https://www.stats.ox.ac.uk/~teh/research/compstats/WelTeh2011a.pdf

    - We propose to inject Gaussian noise with a specific variance into SGD optimization
    - This prevents collapse of SGD to MAP solution
    - Instead, it converges to samples from the true posterior as we polynomially anneal the stepsize
    - The resulting algorithm starts off being similar to stochastic optimization
    - Then it automatically transits to Langevin dynamics (which simulates samples from the posterior)
    - We propose a practical method to estimate when this transition happens
    - This can be seen as automatic "early stopping"
    - We test on a mixture of Gaussians, logistic regression and ICA with natural gradients

## 2012

Adams, R. A., Shipp, S., & Friston, K. J. (2012). Predictions not commands: active inference in the motor system. Brain Struct. Funct. Retrieved from https://www.semanticscholar.org/paper/Predictions-not-commands%3A-active-inference-in-the-Adams-Shipp/17b7e0b33847ca1c0546f714b75bdd768263b267

Alain, G., & Bengio, Y. (2012). What Regularized Auto-Encoders Learn from the Data Generating Distribution. arXiv, 1211.4246. Retrieved from https://arxiv.org/abs/1211.4246v5

    - The question: what does an auto-encoder learn about the data generating density?
    - Our main answer is that it estimates the score (first derivative of the log-density), and it also estimates the Hessian (second derivative of the log-density)
    - It locally characterizes the shape of the data generating density (local manifold structure of data)
    - It contradicts previous interpretations of reconstruction error as an energy function
    - Approximate Metropolis-Hastings MCMC can be setup to recover samples from the estimated distribution

Bastos, A., Usrey, W., Adams, R. A., Mangun, G., Fries, P., & Friston, K. J. (2012). Canonical Microcircuits for Predictive Coding. Neuron. Retrieved from https://www.semanticscholar.org/paper/Canonical-Microcircuits-for-Predictive-Coding-Bastos-Usrey/755bfd4f8060d5fcda64eaedb81a520ab7c8bdba

Bengio, Y., Courville, A., & Vincent, P. (2012). Representation Learning: A Review and New Perspectives. arXiv, 1206.5538. Retrieved from https://arxiv.org/abs/1206.5538v3

    - This paper is about representation learning
    - We discuss advances in probabilistic models, auto-encoders, manifold learning, and deep networks
    - We discuss the problem of the multimodality of the posterior P(h|x)
    - We cover many high-level generic priors that we believe could improve representation learning
    - The long-term objective is to discover learning algorithms that can disentangle underlying factors

Bengio, Y. (2012). Practical recommendations for gradient-based training of deep architectures. arXiv, 1206.5533. Retrieved from https://arxiv.org/abs/1206.5533v2

    - A practical guide for a common hyper-parameters for GD optimization
    - We describe methods to debug and visualize neural networks
    - We cover parallelism, sparse high-dimensional inputs, symbolic inputs and embeddings, multi-relational learning
    - We pose open questions on the difficulty of training deep architectures

Bengio, Y., Mesnil, G., Dauphin, Y., & Rifai, S. (2012). Better Mixing via Deep Representations. arXiv, 1207.4404. Retrieved from https://arxiv.org/abs/1207.4404v1

    - We test several hypotheses:
    - H1: a successfully trained DNN yields representation spaces in which Markov chains mix faster
    - H2: H1 is true because deeper representations can better disentangle the underlying factors of variation
    - H3: H2 is true because in disentangled representations samples fill the space uniformly
    - The experimental results were in agreement with these hypotheses
    - Empirically, at higher levels, good samples are obtained when interpolating between examples
    - Empirically, at higher levels, good samples are obtained when adding isotropic noise

Friston, K. J., Adams, R. A., Perrinet, L. U., & Breakspear, M. (2012). Perceptions as Hypotheses: Saccades as Experiments. Front. Psychol. Retrieved from https://www.semanticscholar.org/paper/Perceptions-as-Hypotheses%3A-Saccades-as-Experiments-Friston-Adams/04bc5da3f1e62a68f03692182ac4fededf9d1518

Hinton, G. E., Srivastava, N., Krizhevsky, A., Sutskever, I., & Salakhutdinov, R. R. (2012). Improving neural networks by preventing co-adaptation of feature detectors. arXiv, 1207.0580. Retrieved from https://arxiv.org/abs/1207.0580v1

    - Dropout method: randomly omitting half of the feature detectors (FD) on each training case
    - This prevents co-adaptations, when a FD is only helpful in the context of several other specific FDs
    - Instead, each neuron learns to detect a feature that is generally helpful
    - Gives big improvements on many benchmark tasks

Pascanu, R., Mikolov, T., & Bengio, Y. (2012). On the difficulty of training Recurrent Neural Networks. arXiv, 1211.5063. Retrieved from https://arxiv.org/abs/1211.5063v2

    - We attempt to improve the understanding of gradient vanishing and exploding in RNNs
    - We propose a gradient norm clipping strategy to deal with exploding gradients
    - We propose a soft constraint for the vanishing gradients problem

Rifai, S., Bengio, Y., Dauphin, Y., & Vincent, P. (2012). A Generative Process for Sampling Contractive Auto-Encoders. arXiv, 1206.6434. Retrieved from https://arxiv.org/abs/1206.6434v1

    - We propose a procedure for generating samples from contractive auto-encoders
    - It experimentally converges quickly and mix well between modes, compared to RBM and DBN
    - We propose to to train the second layer of contraction that pools lower-level features and learns to be invariant to the local directions of variation discovered in the first layer

Schoelkopf, B., Janzing, D., Peters, J., Sgouritsa, E., Zhang, K., & Mooij, J. (2012). On Causal and Anticausal Learning. arXiv, 1206.6471. Retrieved from https://arxiv.org/abs/1206.6471v1

    - To predict one variable from another, it helps to know the causal structure underlying the variables
    - We discuss causal (predict Effect from Cause) and anticausal (Cause from Effect) predictions
    - Hypothesis: under an independence assumption for causal mechanism and input, semi-supervised learning works better in anticausal or confounded problems
    - This can be useful for covariate shift, concept drift, transfer learning, semi-supervised learning

Swersky, K., Ranzato, M., Buchman, D., Marlin, B. M., & de Freitas, N. (2011). On Autoencoders and Score Matching for Energy Based Models. ResearchGate, 1201–1208. Retrieved from https://www.cs.toronto.edu/~ranzato/publications/Swersky_icml2011.pdf

    - We discuss continuous-data energy based models (EBMs)
    - Let the conditional distribution over the visible units is Gaussian
    - In this case, provide a link between EBM with score matching and a form of regularized autoencoder

## 2013

Ba, J., & Frey, B. (2013). Adaptive dropout for training deep neural networks. Advances in Neural Information Processing Systems, 26. Retrieved from https://papers.nips.cc/paper_files/paper/2013/hash/7b5b23f4aadf9513306bcd59afb6e4c9-Abstract.html

    - We propose standout method, when dropout is performed with a specific binary belief network
    - Both networks can be trained jointly

Baldi, P., & Sadowski, P. J. (2013). Understanding Dropout. Advances in Neural Information Processing Systems, 26. Retrieved from https://papers.nips.cc/paper_files/paper/2013/hash/71f6278d140af599e06ad9bf1ba03cb0-Abstract.html

    - We propose a general formalism for studying dropout
    - The averaging properties of dropout are characterized by three recursive equations
    - We also show how dropout performs SGD on a regularized error function

Bengio, Y. (2013). Deep Learning of Representations: Looking Forward. arXiv, 1305.0445. Retrieved from https://arxiv.org/abs/1305.0445v2

    - We present a few appealing directions of research towards deep learning challenges, including:
    - 1) Scaling computations
    - 2) Reducing the difficulties in optimizing parameters
    - 3) designing (or avoiding) expensive inference and sampling
    - 4) Helping to learn representations that better disentangle the unknown underlying factors of variation

Bengio, Y., Yao, L., Alain, G., & Vincent, P. (2013). Generalized Denoising Auto-Encoders as Generative Models. arXiv, 1305.6663. Retrieved from https://arxiv.org/abs/1305.6663v4

    - We propose a different probabilistic interpretation of Denoising Auto-Encoders
    - It is valid for any data type, any corruption process and any reconstruction loss
    - We can improve the sampling behavior by using the model itself to define the corruption process

Bengio, Y., Léonard, N., & Courville, A. (2013). Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation. arXiv, 1308.3432. Retrieved from https://arxiv.org/abs/1308.3432v1

    - A question: can we "back-propagate" through stochastic neurons?
    - An existing approach is a special case of the REINFORCE algorithm
    - We propose an appoach to decompose neuron into a stochastic binary part and a smooth differentiable part
    - We also propose to inject additive or multiplicative noise in a differentiable computational graph
    - A fourth approach is a "straight-through estimator"
    - Experiments show that all the tested methods actually allow training to proceed
    - We describe several conclusions from the experiments

Bengio, Y., Thibodeau-Laufer, É., Alain, G., & Yosinski, J. (2013). Deep Generative Stochastic Networks Trainable by Backprop. arXiv, 1306.1091. Retrieved from https://arxiv.org/abs/1306.1091v5

Clark, A. (2013). Whatever next? Predictive brains, situated agents, and the future of cognitive science. Behav. Brain Sci. Retrieved from https://www.semanticscholar.org/paper/Whatever-next-Predictive-brains%2C-situated-agents%2C-Clark/69304ba2c5d2bff09f7059916ab0aa117bdbea41

Daniely, A., Linial, N., & Shalev-Shwartz, S. (2013). From average case complexity to improper learning complexity. arXiv, 1311.2272. Retrieved from https://arxiv.org/abs/1311.2272v2

Friston, K. J., & Friston, D. (2013). A Free Energy Formulation of Music Generation and Perception: Helmholtz Revisited. Retrieved from https://www.semanticscholar.org/paper/A-Free-Energy-Formulation-of-Music-Generation-and-Friston-Friston/b79995c7b34e639b273af4468702c53c9c3c9fc8

Friston, K. J., Schwartenbeck, P., FitzGerald, T., Moutoussis, M., Behrens, T., & Dolan, R. (2013). The anatomy of choice: active inference and agency. Front. Hum. Neurosci. Retrieved from https://www.semanticscholar.org/paper/The-anatomy-of-choice%3A-active-inference-and-agency-Friston-Schwartenbeck/3144baaeb4cab2bf48a8140de03080c03c4488b9

Goodfellow, I. J., Mirza, M., Xiao, D., Courville, A., & Bengio, Y. (2013). An Empirical Investigation of Catastrophic Forgetting in Gradient-Based Neural Networks. arXiv, 1312.6211. Retrieved from https://arxiv.org/abs/1312.6211v3

    - We found that using dropout is good at adapting to the new task, remembering the old task
    - As for comparing activation functions, different tasks and relationships between them result in very different rankings, this suggests that the choice of activation function should always be cross-validated

Goodfellow, I. J., Warde-Farley, D., Mirza, M., Courville, A., & Bengio, Y. (2013). Maxout Networks. arXiv, 1302.4389. Retrieved from https://arxiv.org/abs/1302.4389v4

Goodfellow, I. J., Erhan, D., Carrier, P. L., Courville, A., Mirza, M., Hamner, B., ...Bengio, Y. (2013). Challenges in Representation Learning: A report on three machine learning contests. arXiv, 1307.0414. Retrieved from https://arxiv.org/abs/1307.0414v1

    - We describe the ICML 2013 Workshop on Challenges in Representation Learning (datasets and results)
    - It was focused on the black box learning, the facial expression recognition and the multimodal learning
    - In this contest, unsupervised learning is important because of small amount of labeled data
    - The winner used sparse filtering for feature learning, RF for feature selection, SVM for classification
    - The second place rediscovered entropy regularization
    - Other competitors also obtained very competitive results with sparse filtering; this is interesting because sparse filtering has been perceived as an inexpensive and simple method that gives good but not optimal results
    - Sparse filtering features on the combination of the labeled and unlabeled data worked worse than learning the features on just the labeled data; this may be because the labeled data was drawn from the more difficult portion of the SVHN dataset

Gregor, K., Danihelka, I., Mnih, A., Blundell, C., & Wierstra, D. (2013). Deep AutoRegressive Networks. arXiv, 1310.8499. Retrieved from https://arxiv.org/abs/1310.8499v2

    - We propose Deep AutoRegressive Network (DARN)
    - This is a deep, generative autoencoder with stochastic layers equipped with autoregressive connections
    - This enable to sample from model quickly and exactly via ancestral sampling
    - We derive an efficient approximate parameter estimation method based on MDL and maximizing ELBO
    - We achieve SOTA generative performance on a number of datasets

Kingma, D. P., & Welling, M. (2013). Auto-Encoding Variational Bayes. arXiv, 1312.6114. Retrieved from https://arxiv.org/abs/1312.6114v11

    - We aim to learn a directed probabilistic model with continuous latent variables
    - A reparameterization of the ELBO yields an estimator that can be optimized using SGD
    - We fit an approximate inference model to the intractable posterior using this estimator

Friston, K. J. (2013). Life as we know it. J. R. Soc. Interface. Retrieved from https://www.semanticscholar.org/paper/Life-as-we-know-it-Friston/9d91dead9612e1c6a8dd9ae3793fe697565c744e

Pascanu, R., Montufar, G., & Bengio, Y. (2013). On the number of response regions of deep feed forward networks with piece-wise linear activations. arXiv, 1312.6098. Retrieved from https://arxiv.org/abs/1312.6098v5

Saxe, A. M., McClelland, J. L., & Ganguli, S. (2013). Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. arXiv, 1312.6120. Retrieved from https://arxiv.org/abs/1312.6120v3

    - Consider a DNN without activations
    - Despite the linearity, they have nonlinear GD dynamics, long plateaus followed by rapid transitions
    - Discussing some initial conditions on the weights that emerge during unsupervised pretraining
    - Propose a dynamical isometry (all singular values of the Jacobian concentrate near 1)
    - Propose orthonormal initialization
    - Faithful gradient propagation occurs in a special regime known as the edge of chaos

Sutskever, I., Martens, J., Dahl, G. E., & Hinton, G. E. (2013). On the importance of initialization and momentum in deep learning. International Conference on Machine Learning. Retrieved from https://www.cs.toronto.edu/~gdahl/papers/momentumNesterovDeepLearning.pdf

    - We show that SGD with momentum can be successfully applied to train DNNs
    - We use a well-designed random initialization and a particular type of slowly increasing schedule
    - Poorly initialized networks cannot be trained with momentum
    - Well-initialized networks perform worse when the momentum is absent or poorly tuned

Uria, B., Murray, I., & Larochelle, H. (2013). A Deep and Tractable Density Estimator. arXiv, 1310.1757. Retrieved from https://arxiv.org/abs/1310.1757v2

Wager, S., Wang, S., & Liang, P. (2013). Dropout Training as Adaptive Regularization. arXiv, 1307.1493. Retrieved from https://arxiv.org/abs/1307.1493v2

Wang, S., & Manning, C. (2013). Fast dropout training. International Conference on Machine Learning. PMLR. Retrieved from https://proceedings.mlr.press/v28/wang13a.html

    - Problem: dropout makes training much slower
    - We consider an implied objective function of dropout and propose a method to speed up its optimization
    - We show how to do fast dropout training for classification, regression, and DNNs
    - Fast dropout often reaches the same validation set performance in a shorter time and in less iterations
    - Beyond dropout, our technique is extended for types of noise and small image transformations

## 2014

Bach, F. (2014). Breaking the Curse of Dimensionality with Convex Neural Networks. arXiv, 1412.8690. Retrieved from https://arxiv.org/abs/1412.8690v2

Bahdanau, D., Cho, K., & Bengio, Y. (2014). Neural Machine Translation by Jointly Learning to Align and Translate. arXiv, 1409.0473. Retrieved from https://arxiv.org/abs/1409.0473v7

    - We conjecture that the use of a fixed-length vector between encoder and decoder is a bottleneck for NMT
    - We propose and attention mechanism: model automatically (soft-)searches for parts of a source sentence that are relevant to predicting a target word
    - Analysis reveals that the (soft-)alignments found by the model agree well with our intuition

Bengio, Y. (2014). How Auto-Encoders Could Provide Credit Assignment in Deep Networks via Target Propagation. arXiv, 1407.7906. Retrieved from https://arxiv.org/abs/1407.7906v3

    - Three existing approaches are discussed to propagate training signals across the levels of representation
    - There is a fourth explored approach, to which the proposal discussed here belongs, based on target propagation
    - We propose to train DL models with reconstruction as a layer-local training signal
    - The auto-encoder may take both a representation of input and target
    - A deep auto-encoder decoding path generalizes gradient propagation
    - So, we are proposing to learn the back-propagation computation
    - The potential of this framework is to provide a biologically plausible credit assignment mechanism that would replace and not require back-propagation

Bornschein, J., & Bengio, Y. (2014). Reweighted Wake-Sleep. arXiv, 1406.2751. Retrieved from https://arxiv.org/abs/1406.2751v4

Choromanska, A., Henaff, M., Mathieu, M., Arous, G. B., & LeCun, Y. (2014). The Loss Surfaces of Multilayer Networks. arXiv, 1412.0233. Retrieved from https://arxiv.org/abs/1412.0233v3

    - We study FCN loss surface using random matrix theory
    - Assumptions of i) variable independence, ii) overparametrization, and iii) uniformity
    - FCN loss landscape is similar to the Hamiltonian of the H-spin spherical spin-glass model
    - We study locations of critical points (maxima, minima, and saddle points) of the random loss function
    - They are located in a well-defined band lower-bounded by the global minimum
    - They are of high quality measured by the test error
    - Both simulated annealing and SGD converges to them
    - On the contrary, in small-size networks poor quality local minima have nonzero probability of being recovered

Dauphin, Y., Pascanu, R., Gulcehre, C., Cho, K., Ganguli, S., & Bengio, Y. (2014). Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. arXiv, 1406.2572. Retrieved from https://arxiv.org/abs/1406.2572v1

    - We discuss high-dimensional non-convex optimization
    - It is often believed that local minima are a problem for GD and quasi-Newton methods
    - We argue that saddle points, not local minima are a problem
    - They are surrounded by high error plateaus and give the illusory impression of local minima
    - We propose the saddle-free Newton method, that can rapidly escape high dimensional saddle points
    - It has superior optimization performance for RNNs

Dinh, L., Krueger, D., & Bengio, Y. (2014). NICE: Non-linear Independent Components Estimation. arXiv, 1410.8516. Retrieved from https://arxiv.org/abs/1410.8516v6

    - We think that a good representation is one in which the data has a distribution that is easy to model
    - We propose Non-linear Independent Component Estimation (NICE) for modeling complex high-dimensional densities
    - It learns factorized latent distribution, i.e. independent latent variables
    - This approach yields good generative models on four image datasets and can be used for inpainting

Friston, K. J., Sengupta, B., & Auletta, G. (2014). Cognitive Dynamics: From Attractors to Active Inference. Proc. IEEE. Retrieved from https://www.semanticscholar.org/paper/Cognitive-Dynamics%3A-From-Attractors-to-Active-Friston-Sengupta/e3069e95026378d344e22766adac10490b053078

Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., ...Bengio, Y. (2014). Generative Adversarial Networks. arXiv, 1406.2661. Retrieved from https://arxiv.org/abs/1406.2661v1

Goodfellow, I. J., Vinyals, O., & Saxe, A. M. (2014). Qualitatively characterizing neural network optimization problems. arXiv, 1412.6544. Retrieved from https://arxiv.org/abs/1412.6544v6

    - We analyze why model NNs are overcoming local optima
    - We find that on a straight path from initialization to solution, a variety of SOTA NNs never encounter any significant obstacles

Graves, A., Wayne, G., & Danihelka, I. (2014). Neural Turing Machines. arXiv, 1410.5401. Retrieved from https://arxiv.org/abs/1410.5401v2

Kingma, D. P., & Welling, M. (2014). Efficient Gradient-Based Inference through Transformations between Bayes Nets and Neural Nets. arXiv, 1402.0480. Retrieved from https://arxiv.org/abs/1402.0480v5

Kingma, D. P., & Ba, J. (2014). Adam: A Method for Stochastic Optimization. arXiv, 1412.6980. Retrieved from https://arxiv.org/abs/1412.6980v9

    - We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objectives
    - It is scalable and invariant to diagonal rescaling of the gradients
    - It is appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients
    - We provide a theoretical analysis of Adam’s convergence in online convex programming
    - We discuss AdaMax, a variant of Adam based on the infinity norm

Kingma, D. P., Rezende, D. J., Mohamed, S., & Welling, M. (2014). Semi-Supervised Learning with Deep Generative Models. arXiv, 1406.5298. Retrieved from https://arxiv.org/abs/1406.5298v2

Kistler, N. (2014). Derrida's random energy models. From spin glasses to the extremes of correlated random fields. arXiv, 1412.0958. Retrieved from https://arxiv.org/abs/1412.0958v1

    - This is notes for a mini-course on the extremes of correlated random fields
    - Derrida's random energy models are described
    - Gaussian hierarchical field are described

Koutník, J., Greff, K., Gomez, F., & Schmidhuber, J. (2014). A Clockwork RNN. arXiv, 1402.3511. Retrieved from https://arxiv.org/abs/1402.3511v1

Lillicrap, T. P., Cownden, D., Tweed, D. B., & Akerman, C. J. (2014). Random feedback weights support learning in deep neural networks. arXiv, 1411.0247. Retrieved from https://arxiv.org/abs/1411.0247v1

    - Backpropagation requires biologically implausible transport of individual synaptic weight information
    - We find that a network can learn with random feedback connections instead of backpropagation
    - This means replacing W^T by a matrix of fixed random weights
    - We call this feedback alignment
    - This new mechanism performs as quickly and accurately as backpropagation on a variety of problems

Maddison, C. J., Tarlow, D., & Minka, T. (2014). A* Sampling. arXiv, 1411.0030. Retrieved from https://arxiv.org/abs/1411.0030v2

Maeda, S.-i. (2014). A Bayesian encourages dropout. arXiv, 1412.7003. Retrieved from https://arxiv.org/abs/1412.7003v3

    - We provide a Bayesian interpretation to dropout
    - The inference after dropout training can be considered as an approximate inference by Bayesian model averaging
    - This view enables us to optimize the dropout rate (Bayesian dropout)

Montúfar, G., Pascanu, R., Cho, K., & Bengio, Y. (2014). On the Number of Linear Regions of Deep Neural Networks. arXiv, 1402.1869. Retrieved from https://arxiv.org/abs/1402.1869v2

    - We study the advantage of depth for neural networks with piecewise linear activation functions
    - We calculate the maximal number of linear regions of the functions computed by a neural network
    - The depth yields an exponential number of input regions mapped to the same output

Neyshabur, B., Tomioka, R., & Srebro, N. (2014). In Search of the Real Inductive Bias: On the Role of Implicit Regularization in Deep Learning. arXiv, 1412.6614. Retrieved from https://arxiv.org/abs/1412.6614v4

    - We tried to force neural net overfitting by adding random label noise to the data
    - We show on single-hidden-layer networks that size does not behave as a capacity control parameter
    - We suggest that optimization is introducing some implicit regularization by trying to find a solution with small "complexity", for some notion of complexity, perhaps norm
    - We demonstrate how implicit L2 weight decay in an infinite two-layer network gives rise to a "convex neural net" with L1 (not L2) regularization in the top layer

Ozair, S., & Bengio, Y. (2014). Deep Directed Generative Autoencoders. arXiv, 1410.0630. Retrieved from https://arxiv.org/abs/1410.0630v1

    - We consider discrete data
    - The objective is to learn an encoder f that maps X to f(X) that has a much simpler distribution
    - Generating samples from the model is straightforward using ancestral sampling
    - We can pre-train and stack these encoders, gradually transforming the data distribution

Rezende, D. J., Mohamed, S., & Wierstra, D. (2014). Stochastic Backpropagation and Approximate Inference in Deep Generative Models. arXiv, 1401.4082. Retrieved from https://arxiv.org/abs/1401.4082v3

Romero, A., Ballas, N., Kahou, S. E., Chassang, A., Gatta, C., & Bengio, Y. (2014). FitNets: Hints for Thin Deep Nets. arXiv, 1412.6550. Retrieved from https://arxiv.org/abs/1412.6550v4

Salimans, T., Kingma, D. P., & Welling, M. (2014). Markov Chain Monte Carlo and Variational Inference: Bridging the Gap. arXiv, 1410.6460. Retrieved from https://arxiv.org/abs/1410.6460v4

Schmidhuber, J. (2014). Deep Learning in Neural Networks: An Overview. arXiv, 1404.7828. Retrieved from https://arxiv.org/abs/1404.7828v4

Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., & Salakhutdinov, R. (2014). Dropout: A Simple Way to Prevent Neural Networks from Overfitting. Journal of Machine Learning Research, 15(56), 1929–1958. Retrieved from https://jmlr.org/papers/v15/srivastava14a.html

    - We propose a Gaussian Dropout that multiplies the outputs of the neurons by Gaussian random noise
    - We show that dropout improves the performance of NNss on supervised learning, tasks in vision, speech recognition, document classification and computational biology

Yosinski, J., Clune, J., Bengio, Y., & Lipson, H. (2014). How transferable are features in deep neural networks? arXiv, 1411.1792. Retrieved from https://arxiv.org/abs/1411.1792v1

    - An well-known fact is that the first-layer features of CNN appear to learn universally-applicable features
    - We define a way to quantify the degree to which a particular layer is general or specific
    - We then train pairs of CNNs on ImageNet and characterize the layer-by-layer transition from general to specific
    - We show two issues that cause performance degradation when using transferred features without fine-tuning:
    - 1) the specificity of the features themselves
    - 2) optimization difficulties due to splitting the base network between co-adapted layers
    - Initializing a network with transferred features from almost any number of layers boosts fine-tuning perfomance
    - This effect persists even after extensive fine-tuning
    - Transferring features even from distant tasks can be better than using random features

Zhang, S., Choromanska, A., & LeCun, Y. (2014). Deep learning with Elastic Averaging SGD. arXiv, 1412.6651. Retrieved from https://arxiv.org/abs/1412.6651v8

Zhao, P., & Zhang, T. (2014). Accelerating Minibatch Stochastic Gradient Descent using Stratified Sampling. arXiv, 1405.3080. Retrieved from https://arxiv.org/abs/1405.3080v1

## 2015

Baldassi, C., Ingrosso, A., Lucibello, C., Saglietti, L., & Zecchina, R. (2015). Subdominant Dense Clusters Allow for Simple Learning and High Computational Performance in Neural Networks with Discrete Synapses. arXiv, 1509.05753. Retrieved from https://arxiv.org/abs/1509.05753v1

Barrett, L. F., & Simmons, K. (2015). Interoceptive predictions in the brain. Nat. Rev. Neurosci. Retrieved from https://www.semanticscholar.org/paper/Interoceptive-predictions-in-the-brain-Barrett-Simmons/7a0b470ccc96aa1c8a95f0723c49e70cb18507b2

Chung, J., Kastner, K., Dinh, L., Goel, K., Courville, A., & Bengio, Y. (2015). A Recurrent Latent Variable Model for Sequential Data. arXiv, 1506.02216. Retrieved from https://arxiv.org/abs/1506.02216v6

Choromanska, A., LeCun, Y., & Arous, G. B. (2015). Open Problem: The landscape of the loss surfaces of multilayer networks. Conference on Learning Theory. PMLR. Retrieved from https://proceedings.mlr.press/v40/Choromanska15.html

    - We pose an open problem: is it possible to establish a connection between the loss function of the neural networks and the Hamiltonian of the spherical spin-glass models under realistic assumptions?
    - The central problem is to eliminate unrealistic assumptions of variable independence

Clevert, D.-A., Unterthiner, T., & Hochreiter, S. (2015). Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs). arXiv, 1511.07289. Retrieved from https://arxiv.org/abs/1511.07289v5

Courbariaux, M., Bengio, Y., & David, J.-P. (2015). BinaryConnect: Training Deep Neural Networks with binary weights during propagations. arXiv, 1511.00363. Retrieved from https://arxiv.org/abs/1511.00363v3

Figurnov, M., Ibraimova, A., Vetrov, D., & Kohli, P. (2015). PerforatedCNNs: Acceleration through Elimination of Redundant Convolutions. arXiv, 1504.08362. Retrieved from https://arxiv.org/abs/1504.08362v4

Friston, K. J., Rigoli, F., Ognibene, D., Mathys, C., FitzGerald, T. H. B., & Pezzulo, G. (2015). Active inference and epistemic value. Cognitive neuroscience. Retrieved from https://www.semanticscholar.org/paper/Active-inference-and-epistemic-value-Friston-Rigoli/57620e357ee348cd5ffa8eafa480e002a2aba06a

Friston, K. J., Levin, M., Sengupta, B., & Pezzulo, G. (2015). Knowing one's place: a free-energy approach to pattern regulation. J. R. Soc. Interface. Retrieved from https://www.semanticscholar.org/paper/Knowing-one's-place%3A-a-free-energy-approach-to-Friston-Levin/2c13294ccc0045d24fe0ae01a5ff6dd21d0566d1

Gal, Y., & Ghahramani, Z. (2015). Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning. arXiv, 1506.02142. Retrieved from https://arxiv.org/abs/1506.02142v6

    - We show that dropout training in DNNs is an approximate Bayesian inference in deep Gaussian processes
    - We propose Monte Carlo dropout: N stochastic forward passes with averaging
    - This allows to estimate the model uncertainty
    - We show that model uncertainty is indispensable for classification tasks
    - We also discuss model uncertainty in RL

Ge, R., Huang, F., Jin, C., & Yuan, Y. (2015). Escaping From Saddle Points --- Online Stochastic Gradient for Tensor Decomposition. arXiv, 1503.02101. Retrieved from https://arxiv.org/abs/1503.02101v1

    - We consider non-convex functions with exponentially many local minima and saddle points, for example, orthogonal tensor decomposition problem, that is the key step in spectral learning for many latent variable models
    - We identify a property of non-convex functions which we call "strict saddle"
    - Intuitively, it guarantees local progress if we have access to the Hessian information
    - With strict saddle property SGD converges to a local minimum in a polynomial number of iterations

He, K., Zhang, X., Ren, S., & Sun, J. (2015). Deep Residual Learning for Image Recognition. arXiv, 1512.03385. Retrieved from https://arxiv.org/abs/1512.03385v1

He, K., Zhang, X., Ren, S., & Sun, J. (2015). Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification. arXiv, 1502.01852. Retrieved from https://arxiv.org/abs/1502.01852v1

Hinton, G., Vinyals, O., & Dean, J. (2015). Distilling the Knowledge in a Neural Network. arXiv, 1503.02531. Retrieved from https://arxiv.org/abs/1503.02531v1

Ioffe, S., & Szegedy, C. (2015). Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. arXiv, 1502.03167. Retrieved from https://arxiv.org/abs/1502.03167v3

Joulin, A., & Mikolov, T. (2015). Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets. arXiv, 1503.01007. Retrieved from https://arxiv.org/abs/1503.01007v4

Kaiser, Ł., & Sutskever, I. (2015). Neural GPUs Learn Algorithms. arXiv, 1511.08228. Retrieved from https://arxiv.org/abs/1511.08228v3

Lee, T. S. (2015). The visual system's internal model of the world. Proc. IEEE Inst. Electr. Electron. Eng., 26566294. Retrieved from https://pubmed.ncbi.nlm.nih.gov/26566294

Li, Y., Yosinski, J., Clune, J., Lipson, H., & Hopcroft, J. (2015). Convergent Learning: Do different neural networks learn the same representations? arXiv, 1511.07543. Retrieved from https://arxiv.org/abs/1511.07543v3

Mishkin, D., & Matas, J. (2015). All you need is a good init. arXiv, 1511.06422. Retrieved from https://arxiv.org/abs/1511.06422v7

    - We propose a layer-sequential unit-variance (LSUV) initialization:
    - 1) use orthonormal matrices
    - 2) sequentially normalize the variance of the output of each layer to be equal to one

Novikov, A., Podoprikhin, D., Osokin, A., & Vetrov, D. (2015). Tensorizing Neural Networks. arXiv, 1509.06569. Retrieved from https://arxiv.org/abs/1509.06569v2

    - We convert FCN weight matrices to Tensor Train format (we call it TT-layer, TensorNet)
    - Number of parameters is reduced by a huge factor (up to 200000 times for VGG dense layers)
    - The expressive power is preserved

Peters, J., Bühlmann, P., & Meinshausen, N. (2015). Causal inference using invariant prediction: identification and confidence intervals. arXiv, 1501.01332. Retrieved from https://arxiv.org/abs/1501.01332v3

Pezzulo, G., Rigoli, F., & Friston, K. J. (2015). Active Inference, homeostatic regulation and adaptive behavioural control. Prog. Neurobiol. Retrieved from https://www.semanticscholar.org/paper/Active-Inference%2C-homeostatic-regulation-and-Pezzulo-Rigoli/c469f8a7f02015bc49e93df26c396228267a7e7b

Smith, L. N. (2015). Cyclical Learning Rates for Training Neural Networks. arXiv, 1506.01186. Retrieved from https://arxiv.org/abs/1506.01186v6

Srivastava, R. K., Greff, K., & Schmidhuber, J. (2015). Training Very Deep Networks. arXiv, 1507.06228. Retrieved from https://arxiv.org/abs/1507.06228v2

    - We propose Highway networks inspired by LSTM
    - They use adaptive gating units to regulate the information flow, we call such paths "information highways"
    - Even with hundreds of layers, highway networks can be trained directly through SGD

Tishby, N., & Zaslavsky, N. (2015). Deep Learning and the Information Bottleneck Principle. arXiv, 1503.02406. Retrieved from https://arxiv.org/abs/1503.02406v1

    - We suggest a novel information theoretic analysis of DNNs based on the information bottleneck (IB) principle
    - DNN can be quantified by the mutual information between the layers and the input and output variables
    - The goal of the network is to optimize the IB tradeoff between compression and prediction
    - We view DNNs as an successive (Markovian) relevant compression of the input
    - We discuss advantages of this new view

Wiatowski, T., & Bölcskei, H. (2015). A Mathematical Theory of Deep Convolutional Neural Networks for Feature Extraction. arXiv, 1512.06293. Retrieved from https://arxiv.org/abs/1512.06293v3

Wu, H., & Gu, X. (2015). Towards Dropout Training for Convolutional Neural Networks. arXiv, 1512.00242. Retrieved from https://arxiv.org/abs/1512.00242v1

    - For CNN, dropout is known to work well in fully-connected layers
    - However, its effect in convolutional and pooling layers is still not clear
    - We draw a connection between dropout before max-pooling and sampling from multinomial distribution
    - We propose probabilistic weighted pooling and achieve competitive results
    - We found that the effect of convolutional dropout is not trivial

Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A., Salakhutdinov, R., ...Bengio, Y. (2015). Show, Attend and Tell: Neural Image Caption Generation with Visual Attention. arXiv, 1502.03044. Retrieved from https://arxiv.org/abs/1502.03044v3

    - We introduce an attention based model for image captioning with either soft or hard attention
    - Soft deterministic attention is trainable by SGD
    - Hard stochastic attention is trainable by maximizing an approximate ELBO or equivalently by REINFORCE
    - We achieve SOTA on Flickr8k, Flickr30k and MS COCO

## 2016

Abdi, M., & Nahavandi, S. (2016). Multi-Residual Networks: Improving the Speed and Accuracy of Residual Networks. arXiv, 1609.05672. Retrieved from https://arxiv.org/abs/1609.05672v4

Alemi, A. A., Fischer, I., Dillon, J. V., & Murphy, K. (2016). Deep Variational Information Bottleneck. arXiv, 1612.00410. Retrieved from https://arxiv.org/abs/1612.00410v7

    - We propose Deep Variational Information Bottleneck (Deep VIB): a variational approximation to the information bottleneck that allows us to parameterize the information bottleneck model with a NN and efficiently train it
    - VIB objective outperform other forms of regularization in terms of generalization performance and robustness to adversarial attack
    - We describe a connection to VAE

Ba, J. L., Kiros, J. R., & Hinton, G. E. (2016). Layer Normalization. arXiv, 1607.06450. Retrieved from https://arxiv.org/abs/1607.06450v1

    - Problem: batch normalization is dependent on batch size and is not trivial to apply to RNNs
    - We propose a Layer Normalization that works independently for every training case
    - It performs the same computation at training and test times, and is also straightforward to apply to RNNs
    - Like in BN, we also use scale and shift, after the normalization but before the non-linearity
    - Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks

Baldassi, C., Gerace, F., Lucibello, C., Saglietti, L., & Zecchina, R. (2016). Learning may need only a few bits of synaptic precision. arXiv, 1602.04129. Retrieved from https://arxiv.org/abs/1602.04129v2

Bottou, L., Curtis, F. E., & Nocedal, J. (2016). Optimization Methods for Large-Scale Machine Learning. arXiv, 1606.04838. Retrieved from https://arxiv.org/abs/1606.04838v3

    - In this review we discuss the past, present, and future of numerical optimization in ML applications
    - We discuss two main streams of research:
    - 1) techniques that diminish noise in the stochastic directions
    - 2) methods that make use of second-order derivative approximations

Bruineberg, J., Kiverstein, J., & Rietveld, E. (2016). The anticipating brain is not a scientist: the free-energy principle from an ecological-enactive perspective. Synthese. Retrieved from https://www.semanticscholar.org/paper/The-anticipating-brain-is-not-a-scientist%3A-the-from-Bruineberg-Kiverstein/d923112dbf3c46d792b9d1172dd8fa69a68e3386

Chaudhari, P., Choromanska, A., Soatto, S., LeCun, Y., Baldassi, C., Borgs, C., ...Zecchina, R. (2016). Entropy-SGD: Biasing Gradient Descent Into Wide Valleys. arXiv, 1611.01838. Retrieved from https://arxiv.org/abs/1611.01838v5

    - Well-generalizable solutions lie in large flat regions with almost-zero eigenvalues in the Hessian
    - We propose Entropy-SGD optimizer that favors such solutions
    - Our algorithm resembles two nested loops of SGD
    - We use Langevin dynamics in the inner loop to compute the gradient of the local entropy
    - The new objective has a smoother energy landscape
    - Entropy-SGD obtains is comparable to competitive baselines and  gets a 2x speed-up over SGD

Chen, X., Kingma, D. P., Salimans, T., Duan, Y., Dhariwal, P., Schulman, J., ...Abbeel, P. (2016). Variational Lossy Autoencoder. arXiv, 1611.02731. Retrieved from https://arxiv.org/abs/1611.02731v2

Cichocki, A., Lee, N., Oseledets, I. V., Phan, A.-H., Zhao, Q., & Mandic, D. (2016). Low-Rank Tensor Networks for Dimensionality Reduction and Large-Scale Optimization Problems: Perspectives and Challenges PART 1. arXiv, 1609.00893. Retrieved from https://arxiv.org/abs/1609.00893v3

    - A book (pt. 1) about Tucker and Tensor Train (TT) decompositions and their extensions or generalizations
    - This can be used to convert intractable huge-scale optimization problems into a set of smaller problems
    - Chapter 1: Introduction and Motivation
    - Chapter 2: Tensor Operations and Tensor Network Diagrams
    - Chapter 3: Constrained Tensor Decompositions: From Two-way to Multiway Component Analysis
    - Chapter 4: Tensor Train Decompositions: Graphical Interpretations and Algorithms
    - Chapter 5: Discussion and Conclusions

Courbariaux, M., Hubara, I., Soudry, D., El-Yaniv, R., & Bengio, Y. (2016). Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1. arXiv, 1602.02830. Retrieved from https://arxiv.org/abs/1602.02830v3

Daniely, A., Frostig, R., & Singer, Y. (2016). Toward Deeper Understanding of Neural Networks: The Power of Initialization and a Dual View on Expressivity. Advances in Neural Information Processing Systems, 29. Retrieved from https://proceedings.neurips.cc/paper_files/paper/2016/hash/abea47ba24142ed16b7d8fbf2c740e0d-Abstract.html

    - We introduce the notion of a computation skeleton, an acyclic graph that can describe NN computations
    - A skeleton defines a class H of functions obtained from the skeleton’s structure
    - We analyze the set of functions that can be expressed by varying the weights of the last layer
    - (in this case the objective is convex)
    - We show that with high probability over the choice of initial network weights, any function in H can be approximated by selecting the final layer’s weights
    - It follows that random weight initialization often yields a favorable starting point for optimization

Duvenaud, D., Maclaurin, D., & Adams, R. (2016). Early Stopping as Nonparametric Variational Inference. Artificial Intelligence and Statistics. PMLR. Retrieved from https://proceedings.mlr.press/v51/duvenaud16.html

    - We propose a Bayesian interpretation of SGD
    - Unconverged SGD yields a sequence of distributions which are variational approximations to the true posterior
    - This allows us to estimate a lower bound on the marginal likelihood of any model, even very large
    - This can be used for hyperparameter selection and early stopping without a validation set
    - The results are promising, but further refinements are likely to be necessary

Edwards, H., & Storkey, A. (2016). Towards a Neural Statistician. arXiv, 1606.02185. Retrieved from https://arxiv.org/abs/1606.02185v2

    - We take seriously the idea of working with datasets, rather than datapoints, as the key objects to model
    - We demonstrate an extension to VAE called a neural statistician
    - It is trained to produce statistics that encapsulate a generative model for each dataset
    - It unsupervisedly learns representations, or statistics, of datasets
    - This enables efficient few-shot learning from new datasets

Freeman, C. D., & Bruna, J. (2016). Topology and Geometry of Half-Rectified Network Optimization. arXiv, 1611.01540. Retrieved from https://arxiv.org/abs/1611.01540v4

    - We prove that single layer ReLU networks are asymptotically connected
    - We show that level sets remain connected throughout all the learning phase, suggesting a near convex behavior, but they become exponentially more curvy as the energy level decays

Friston, K. J., FitzGerald, T., Rigoli, F., Schwartenbeck, P., O'Doherty, J., & Pezzulo, G. (2016). Active inference and learning. Neurosci. Biobehav. Rev. Retrieved from https://www.semanticscholar.org/paper/Active-inference-and-learning-Friston-FitzGerald/3b3903f7914483e21576f9d098e611deef95ec45

Friston, K. J. (2016). I am therefore I think. Retrieved from https://www.semanticscholar.org/paper/I-am-therefore-I-think-Friston/2d450521f168fccbc3cd13112ca07159c7f1bd50

Hardt, M., & Ma, T. (2016). Identity Matters in Deep Learning. arXiv, 1611.04231. Retrieved from https://arxiv.org/abs/1611.04231v3

    - We give a strikingly simple proof that arbitrarily deep LINEAR ResNets have no spurious local optima
    - (the same result for standard linear DNNs is substantially more delicate)
    - We show that ResNets with ReLU can express a dataset of size N with R classes given O(n log n + r^2) parameters
    - We show SOTA on all-convolutional networks (no batch norm, dropout, or max pool) on CIFAR10, CIFAR100, ImageNet

Higgins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X., Botvinick, M., ...Lerchner, A. (2016). beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework. OpenReview. Retrieved from https://openreview.net/pdf?id=Sy2fzU9gl

    - We propose beta-VAE, a modification of VAE that relies on tuning a single hyperparameter beta
    - Beta can be tuned using weakly labelled data or through heuristic visual inspection
    - Beta-VAE reduces to VAE if beta = 1
    - beta-VAE allows to discover interpretable factorised latent representations from images without supervision
    - It achieves SOTA on disentangled factor learning on a variety of datasets (celebA, faces and chairs)
    - We devise a protocol to quantitatively compare the degree of disentanglement learnt by different models

Huang, G., Sun, Y., Liu, Z., Sedra, D., & Weinberger, K. (2016). Deep Networks with Stochastic Depth. arXiv, 1603.09382. Retrieved from https://arxiv.org/abs/1603.09382v3

Hohwy, J. (2016). The self-evidencing brain. Noûs. Retrieved from https://www.semanticscholar.org/paper/The-self-evidencing-brain-Hohwy/01aa6ef498431fb8c6d45e9375bb39a7a923b9bb

Im, D. J., Tao, M., & Branson, K. (2016). An empirical analysis of the optimization of deep network loss surfaces. arXiv, 1612.04010. Retrieved from https://arxiv.org/abs/1612.04010v4

    - We visualize the loss function by projecting them down to low-dimensional spaces
    - We show that optimization algorithms encounter and choose different descent directions at many saddle points
    - We hypothesize that each optimization algorithm makes characteristic choices at these saddle points

Jang, E., Gu, S., & Poole, B. (2016). Categorical Reparameterization with Gumbel-Softmax. arXiv, 1611.01144. Retrieved from https://arxiv.org/abs/1611.01144v5

    - Stochastic NNs rarely use categorical latent variables due to the inability to backpropagate through samples
    - We introduce Gumbel-Softmax, a continuous distribution on the simplex that can approximate categorical samples
    - Its gradients can be easily computed via the reparameterization trick
    - It can be used to efficiently train semi-supervised models

Kaiser, Ł., & Bengio, S. (2016). Can Active Memory Replace Attention? arXiv, 1610.08613. Retrieved from https://arxiv.org/abs/1610.08613v2

Kawaguchi, K. (2016). Deep Learning without Poor Local Minima. arXiv, 1605.07110. Retrieved from https://arxiv.org/abs/1605.07110v3

    - We prove the following statements for linear DNNs  with squared loss function:
    - 1) the function is non-convex and non-concave
    - 2) there are only global minima and saddle points, no local minima
    - 3) there exist "bad" saddle points (where the Hessian has no negative eigenvalue) for deep networks
    - The bad local minima would arise in a deep nonlinear model but only as an effect of adding nonlinearities

Keskar, N. S., Mudigere, D., Nocedal, J., Smelyanskiy, M., & Tang, P. T. P. (2016). On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima. arXiv, 1609.04836. Retrieved from https://arxiv.org/abs/1609.04836v2

    -  It has been observed that large batch size leads to degradation in the quality of the model
    -  We show that large-batch methods tend to converge to sharp minima with poor generalization
    -  Small-batch methods consistently converge to flat minima due to the inherent noise in the gradient estimation
    -  We discuss strategies to improve large-batch training

Kim, B., Khanna, R., & Koyejo, O. O. (2016). Examples are not enough, learn to criticize! Criticism for Interpretability. Advances in Neural Information Processing Systems, 29. Retrieved from https://papers.nips.cc/paper_files/paper/2016/hash/5680522b8e2bb01943234bce7bf84534-Abstract.html

    - Relying only on examples to explain the models’ behavior can lead to misunderstanding
    - It is also important to signify "criticism samples", for which prototypical examples do not provide good explanations
    - We develop MMD-critic motivated by the Bayesian model criticism (BMC) framework
    - It calculates the maximum mean discrepancy (MMD) as a measure of similarity between points and potential prototypes, and 1) selects prototypes that maximize the statistic, and 2) selects criticism samples i.e. samples that are not well-explained by the prototypes (sec. 3)
    - We also evaluate the performance of MMD-critic as a nearest prototype classifier, and show that it achieves comparable performance to existing methods

Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G., Rusu, A. A., ...Hadsell, R. (2016). Overcoming catastrophic forgetting in neural networks. arXiv, 1612.00796. Retrieved from https://arxiv.org/abs/1612.00796v2

    - It was prevously shown that "catastrophic forgetting" occurs when the network is trained sequentially on multiple tasks because the weights in the network that are important for task A are changed to meet the objectives of task B. One can avoid this by interleaving data from multiple tasks, however, for agents this would require a large episodic memory system that will replay the data during training.
    - Humans and other animals appear to be able to learn in a continual fashion by protecting previously-acquired knowledge in neocortical circuits.
    - We propose elastic weight consolidation (EWC) method that ensures task A is remembered whilst training on task B. The posterior probability of weights given the dataset A must contain information about which parameters were important to task A and is therefore the key to implementing EWC. The true posterior probability is intractable, so, we use Laplace approximation. We approximate the posterior as a Gaussian distribution with mean given by the MLE parameters A and a diagonal precision given by the diagonal of the Fisher information matrix F, which is equivalent to the second derivative of the loss near a minimum and can be easily computed from first-order derivatives alone. Based on this, the EWC objective contains a regularizer shown in eq. 3 (when learning task B after task A). When further moving to a third task C, EWC can be enforced either with two separate penalties (for A and B), or as one by noting that the sum of two quadratic penalties is itself a quadratic penalty.
    - With EWC, three values have to be stored for each synapse: the weight itself, its variance and its mean. So, the perspective we offer here aligns with a recent proposal that each synapse not only stores its current weight, but also an implicit representation of its uncertainty about that weight.

Lakshminarayanan, B., Pritzel, A., & Blundell, C. (2016). Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles. arXiv, 1612.01474. Retrieved from https://arxiv.org/abs/1612.01474v3   
  
    - We propose non-Bayesian deep ensembles  
    - They are an alternative to Bayesian NNs (e.g. variational inference or MCMC methods)  
    - They are simple, parallelizable and yields high quality  
    - They produce well-calibrated uncertainty estimates  
    - They express higher uncertainty on OOD examples

Li, S., Jiao, J., Han, Y., & Weissman, T. (2016). Demystifying ResNet. arXiv, 1611.01186. Retrieved from https://arxiv.org/abs/1611.01186v2

    - It was empirically observed that shortcuts that have depth 2 results in smaller training error, while it is not true for shortcut of depth 1 or 3
    - We prove that shortcuts that have depth 2 yields depth-invariant condition number of the Hessian
    - Shortcuts of higher depth result in an extremely flat (high-order) stationary point initially
    - The shortcut 1 has a condition number exploding to infinity as the number of layers grows
    - We experimentally show that initializing the network to small weights with shortcut 2 achieves significantly better results than random Gaussian Xavier initialization, orthogonal initialization, and shortcuts of deeper depth

Liao, Q., & Poggio, T. (2016). Bridging the Gaps Between Residual Learning, Recurrent Neural Networks and Visual Cortex. arXiv, 1604.03640. Retrieved from https://arxiv.org/abs/1604.03640v2

Loshchilov, I., & Hutter, F. (2016). SGDR: Stochastic Gradient Descent with Warm Restarts. arXiv, 1608.03983. Retrieved from https://arxiv.org/abs/1608.03983v5

    - We propose a simple warm restart technique for SGD
    - With this technique we demonstate SOTA on CIFAR-10 and CIFAR-100 datasets
    - Future work should consider warm restarts for other algorithms such as Adam

Novikov, A., Trofimov, M., & Oseledets, I. (2016). Exponential Machines. arXiv, 1605.03795. Retrieved from https://arxiv.org/abs/1605.03795v3

    - Exponential Machines (ExM), a predictor that models all interactions of every order
    - The Tensor Train format regularizes an exponentially large tensor of parameters
    - SOTA performance on synthetic data with high-order interactions

Poole, B., Lahiri, S., Raghu, M., Sohl-Dickstein, J., & Ganguli, S. (2016). Exponential expressivity in deep neural networks through transient chaos. arXiv, 1606.05340. Retrieved from https://arxiv.org/abs/1606.05340v2

Rolfe, J. T. (2016). Discrete Variational Autoencoders. arXiv, 1609.02200. Retrieved from https://arxiv.org/abs/1609.02200v2

Sagun, L., Bottou, L., & LeCun, Y. (2016). Eigenvalues of the Hessian in Deep Learning: Singularity and Beyond. arXiv, 1611.07476. Retrieved from https://arxiv.org/abs/1611.07476v2

    - We show that the eigenvalue distribution in NNs is seen to be composed of two parts:
    - 1) the bulk which is concentrated around zero and indicating how overparametrized the system is
    - 2) the edges which are scattered away from zero and depend on the input data
    - (this paper seems to be an early version of https://arxiv.org/abs/1706.04454v3)

Salimans, T., & Kingma, D. P. (2016). Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks. arXiv, 1602.07868. Retrieved from https://arxiv.org/abs/1602.07868v3

    - a weight that decouples the length of vectors from their direction
    - we improve the conditioning of the optimization problem and we speed up convergence of SGD
    - is inspired by batch normalization but is more widely applicable
    - useful in supervised image recognition, generative modelling, and deep RL

Scellier, B., & Bengio, Y. (2016). Equilibrium Propagation: Bridging the Gap Between Energy-Based Models and Backpropagation. arXiv, 1602.05179. Retrieved from https://arxiv.org/abs/1602.05179v5

    - We introduce Equilibrium Propagation, a learning framework for energy-based models
    - It involves computing the gradient of an objective, but is different from backpropagation
    - The first phase is when the prediction is made, and the second phase is when the target is revealed
    - The only local difference between the two phases is whether synaptic changes are allowed or not

Stoudenmire, E. M., & Schwab, D. J. (2016). Supervised Learning with Quantum-Inspired Tensor Networks. arXiv, 1605.05775. Retrieved from https://arxiv.org/abs/1605.05775v2

    - Algorithms for optimizin Tensor networks can be adapted to supervised learning tasks
    - One proposed solution is active memory: it allows the model to access and change all its memory
    - We present an extension of the Neural GPU model that yields good results for NMT
    - We clarify why the previous active memory model did not succeed on machine translation

Ulyanov, D., Vedaldi, A., & Lempitsky, V. (2016). Instance Normalization: The Missing Ingredient for Fast Stylization. arXiv, 1607.08022. Retrieved from https://arxiv.org/abs/1607.08022v3

Xie, B., Liang, Y., & Song, L. (2016). Diverse Neural Network Learns True Target Functions. arXiv, 1611.03131. Retrieved from https://arxiv.org/abs/1611.03131v3

    - Do stationary points of the loss function learn the true target function (that is, generalize)?
    - We analyze one-hidden-layer ReLU neural networks
    - We show that neural networks with diverse units have no spurious local minima: the more diverse the unit weights, the more likely stationary points will result in small training loss and generalization error
    - We suggest a novel regularization function to promote unit diversity for potentially better generalization

Zhang, C., Bengio, S., Hardt, M., Recht, B., & Vinyals, O. (2016). Understanding deep learning requires rethinking generalization. arXiv, 1611.03530. Retrieved from https://arxiv.org/abs/1611.03530v2

    - We show that SOTA CNNs trained with SGD easily fit a random labeling and/or images of random noise
    - For random labeling, training time increases only by a small constant factor
    - This is unaffected by explicit regularization
    - Regularization such as weight decay, dropout, and data augmentation, do not adequately explain the generalization error of neural networks, it plays a rather different role in deep learning
    - We prove that simple two-layer ReLU of linear size can already represent any labeling of the training data
    - Appealing to linear models, we analyze how SGD acts as an implicit regularizer: for linear models, SGD always converges to a solution with small norm, hence, the algorithm itself is implicitly regularizing the solution
    - We suggest that more investigation is needed

## 2017

Arpit, D., Jastrzębski, S., Ballas, N., Krueger, D., Bengio, E., Kanwal, M. S., ...Lacoste-Julien, S. (2017). A Closer Look at Memorization in Deep Networks. arXiv, 1706.05394. Retrieved from https://arxiv.org/abs/1706.05394v2

    - We conduct a series of experiments that contrast the learning dynamics of DNNs on real vs. noise data
    - DNNs learn simple patterns first, before memorizing, taking advantage of patterns shared by multiple examples
    - For appropriately tuned explicit regularization (e.g., dropout) we can degrade DNN training performance on noise datasets without compromising generalization on real data

Balan, R., Singh, M., & Zou, D. (2017). Lipschitz Properties for Deep Convolutional Networks. arXiv, 1701.05217. Retrieved from https://arxiv.org/abs/1701.05217v1

    - We hope to see a small change in the feature vector with respect to a deformation on the input signal
    - The key step is to derive the Lipschitz properties, tt is desired to have a formula for the Lipschitz bound
    - We compare different methods for computing the Lipschitz constants

Bartlett, P., Foster, D. J., & Telgarsky, M. (2017). Spectrally-normalized margin bounds for neural networks. arXiv, 1706.08498. Retrieved from https://arxiv.org/abs/1706.08498v2

    - We calculate DNN's Lipschitz constant: the product of the spectral norms of the weight matrices
    - It highly correlates with a generalization bound (difference between train and test scores?)
    - We experiment on data with original or random labels
    - SGD selects predictors whose complexity scales with the difficulty of the learning task

Bengio, Y. (2017). The Consciousness Prior. arXiv, 1709.08568. Retrieved from https://arxiv.org/abs/1709.08568v2

Bojarski, M., Yeres, P., Choromanska, A., Choromanski, K., Firner, B., Jackel, L., & Muller, U. (2017). Explaining How a Deep Neural Network Trained with End-to-End Learning Steers a Car. arXiv, 1704.07911. Retrieved from https://arxiv.org/abs/1704.07911v1

Brutzkus, A., Globerson, A., Malach, E., & Shalev-Shwartz, S. (2017). SGD Learns Over-parameterized Networks that Provably Generalize on Linearly Separable Data. arXiv, 1710.10174. Retrieved from https://arxiv.org/abs/1710.10174v1

    - Current generalization bounds for NNs fail to explain good generalization in the over-parameterized regime
    - We study two-layer NN with Leaky ReLU trained with SGD on linearly separable data
    - Will a large network will overfit in such a case or not?
    - SGD both finds a global minimum, and avoids overfitting despite the high capacity
    - For this case, we prove convergence rates of SGD to a global minimum independent of the network size
    - This is the first theoretical demonstration that SGD can avoid overfitting

Chang, B., Meng, L., Haber, E., Ruthotto, L., Begert, D., & Holtham, E. (2017). Reversible Architectures for Arbitrarily Deep Residual Neural Networks. arXiv, 1709.03698. Retrieved from https://arxiv.org/abs/1709.03698v2

Cichocki, A., Phan, A.-H., Zhao, Q., Lee, N., Oseledets, I. V., Sugiyama, M., & Mandic, D. (2017). Tensor Networks for Dimensionality Reduction and Large-Scale Optimizations. Part 2 Applications and Future Perspectives. arXiv, 1708.09165. Retrieved from https://arxiv.org/abs/1708.09165v1

    -  A book (pt. 2) about tensor network models for super-compressed representation of data/parameters
    -  Emphasis is on the tensor train (TT) and Hierarchical Tucker (HT) decompositions
    -  Applied areas: regression and classification, eigenvalue decomposition, Riemannian optimization, DNNs
    -  Part 1 and Part 2 of this work can be used either as stand-alone separate texts

Dinh, L., Pascanu, R., Bengio, S., & Bengio, Y. (2017). Sharp Minima Can Generalize For Deep Nets. arXiv, 1703.04933. Retrieved from https://arxiv.org/abs/1703.04933v2

    - We argue that most notions of flatness are problematic for deep models to explain generalization
    - Given ReLU DNN with flat minima, we can build equivalent model corresponding to arbitrarily sharper minima using the inherent symmetries of the architecture
    - If we allow to reparametrize a function, the geometry of its parameters can change drastically without affecting its generalization properties

Du, S. S., Lee, J. D., Tian, Y., Poczos, B., & Singh, A. (2017). Gradient Descent Learns One-hidden-layer CNN: Don't be Afraid of Spurious Local Minima. arXiv, 1712.00779. Retrieved from https://arxiv.org/abs/1712.00779v2

    - We consider 1-layer CNN with ReLU and take the same network with fixed weights as target function
    - We prove that with Gaussian input, there is a spurious local minimizer
    - GD with weight normalization converges to global minimum or to bad local minimum
    - We see that GD dynamics has two phases: it starts off slow, but converges much faster after several iterations

George, D., Lehrach, W., Kansky, K., Lázaro-Gredilla, M., Laan, C., Marthi, B., ...Phoenix, D. (2017). A generative vision model that trains with high data efficiency and breaks text-based CAPTCHAs. Science. Retrieved from https://www.cs.jhu.edu/~ayuille/JHUcourses/ProbabilisticModelsOfVisualCognition2019/Lec23/nov19lecture/GeorgeCAPCHAS.pdf

    - We introduce a hierarchical probabilistic generative model called the Recursive Cortical Network (RCN)
    - RCN incorporates message-passing based inference based on neuroscience insights
    - RCN handles recognition, segmentation and reasoning
    - To parse a scene, RCN maintains hierarchical graphs for object instances at multiple locations
    - A backward pass can reject many object hypotheses that were falsely identified in the forward pass
    - It outperforms DNNs on a scene text recognition benchmark while being 300-fold more data efficient
    - It breaks the defense of modern text-based CAPTCHAs

Gomez, A. N., Ren, M., Urtasun, R., & Grosse, R. B. (2017). The Reversible Residual Network: Backpropagation Without Storing Activations. arXiv, 1707.04585. Retrieved from https://arxiv.org/abs/1707.04585v1

Goyal, P., Dollár, P., Girshick, R., Noordhuis, P., Wesolowski, L., Kyrola, A., ...He, K. (2017). Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour. arXiv, 1706.02677. Retrieved from https://arxiv.org/abs/1706.02677v2

    - We adopt a linear scaling rule for adjusting learning rates as a function of minibatch size
    - We develop a new warmup scheme that overcomes optimization challenges early in training
    - Our implementation achieves ∼90% scaling efficiency when moving from 8 to 256 GPUs

Guerguiev, J., Lillicrap, T. P., & Richards, B. A. (2017). Towards deep learning with segregated dendrites. eLife. Retrieved from https://elifesciences.org/articles/22901

Guo, C., Pleiss, G., Sun, Y., & Weinberger, K. Q. (2017). On Calibration of Modern Neural Networks. arXiv, 1706.04599. Retrieved from https://arxiv.org/abs/1706.04599v2

Hoffer, E., Hubara, I., & Soudry, D. (2017). Train longer, generalize better: closing the generalization gap in large batch training of neural networks. arXiv, 1705.08741. Retrieved from https://arxiv.org/abs/1705.08741v2

    - It has been observed that large batch sizes lead to degradation in generalization performance. This remained true even when the models were trained without any budget or limits, until the loss function ceased to improve. This decrease in performance has been named the "generalization gap".
    - We find that the weight distance from its initialization grows logarithmically with the number of weight updates.
    - Our analysis shows that the initial training phase with high LR enables the model to reach farther locations in the parameter space, which may be necessary to find wider local minima and better generalization.
    - We propose Ghost Batch Normalization (GBN). It acquires the statistics on small virtual ("ghost") batches instead of the real large batch. We note that in a multi-device distributed setting, some of the benefits of "Ghost BN" may already occur, since BN is often preformed on each device separately.
    - We also propose to either increase the LR by the square root of the mini-batch size, or add multiplicative noise to the gradient estimate (both methods yielded similar performance). For the first few iterations, we had to clip or normalize the gradients to prevent divergence.
    - By these LR and BN adjustments, the generalization gap can be significantly decreased (fig. 2). It is important to note that other types of noise (e.g., dropout, dropconnect, label noise) change the structure of the covariance matrix and do not help to reduce the generalization gap.
    - We noticed that the distance between the current weight and the initialization point can be a good measure to decide upon when to decrease the LR, and this is different from common practices. There is a long-held belief that the optimization process should not be allowed to decrease the training error when validation error "flatlines". However, we observed that substantial improvement to the final accuracy can be obtained by continuing the optimization using the same LR even if the training error decreases while the validation plateaus. Subsequent LR drops resulted with a sharp validation error decrease, and better generalization for the final model. These observations led us to believe that "generalization gap" phenomenon stems from the relatively small number of updates rather than the batch size.
    - So, we show that there is no inherent generalization gap problem, if adapting our methods, and the number of iterations is sufcicient. However, this opens the another issue: can we speed up training by using large batch sizes? After out publication it was shown that, indeed, we can (see "Accurate, large minibatch sgd: Training imagenet in 1 hour")

Huang, G., Li, Y., Pleiss, G., Liu, Z., Hopcroft, J. E., & Weinberger, K. Q. (2017). Snapshot Ensembles: Train 1, get M for free. arXiv, 1704.00109. Retrieved from https://arxiv.org/abs/1704.00109v1

    - We train a single neural network, converging to several local minima along its optimization path
    - We leverage recent work on cyclic learning rate schedules to obtain repeated rapid convergence
    - The resulting technique, Snapshot Ensembling, compares favorably with traditional network ensembles

Huang, F., Ash, J., Langford, J., & Schapire, R. (2017). Learning Deep ResNet Blocks Sequentially using Boosting Theory. arXiv, 1706.04964. Retrieved from https://arxiv.org/abs/1706.04964v4

Ioffe, S. (2017). Batch Renormalization: Towards Reducing Minibatch Dependence in Batch-Normalized Models. arXiv, 1702.03275. Retrieved from https://arxiv.org/abs/1702.03275v2

Jain, P., Kakade, S. M., Kidambi, R., Netrapalli, P., & Sidford, A. (2017). Accelerating Stochastic Gradient Descent For Least Squares Regression. arXiv, 1704.08227. Retrieved from https://arxiv.org/abs/1704.08227v2

    - We discuss least squares regression as a special case of stochastic approximation where we have access to a stochastic first order oracle (stochastic gradient)
    - We rethink what acceleration (momentum?) has to offer when working with a stochastic gradient
    - We propose an accelerated stochastic gradient method (ASGD)
    - This paper presents the first known provable analysis of the claim that fast gradient methods are stable when dealing with statistical errors, in contrast to previous negative results

Jastrzębski, S., Kenton, Z., Arpit, D., Ballas, N., Fischer, A., Bengio, Y., & Storkey, A. (2017). Three Factors Influencing Minima in SGD. arXiv, 1711.04623. Retrieved from https://arxiv.org/abs/1711.04623v3

    - We investigate the previously proposed approximation of SGD by a stochastic differential equation (SDE)
    - Higher values of the ratio of learning rate to batch size lead to wider minima and often better generalization
    - We study this theoretically and confirm experimentally
    - Learning rate schedules can be replaced with batch size schedules

Kaiser, L., Gomez, A. N., Shazeer, N., Vaswani, A., Parmar, N., Jones, L., & Uszkoreit, J. (2017). One Model To Learn Them All. arXiv, 1706.05137. Retrieved from https://arxiv.org/abs/1706.05137v1

Kaiser, Ł., Nachum, O., Roy, A., & Bengio, S. (2017). Learning to Remember Rare Events. arXiv, 1703.03129. Retrieved from https://arxiv.org/abs/1703.03129v1

    - How to make memory-augmented DNNs better at life-long and one-shot learning of rare events?
    - We present a large-scale life-long memory module that exploits fast nearest-neighbor algorithms
    - It provide the ability to remember and do life-long one-shot learning: it can remember training examples shown many thousands of steps in the past and it can successfully generalize from them
    - We try it with RNNs and image classification CNNs
    - We set a new SOTA for one-shot learning on the Omniglot dataset

Khrulkov, V., & Oseledets, I. (2017). Art of singular vectors and universal adversarial perturbations. arXiv, 1709.03582. Retrieved from https://arxiv.org/abs/1709.03582v2

    -  We propose a new algorithm for constructing adversarial perturbations
    -  To do this we compute (p, q)-singular vectors of the Jacobian matrices of hidden layers of a network

Khrulkov, V., Novikov, A., & Oseledets, I. (2017). Expressive power of recurrent neural networks. arXiv, 1711.00811. Retrieved from https://arxiv.org/abs/1711.00811v2

    - As known, deep Hierarchical Tucker CNNs have exponentially higher expressive power than shallow networks
    - We prove the same for RNNs with Tensor Train (TT) decomposition
    - We compare expressive powers of the HT- and TT-Networks
    - We implement the recurrent TT-Networks and provide numerical evidence of their expressivity

Klambauer, G., Unterthiner, T., Mayr, A., & Hochreiter, S. (2017). Self-Normalizing Neural Networks. arXiv, 1706.02515. Retrieved from https://arxiv.org/abs/1706.02515v5

    - We propose self-normalizing neural networks (SNNs) with SELU activation function
    - Given some (not very realistic) assumptions about input independence we prove that in SNNs activations close to zero mean and unit variance that are propagated through many network layers will converge towards zero mean and unit variance, and vanishing and exploding gradients are impossible
    - This enables to train FCNs with many layers
    - SNNs significantly outperformed all competing FNN methods at 121 UCI tasks

Laurent, T., & Brecht, J. (2018). Deep Linear Networks with Arbitrary Loss: All Local Minima Are Global. International Conference on Machine Learning. PMLR. Retrieved from https://proceedings.mlr.press/v80/laurent18a.html

    - Consider a DNN with the following properties:
    - 1) Arbitrary convex differentiable loss
    - 2) Hidden layers are either at least as wide as the input layer, or at least as wide as the output layer
    - We proof that in this case all local minima are global minima
    - If the loss is convex and Lipschitz but not differentiable then DNN can have sub-optimal local minima

Lee, J., Bahri, Y., Novak, R., Schoenholz, S. S., Pennington, J., & Sohl-Dickstein, J. (2017). Deep Neural Networks as Gaussian Processes. arXiv, 1711.00165. Retrieved from https://arxiv.org/abs/1711.00165v3

    - It is known that shallow infinite-width NNs are equivalent to gaussian processes (GP)
    - We derive the exact equivalence between infinitely wide deep networks and GPs
    - We develop a computationally efficient pipeline to compute the covariance function for these GPs
    - We use the resulting NNGPs to perform Bayesian inference for wide DNNs on MNIST and CIFAR10
    - GP predictions typically outperform those of finite-width networks
    - One benefit in using a GP is that, due to its Bayesian nature, all predictions have uncertainty estimates
    - We observe that the NNGP uncertainty estimate is highly correlated with prediction error
    - We intend to look into scalability for larger learning tasks

Li, H., Xu, Z., Taylor, G., Studer, C., & Goldstein, T. (2017). Visualizing the Loss Landscape of Neural Nets. arXiv, 1712.09913. Retrieved from https://arxiv.org/abs/1712.09913v3

    - Simple visualization strategies fail to accurately capture the local geometry
    - We present a visualization method based on "filter normalization"
    - When networks become deep, loss surface turns from convex to chaotic, but skip connections prevent this
    - We measure non-convexity by calculating eigenvalues of the Hessian around local minima
    - We show that SGD optimization trajectories lie in an extremely low dimensional space
    - This can be explained by the presence of large, nearly convex regions in the loss landscape

Liu, T., Lugosi, G., Neu, G., & Tao, D. (2017). Algorithmic stability and hypothesis complexity. arXiv, 1702.08712. Retrieved from https://arxiv.org/abs/1702.08712v2

Loshchilov, I., & Hutter, F. (2017). Decoupled Weight Decay Regularization. arXiv, 1711.05101. Retrieved from https://arxiv.org/abs/1711.05101v3

    - Common implementations of adaptive GD algorithms employ L2 regularization, often calling it "weight decay"
    - However, weight decay is equivalent to L2 regularization only for standard SGD
    - We propose to decouple the weight decay from the optimization steps taken w.r.t. the loss function
    - It decouples the optimal choice of weight decay factor from the setting of the LR for SGD and Adam
    - It substantially improves Adam’s generalization performance (AdamW), allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter)

Lu, Y., Zhong, A., Li, Q., & Dong, B. (2017). Beyond Finite Layer Neural Networks: Bridging Deep Architectures and Numerical Differential Equations. arXiv, 1710.10121. Retrieved from https://arxiv.org/abs/1710.10121v3

#training Ma, S., Bassily, R., & Belkin, M. (2017). The Power of Interpolation: Understanding the Effectiveness of SGD in Modern Over-parametrized Learning. arXiv, 1712.06559. Retrieved from https://arxiv.org/abs/1712.06559v3

    - We study why SGD converges so fast
    - We show that there is a critical batch size m∗ such that:
    - 1) SGD iteration with mini-batch size m ≤ m∗ is nearly equivalent to m iterations of mini-batch size 1 (linear scaling regime). Doubling the batch size in this regime will roughly halve the number of iterations needed
    - 2) SGD iteration with mini-batch m > m∗ is nearly equivalent to a full gradient descent iteration (saturation regime). Increasing batch size in this regime becomes much less beneficial
    - A critical batch size is nearly independent of the data size

Mahsereci, M., Balles, L., Lassner, C., & Hennig, P. (2017). Early Stopping without a Validation Set. arXiv, 1703.09580. Retrieved from https://arxiv.org/abs/1703.09580v3

    - We propose a cheap and scalable early stopping criterion based on local statistics of the gradients
    - It does not require a validation set, thus enabling the optimizer to use all available training data
    - We test on linear and MLP models

Mallya, A., & Lazebnik, S. (2017). PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning. arXiv, 1711.05769. Retrieved from https://arxiv.org/abs/1711.05769v2

Molchanov, D., Ashukha, A., & Vetrov, D. (2017). Variational Dropout Sparsifies Deep Neural Networks. arXiv, 1701.05369. Retrieved from https://arxiv.org/abs/1701.05369v3

    - We extend a recently proposed Variational Dropout to the case when dropout rates are unbounded
    - We propose a way to reduce the variance of the gradient estimator
    - Individual dropout rates per weight leads to extremely sparse solutions both in FCNs and CNNs
    - This effect is similar to automatic relevance determination
    - We reduce the number of parameters up to 68 times on VGG with a negligible decrease of accuracy

Nguyen, Q., & Hein, M. (2017). The loss surface of deep and wide neural networks. arXiv, 1704.08045. Retrieved from https://arxiv.org/abs/1704.08045v2

    - Consider a FCN with the following properties:
    - 1) squared loss
    - 2) analytic activation function
    - 3) the number of hidden units of one layer is larger than the number of training points
    - 4) the network structure from this layer on is pyramidal
    - We prove that in this case all local minima are global

Nickel, M., & Kiela, D. (2017). Poincaré Embeddings for Learning Hierarchical Representations. arXiv, 1705.08039. Retrieved from https://arxiv.org/abs/1705.08039v2

    - Problem: no method yet exists that is able to compute embeddings of large graph-structured data – such as social networks, knowledge graphs or taxonomies – without loss of information
    - We propose to compute embeddings in hyperbolic space (the Poincaré ball)
    - Informally, hyperbolic space can be thought of as a continuous version of trees
    - It is naturally equipped to model hierarchical structures
    - Poincaré embeddings are successful in lexical entailment on WordNet and in predicting links in graphs where they outperform Euclidean embeddings, especially in low dimensions

Oord, A. v. d., Vinyals, O., & Kavukcuoglu, K. (2017). Neural Discrete Representation Learning. arXiv, 1711.00937. Retrieved from https://arxiv.org/abs/1711.00937v2

Peng, K.-C., Wu, Z., & Ernst, J. (2017). Zero-Shot Deep Domain Adaptation. arXiv, 1707.01922. Retrieved from https://arxiv.org/abs/1707.01922v5

Pennington, J., Schoenholz, S. S., & Ganguli, S. (2017). Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice. arXiv, 1711.04735. Retrieved from https://arxiv.org/abs/1711.04735v1

    - We compute analytically the entire singular value distribution of a DNN’s input-output Jacobian
    - ReLU networks are incapable of dynamical isometry (see https://arxiv.org/abs/1312.6120)
    - Sigmoidal networks with orthogonal weight initialization can achieve isometry and outperform ReLU nets
    - DNNs achieving dynamical isometry learn orders of magnitude faster than networks that do not

Ramstead, M., Badcock, P. B., & Friston, K. J. (2017). Answering Schrödinger's question: A free-energy formulation. Phys. Life Rev. Retrieved from https://www.semanticscholar.org/paper/Answering-Schr%C3%B6dinger's-question%3A-A-free-energy-Ramstead-Badcock/cbf4040cb14a019ff3556fad5c455e99737f169f

Reddi, S. J., Zaheer, M., Sra, S., Poczos, B., Bach, F., Salakhutdinov, R., & Smola, A. J. (2017). A Generic Approach for Escaping Saddle points. arXiv, 1709.01434. Retrieved from https://arxiv.org/abs/1709.01434v1

    - Problem: second-order methods effectively escape saddle points, but require expensive Hessian-based computations
    - We alternate between a first-order and a second-order optimizers, using the latter only close to saddle points
    - This minimizes Hessian-based computations and yields convergence results competitive to SOTA

Sagun, L., Evci, U., Guney, V. U., Dauphin, Y., & Bottou, L. (2017). Empirical Analysis of the Hessian of Over-Parametrized Neural Networks. arXiv, 1706.04454. Retrieved from https://arxiv.org/abs/1706.04454v3

    - In DL, we empirically show that the spectrum of the Hessian is composed of two parts:
    - 1) The bulk centered near zero, and 2) outliers away from the bulk
    - We analyze sevaral findings, hoping it would shed light on the geometry of high-dimensional non-convex spaces
    - Small and large batch gradient descent appear to converge to different basins of attraction but we show that they are in fact connected through their flat region and so belong to the same basin

Safran, I., & Shamir, O. (2017). Spurious Local Minima are Common in Two-Layer ReLU Neural Networks. arXiv, 1712.08968. Retrieved from https://arxiv.org/abs/1712.08968v3

    - We consider ReLU networks with one hidden layer and squared loss
    - We study spurious local minima in this case (not clearly described). The results imply that in high input dimensions, nearly all target networks (what is this?) of the relevant sizes lead to spurious local minima

Shalev-Shwartz, S., Shamir, O., & Shammah, S. (2017). Failures of Gradient-Based Deep Learning. arXiv, 1703.07950. Retrieved from https://arxiv.org/abs/1703.07950v2

    - We describe 4 types of simple problems, for which gradient descent suffers from significant difficulties
    - These difficulties are not necessarily related to local minima or saddle points, but are related to:
    - 1) Insufficient information in the gradients about the underlying target function
    - 2) Low signal-to-noise ratio
    - 3) Bad conditioning
    - 4) Flatness in the activations

Shin, H., Lee, J. K., Kim, J., & Kim, J. (2017). Continual Learning with Deep Generative Replay. arXiv, 1705.08690. Retrieved from https://arxiv.org/abs/1705.08690v3

#training_dynamics Shwartz-Ziv, R., & Tishby, N. (2017). Opening the Black Box of Deep Neural Networks via Information. arXiv, 1703.00810. Retrieved from https://arxiv.org/abs/1703.00810v3

    - We visualize DNNs based on Information Bottleneck (IB) theory and stucy the SGD dynamics:
    - 1) Most of the training epochs in standard DL are spent on compression of the input to efficient representation and not on fitting the training labels
    - 2) Then SGD switches from a fast drift to smaller training error into a stochastic relaxation, or random diffusion, and the representation compression phase begins
    - 3) Finally, the converged layers lie on or very close to the Information Bottleneck (IB) theoretical bound
    - This provides a new explanation for the computational benefit of the hidden layers

#training Smith, L. N., & Topin, N. (2017). Exploring loss function topology with cyclical learning rates. arXiv, 1702.04283. Retrieved from https://arxiv.org/abs/1702.04283v1

    - We apply Cyclical Learning Rates (CLR) and linear network interpolation between checkpoints, and observe counterintuitive increases and decreases in training loss and instances of rapid training
    - Cyclical Learning Rates can produce greater testing accuracy than traditional training 

#training Smith, L. N., & Topin, N. (2017). Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates. arXiv, 1708.07120. Retrieved from https://arxiv.org/abs/1708.07120v3

    - We describe "super-convergence", where CNNs can be trained an order of magnitude faster than with standard methods
    - We train NNs with one LR cycle and a large maximum LR
    - Large LRs regularize the training, hence requiring a reduction of all other forms of regularization, because the amount of regularization must be balanced for each dataset and architecture
    - We derive a method to compute an estimate of the optimal learning rate
    - Super-convergence provides a greater boost in performance when the amount of labeled training data is limited
    - The philosophy behind CLR is a combination of curriculum learning and simulated annealing

#training Smith, S. L., & Le, Q. V. (2017). A Bayesian Perspective on Generalization and Stochastic Gradient Descent. arXiv, 1710.06451. Retrieved from https://arxiv.org/abs/1710.06451v3

    - We show that the optimum batch size is proportional to both the learning rate and the size of the training set
    - Why NNs can easily memorize randomly labeled training data, but generalizing well on real labels?
    - We show that the same phenomenon occurs in small linear models
    - This is because Bayesian evidence penalizes sharp minima but is invariant to model parameterization
    - The noise introduced by small mini-batches drives the parameters towards minima whose evidence is large

#training Smith, S. L., Kindermans, P.-J., Ying, C., & Le, Q. V. (2017). Don't Decay the Learning Rate, Increase the Batch Size. arXiv, 1711.00489. Retrieved from https://arxiv.org/abs/1711.00489v2

    - Decaying the learning rate is simulated annealing
    - Instead, we propose to increase the batch size during training
    - This has the potential to dramatically reduce model training times

Stock, P., & Cisse, M. (2017). ConvNets and ImageNet Beyond Accuracy: Understanding Mistakes and Uncovering Biases. arXiv, 1711.11443. Retrieved from https://arxiv.org/abs/1711.11443v2

    - In CV, we need to improve robustness to adversarial examples and immunity to biases including racial and gender biases
    - We conduct a study of the misclassifications of various pre-trained architectures and compare with human judgements.
    - We find that the accuracy of CNNs evaluated on ImageNet is vastly underestimated. When the model makes a mistake, humans often agree with the model. If top-5 error is the measure of interest, then ImageNet is almost solved.
    - In experiments we use MMD-critic: an approach inspired by bayesian model criticism to select the prototypes and the critics among a given set of examples. MMD-critic uses the maximum mean discrepancy and large-scale submodular optimization
    - We also generate adversarial examples using the IFGSM attack on a pre-trained ResNet-101, and show to humans the interpretation image generated by LIME instead of the whole adversarial image using the top 8 most important features. The percentage of agreement between the predictions of the model and the humans increases from 22.01% to 30.80% when the explanation is shown instead of the whole image.
    - So, the robustness of CNNs to adversarial examples is also underestimated. Providing explanations helps to mitigate the misclassification of adversarial examples from the perspective of human judgement.

Soudry, D., & Hoffer, E. (2017). Exponentially vanishing sub-optimal local minima in multilayer neural networks. arXiv, 1702.05777. Retrieved from https://arxiv.org/abs/1702.05777v5

Taki, M. (2017). Deep Residual Networks and Weight Initialization. arXiv, 1709.02956. Retrieved from https://arxiv.org/abs/1709.02956v1

    - ResNets are relatively insensitive to choice of initial weights
    - How does batch normalization improve backpropagation in ResNet?
    - We propose a new weight initialization distribution to prevent exploding gradients

#attention #nmt Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ...Polosukhin, I. (2017). Attention Is All You Need. arXiv, 1706.03762. Retrieved from https://arxiv.org/abs/1706.03762v7

Veličković, P., Cucurull, G., Casanova, A., Romero, A., Liò, P., & Bengio, Y. (2017). Graph Attention Networks. arXiv, 1710.10903. Retrieved from https://arxiv.org/abs/1710.10903v3

Whittington, J. C. R., & Bogacz, R. (2017). An Approximation of the Error Backpropagation Algorithm in a Predictive Coding Network with Local Hebbian Synaptic Plasticity. Neural Comput., 28333583. Retrieved from https://pubmed.ncbi.nlm.nih.gov/28333583

Wiatowski, T., Grohs, P., & Bölcskei, H. (2017). Energy Propagation in Deep Convolutional Neural Networks. arXiv, 1704.03636. Retrieved from https://arxiv.org/abs/1704.03636v3

Wu, L., Zhu, Z., & E, W. (2017). Towards Understanding Generalization of Deep Learning: Perspective of Loss Landscapes. arXiv, 1706.10239. Retrieved from https://arxiv.org/abs/1706.10239v2

    - We find that local minima with large volume of attractors often lead good generalization performance
    - We show that the volume of basin of attraction of good minima dominates over that of poor minima, which guarantees optimization methods with random initialization to converge to good minima
    - This is irrelevant with the types of optimization methods, in contrast with previous understanding which attribute a good generalization to some particular optimizers (SGD) or regularizations (dropout)
    - Low-complexity solutions have a small norm of Hessian matrix with respect to model parameters

Xie, D., Xiong, J., & Pu, S. (2017). All You Need is Beyond a Good Init: Exploring Better Solution for Training Extremely Deep Convolutional Neural Networks with Orthonormality and Modulation. arXiv, 1703.01827. Retrieved from https://arxiv.org/abs/1703.01827v3

    - Problem: how to train deep nets without any shortcuts/identity mappings?
    - Solution: regularizer which utilizes orthonormality and a backward error modulation mechanism

You, Y., Zhang, Z., Hsieh, C.-J., Demmel, J., & Keutzer, K. (2017). ImageNet Training in Minutes. arXiv, 1709.05011. Retrieved from https://arxiv.org/abs/1709.05011v10

Zaheer, M., Kottur, S., Ravanbakhsh, S., Poczos, B., Salakhutdinov, R., & Smola, A. (2017). Deep Sets. arXiv, 1703.06114. Retrieved from https://arxiv.org/abs/1703.06114v3

## 2018

Agarap, A. F. (2018). Deep Learning using Rectified Linear Units (ReLU). arXiv, 1803.08375. Retrieved from https://arxiv.org/abs/1803.08375v2

    - A joke paper with a lot of citations due to misleading title
    - We just replace softmax with ReLU for classification neural networks
    - We apply cross-entropy loss (IMO this is strage since predicted probabilities are now arbitrary numbers)
    - We test on MNIST, Fashion-MNIST, WDBC
    - The quality of our method is WORSE than standard softmax

Alcorn, M. A., Li, Q., Gong, Z., Wang, C., Mai, L., Ku, W.-S., & Nguyen, A. (2018). Strike (with) a Pose: Neural Networks Are Easily Fooled by Strange Poses of Familiar Objects. arXiv, 1811.11553. Retrieved from https://arxiv.org/abs/1811.11553v3

    - We propose a framework for finding OOD errors in CV models: we make "adversarial renders" of 3D objects in different poses to find object geometry and appearance, lighting, background, or camera settings that cause a target DNN to misbehave.
    - We searched for misclassified 6D poses (i.e., 3D translations and 3D rotations) of 3D objects.
    - We built a dataset of 3D objects corresponding to 30 ImageNet classes relevant to the self-driving car application.
    - SOTA ImageNet classifiers misclassify many generated adversarial examples that are human-recognizable. So, they are still far from true object recognition.
    - Training on adversarial poses generated by the 30 objects (in addition to the original ImageNet data) did not help DNNs generalize well to held-out objects in the same class.

Allen-Zhu, Z., Li, Y., & Song, Z. (2018). A Convergence Theory for Deep Learning via Over-Parameterization. arXiv, 1811.03962. Retrieved from https://arxiv.org/abs/1811.03962v5

    - We study the theory of multi-layer networks
    - We prove that SGD can find global minima on the training objective of over-parameterized DNNs
    - Key insight is that in a neighborhood of the random initialization, the opt. landscape is almost convex
    - This implies an equivalence between over-parameterized finite width NNs and neural tangent kernel
    - Our theory at least applies to FCN, CNN and ResNet

Atanov, A., Ashukha, A., Struminsky, K., Vetrov, D., & Welling, M. (2018). The Deep Weight Prior. arXiv, 1810.06943. Retrieved from https://arxiv.org/abs/1810.06943v6

#uncertainty Atanov, A., Ashukha, A., Molchanov, D., Neklyudov, K., & Vetrov, D. (2018). Uncertainty Estimation via Stochastic Batch Normalization. arXiv, 1802.04893. Retrieved from https://arxiv.org/abs/1802.04893v2

    - We show that Batch Normalization maximizes the ELBO for a certain probabilistic model
    - We design an algorithm which acts consistently during train and test, however, inference becomes inefficient
    - We propose Stochastic Batch Normalization – an efficient approximation of proper inference procedure
    - It successfully extends Dropout and Deep Ensembles methods
    - It allows a scalable uncertainty estimation
    - We show its performance on OOD uncertainty estimation on MNIST and CIFAR10

#training #ensembling Athiwaratkun, B., Finzi, M., Izmailov, P., & Wilson, A. G. (2018). There Are Many Consistent Explanations of Unlabeled Data: Why You Should Average. arXiv, 1806.05594. Retrieved from https://arxiv.org/abs/1806.05594v3

    - We discuss consistency regularization that is known to be successful in semi-supervised learning
    - We show that SGD struggles to converge to a single point on the consistency loss
    - Instead, it continues to explore many solutions with high distances apart
    - This leads to changes in predictions on the test data
    - We propose to train consistency-based methods with Stochastic Weight Averaging
    - We propose fast-SWA which averages multiple points within each cycle of a cyclical LR
    - We achieve the best known semi-supervised results on CIFAR-10 and CIFAR-100

Balestriero, R., & Baraniuk, R. (2018). Mad Max: Affine Spline Insights into Deep Learning. arXiv, 1805.06576. Retrieved from https://arxiv.org/abs/1805.06576v5

    - A large class of DNs can be written as a composition of maxaffine spline operators (MASOs)
    - This links DNs to the theory of vector quantization (VQ) and K-means clustering
    - We propose a simple penalty term to loss function to significantly improve performance

Bartlett, P. L., Helmbold, D. P., & Long, P. M. (2018). Gradient descent with identity initialization efficiently learns positive definite linear transformations by deep residual networks. arXiv, 1802.06093. Retrieved from https://arxiv.org/abs/1802.06093v4

    - We analyze deep linear neural networks (without non-linearities) when target function Φ is also linear
    - While, in practice, DNNs are non-linear, analysis of the linear case can provide a tractable way to gain insights
    - We study GD convergence for Φ with different properfies and regularization of DNN towards identity
    - We show that GD fails to converge for Φ whose distance from the identity is a larger constant

Beery, S., Van Horn, G., & Perona, P. (2018). Recognition in Terra Incognita. Retrieved from https://openaccess.thecvf.com/content_ECCV_2018/html/Beery_Recognition_in_Terra_ECCV_2018_paper.html

    - Current learning algorithms do not generalize well across datasets. For example, cows in common contexts (e.g. Alpine pastures) are detected and classified correctly, while cows in uncommon contexts (beach, waves and boat) are not detected or classified poorly.
    - We present a dataset designed to measure recognition generalization to novel environments
    - The dataset contains images from twenty animal camera traps when positions are fixed and capture is triggered automatically
    - The challenge is to generalize to new locations where no training data is available
    - We benchmark the current SOTA detection and classification pipelines and find that there is much room for improvement

Belkin, M., Ma, S., & Mandal, S. (2018). To understand deep learning we need to understand kernel learning. arXiv, 1802.01396. Retrieved from https://arxiv.org/abs/1802.01396v3

    - We show that strong performance of overfitted classifiers is not a unique feature of deep learning
    - Kernel machines trained to have nearly-zero training error perform very well on test data
    - Non-smooth Laplacian kernels easily fit random labels, but smooth Gaussian kernels do not
    - However, overfitted Laplacian and Gaussian classifiers have similar performance on test
    - This suggests that generalization is tied to the properties of the kernel function, not of the optimization
    - This indicates a need for new theoretical ideas for understanding properties of classical kernel methods
    - Note that kernel methods can be viewed as a special type of two-layer neural networks with a fixed first layer

#training Belkin, M., Hsu, D., Ma, S., & Mandal, S. (2018). Reconciling modern machine learning practice and the bias-variance trade-off. arXiv, 1812.11118. Retrieved from https://arxiv.org/abs/1812.11118v2

    - We introduce the "double descent" curve instead of U-shaped bias-variance curve for DNNs
    - We show that double descent exists for a wide spectrum of models and datasets
    - Later works point out that the same pehenomena was observed earler in "Statistical Mechanics of Learning: Generalization" by Manfred Opper

Behrmann, J., Grathwohl, W., Chen, R. T. Q., Duvenaud, D., & Jacobsen, J.-H. (2018). Invertible Residual Networks. arXiv, 1811.00995. Retrieved from https://arxiv.org/abs/1811.00995v3

Bengio, Y., Lodi, A., & Prouvost, A. (2018). Machine Learning for Combinatorial Optimization: a Methodological Tour d'Horizon. arXiv, 1811.06128. Retrieved from https://arxiv.org/abs/1811.06128v2

Chaudhry, A., Dokania, P. K., Ajanthan, T., & Torr, P. H. S. (2018). Riemannian Walk for Incremental Learning: Understanding Forgetting and Intransigence. arXiv, 1801.10112. Retrieved from https://arxiv.org/abs/1801.10112v3

Chen, R. T. Q., Rubanova, Y., Bettencourt, J., & Duvenaud, D. (2018). Neural Ordinary Differential Equations. arXiv, 1806.07366. Retrieved from https://arxiv.org/abs/1806.07366v5

    - We propose continuous-depth ResNets, continuous-time latent variable models, continuous normalizing flows
    - The derivative of the hidden state is parameterized
    - The output of the network is computed using a blackbox differential equation solver
    - They have constant memory cost
    - We can adapt evaluation strategy to each input
    - We can explicitly trade numerical precision for speed

Chevalier-Boisvert, M., Bahdanau, D., Lahlou, S., Willems, L., Saharia, C., Nguyen, T. H., & Bengio, Y. (2018). BabyAI: A Platform to Study the Sample Efficiency of Grounded Language Learning. arXiv, 1810.08272. Retrieved from https://arxiv.org/abs/1810.08272v4

#attention #rnn Dehghani, M., Gouws, S., Vinyals, O., Uszkoreit, J., & Kaiser, Ł. (2018). Universal Transformers. arXiv, 1807.03819. Retrieved from https://arxiv.org/abs/1807.03819v3

Du, S. S., Lee, J. D., Li, H., Wang, L., & Zhai, X. (2018). Gradient Descent Finds Global Minima of Deep Neural Networks. arXiv, 1811.03804. Retrieved from https://arxiv.org/abs/1811.03804v4

    - We prove that GD achieves zero training loss in polynomial time for a deep over-parameterized ResNet
    - The Gram matrix is stable throughout the training process
    - We obtain a similar convergence result for convolutional ResNet

George, D., Lavin, A., Guntupalli, J. S., Mely, D., Hay, N., & Lazaro-Gredilla, M. (2018). Cortical Microcircuits from a Generative Vision Model. arXiv, 1808.01058. Retrieved from https://arxiv.org/abs/1808.01058v1

    - Based on RCN, we derive a family of anatomically instantiated and functional cortical circuit models
    - It is derived by comparing the computational requirements with known anatomical constraints
    - It suggests precise functional roles for the feedforward, feedback and lateral connections

Draxler, F., Veschgini, K., Salmhofer, M., & Hamprecht, F. A. (2018). Essentially No Barriers in Neural Network Energy Landscape. arXiv, 1803.00885. Retrieved from https://arxiv.org/abs/1803.00885v5

    - We construct continuous paths between minima of DNNs on CIFAR10 and CIFAR100
    - These paths are flat in both the training and test landscapes
    - This implies that minima are points on a single connected manifold of low loss

Du, S. S., Hu, W., & Lee, J. D. (2018). Algorithmic Regularization in Learning Deep Homogeneous Models: Layers are Automatically Balanced. arXiv, 1806.00900. Retrieved from https://arxiv.org/abs/1806.00900v2

Elsayed, G. F., Goodfellow, I., & Sohl-Dickstein, J. (2018). Adversarial Reprogramming of Neural Networks. arXiv, 1806.11146. Retrieved from https://arxiv.org/abs/1806.11146v2

    - The majority of adversarial aim to degrade the performance of a model.
    - We consider a novel adversarial goal: reprogramming the model to perform a task chosen by the attacker. We refer to the class of attacks where a model is repurposed to perform a new task as adversarial reprogramming.
    - We experimentally demonstrate adversarial programs that target several ImageNet-trained CNNs. They alter the network function from ImageNet classification to: counting squares in an image, classifying MNIST digits, and classifying CIFAR-10 images.
    - The magnitude of the perturbation need not be constrained (however, we note that it is still possible to construct reprogramming attacks that are imperceptible). Potential consequences may be repurposing of AI-driven assistants into spies or spam bots, etc.

Fort, S., & Scherlis, A. (2018). The Goldilocks zone: Towards better understanding of neural network loss landscapes. arXiv, 1807.02581. Retrieved from https://arxiv.org/abs/1807.02581v2

    - We explore the loss landscape of FCNs and CNNs with ReLU and tanh non-linearities on MNIST and CIFAR-10
    - We see the excess of local convexity in a range of configuration space radii we call the Goldilocks zone
    - Local convexity of an initialization is predictive of training speed
    - We hypothesize that the Goldilocks zone contains high density of suitable initialization configurations

Frankle, J., & Carbin, M. (2018). The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks. arXiv, 1803.03635. Retrieved from https://arxiv.org/abs/1803.03635v5

    - Contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start.
    - We articulate the lottery ticket hypothesis: dense randomly initialized FFNs contain sparse subnetworks (winning tickets, less than 10-20% of the original size) that — when trained in isolation — reach test accuracy comparable to the original network in a similar number of iterations.
    - We identify a winning ticket by training a network and pruning its smallest-magnitude weights. The remaining, unpruned connections constitute the architecture of the winning ticket. Each unpruned connection’s value is then reset to its initialization from original network before it was trained. In this paper, we focus on iterative pruning, which repeatedly trains, prunes, and resets the network over n rounds. We show that selecting n > 1 iterations is effective.
    - Implications: Since winning tickets can be trained from the start in isolation, a hope is that we can design training schemes that search for winning tickets and prune as early as possible. We also can take inspiration from winning tickets to design new architectures and initialization schemes.

Friston, K. (2018). Am I Self-Conscious? (Or Does Self-Organization Entail Self-Consciousness?). Front. Psychol., 9, 348034. Retrieved from https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2018.00579/full

Galloway, A., Tanay, T., & Taylor, G. W. (2018). Adversarial Training Versus Weight Decay. arXiv, 1804.03308. Retrieved from https://arxiv.org/abs/1804.03308v3

#ensembling Garipov, T., Izmailov, P., Podoprikhin, D., Vetrov, D., & Wilson, A. G. (2018). Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs. arXiv, 1802.10026. Retrieved from https://arxiv.org/abs/1802.10026v4

    - We show that optima (posterior modes) of DNN loss surfaces are connected by simple curves over which training and test accuracy are nearly constant (mode connectivity)
    - We introduce a training procedure to discover these high-accuracy pathways between modes
    - We propose Fast Geometric Ensembling to train ensembles in the time required to train a single model
    - We surpass other Snapshot Ensembles techniques on CIFAR-10, CIFAR-100, and ImageNet

Garriga-Alonso, A., Rasmussen, C. E., & Aitchison, L. (2018). Deep Convolutional Networks as shallow Gaussian Processes. arXiv, 1808.05587. Retrieved from https://arxiv.org/abs/1808.05587v2

    - We draw a connection between infinitely-wide CNNs and a Gaussian process (GP)
    - The equivalent kernel can be computed efficiently
    - We demonstrate the performance increase coming from adding translation-invariant structure to the GP prior
    - Without computing any gradients, and without augmenting the training set, we obtain 0.84% error rate on the MNIST, setting a new record for nonparametric GP-based methods

#training Golmant, N., Vemuri, N., Yao, Z., Feinberg, V., Gholami, A., Rothauge, K., ...Gonzalez, J. (2018). On the Computational Inefficiency of Large Batch Sizes for Stochastic Gradient Descent. arXiv, 1811.12941. Retrieved from https://arxiv.org/abs/1811.12941v1

    - We empirically investigate large-batch training across wide range of network architectures and problem domains
    - As the batch size becomes larger, there are three main phases of scaling behavior for convergence speed:
    - 1) Linear: increasing the batch size results in linear gains in convergence speed
    - 2) Diminishing returns: improving wall-clock training time at the expense of greater total computational cost
    - 3) Stagnation: increasing batch size results in marginal or non-existent reductions in convergence speed
    - So, increasing the batch size beyond a certain point (that is usually substantially below the capacity of current systems) yields no improvement in wall-clock time to convergence, even for a system with perfect parallelism
    - Increasing the batch size leads to a significant increase in generalization error
    - Model architecture and data complexity is important about this, not a dataset size

#training #distillation Gotmare, A., Keskar, N. S., Xiong, C., & Socher, R. (2018). A Closer Look at Deep Learning Heuristics: Learning rate restarts, Warmup and Distillation. arXiv, 1810.13243. Retrieved from https://arxiv.org/abs/1810.13243v1

    - We use mode connectivity and CCA to improve understanding of cosine annealing, learning rate warmup and knowledge distillation, and hypothesize reasons for the success of the heuristics
    - The reasons often quoted for the success of cosine annealing are not substantiated by our experiments (escaping local minima after restarts might be an oversimplification)
    - The effect of LR warmup is to prevent the deeper layers from creating training instability, since it limits weight changes in the deeper layers; freezing them achieves similar outcomes as warmup
    - The latent knowledge shared by the teacher in distillation is primarily disbursed in the deeper layers

#nmt #few_shot Gu, J., Hassan, H., Devlin, J., & Li, V. O. K. (2018). Universal Neural Machine Translation for Extremely Low Resource Languages. arXiv, 1802.05368. Retrieved from https://arxiv.org/abs/1802.05368v2

Gur-Ari, G., Roberts, D. A., & Dyer, E. (2018). Gradient Descent Happens in a Tiny Subspace. arXiv, 1812.04754. Retrieved from https://arxiv.org/abs/1812.04754v1

    - This paper shows that the gradient over training time lies primarily in the subspace spanned by the top few largest eigenvalues of the Hessian H (a sentence from https://arxiv.org/pdf/1910.05929v1)
    - This implies that most of the descent directions lie along extremely low dimensional subspaces of high local positive curvature
    - Exploration in the vastly larger number of other directions utilizes a small portion of the gradient

Kidambi, R., Netrapalli, P., Jain, P., & Kakade, S. M. (2018). On the insufficiency of existing momentum schemes for Stochastic Optimization. arXiv, 1803.05591. Retrieved from https://arxiv.org/abs/1803.05591v2

    - We discuss momentum methods such as heavy ball (HB) and Nesterov’s accelerated gradient descent (NAG)
    - Question: are they optimal even with batchsize of 1? (this case is called stochastic first order oracle, SFO)
    - We describe a linear regression problem instance where it is indeed possible to improve upon SGD, and ASGD (https://arxiv.org/abs/1704.08227) achieves this improvement, but HB (with any step size and momentum) cannot achieve any improvement over SGD; the same holds true for NAG as well
    - We conclude that HB and NAG’s improved performance is attributed to mini-batching and these methods will often struggle to improve over SGD with small constant batch sizes
    - ASGD provides a distinct advantage in training deep networks over SGD, HB and NAG

Hahn, S., & Choi, H. (2018). Understanding Dropout as an Optimization Trick. arXiv, 1806.09783. Retrieved from https://arxiv.org/abs/1806.09783v3

#training Hanin, B., & Rolnick, D. (2018). How to Start Training: The Effect of Initialization and Architecture. arXiv, 1803.01719. Retrieved from https://arxiv.org/abs/1803.01719v3

    - Study failure modes for early training in deep ReLU nets:
    - 1) exploding or vanishing mean activation length
    - 2) exponentially large variance of activation length
    - For FCN, the cure of 1) require a specific init, and the cure of 2) require a specific constraint
    - For ResNets, the cure of 1) require a specific scaling, then 2) also gets cured

#training Hernández-García, A., & König, P. (2018). Do deep nets really need weight decay and dropout? arXiv, 1802.07042. Retrieved from https://arxiv.org/abs/1802.07042v3

    - Recently is was suggested that explicit regularization may not be as important as widely believed
    - We perform ablations on weight decay and dropout
    - We find that they may not be necessary, their generalization gain can be achieved by data augmentation alone

#few_shot #bayesian Hewitt, L. B., Nye, M. I., Gane, A., Jaakkola, T., & Tenenbaum, J. B. (2018). The Variational Homoencoder: Learning to learn high capacity generative models from few examples. arXiv, 1807.08919. Retrieved from https://arxiv.org/abs/1807.08919v1

    - Hierarchical Bayesian methods can unify many related tasks as inference within a single generative model
    - When this generative model is a powerful DNN, we show that existing learning techniques typically fail to effectively use latent variables
    - We develop a a Variational Homoencoder (VHE): a modification of the VAE in which encoded observations are decoded to new elements from the same class
    - It produces a hierarchical latent variable model which better utilises latent variables
    - Using VHE, we learn a hierarchical PixelCNN on the Omniglot dataset and achieve strong one-shot performance

Hjelm, R. D., Fedorov, A., Lavoie-Marchildon, S., Grewal, K., Bachman, P., Trischler, A., & Bengio, Y. (2018). Learning deep representations by mutual information estimation and maximization. arXiv, 1808.06670. Retrieved from https://arxiv.org/abs/1808.06670v5

#training_process Izmailov, P., Podoprikhin, D., Garipov, T., Vetrov, D., & Wilson, A. G. (2018). Averaging Weights Leads to Wider Optima and Better Generalization. arXiv, 1803.05407. Retrieved from https://arxiv.org/abs/1803.05407v3

    - We propose Stochastic Weight Averaging (SWA): simple averaging of multiple checkpoints that leads to better generalization than conventional training
    - It finds much flatter solutions than SGD and approximates Fast Geometric Ensembling (FGE)
    - SWA is extremely easy to implement, improves generalization, and has almost no computational overhead

Jacot, A., Gabriel, F., & Hongler, C. (2018). Neural Tangent Kernel: Convergence and Generalization in Neural Networks. arXiv, 1806.07572. Retrieved from https://arxiv.org/abs/1806.07572v4

    - We find that the evolution of an NN during training can be described by the Neural Tangent Kernel (NTK)
    - This makes it possible to study the training of NNs in function space instead of parameter space
    - We observe NTK behavior for wide networks, and compare it to the infinite-width limit
    - We suggest a theoretical motivation for early stopping

Jastrzębski, S., Kenton, Z., Ballas, N., Fischer, A., Bengio, Y., & Storkey, A. (2018). On the Relation Between the Sharpest Directions of DNN Loss and the SGD Step Length. arXiv, 1807.05031. Retrieved from https://arxiv.org/abs/1807.05031v6

    - This paper shows that the maximal Hessian eigenvalue grows, peaks and then declines during training
    - This implies that gradient descent trajectories tend to enter higher positive curvature regions of the loss landscape before eventually finding the desired flatter regions

Kaiser, Ł., & Bengio, S. (2018). Discrete Autoencoders for Sequence Models. arXiv, 1801.09797. Retrieved from https://arxiv.org/abs/1801.09797v1

Karakida, R., Akaho, S., & Amari, S.-i. (2018). Universal Statistics of Fisher Information in Deep Neural Networks: Mean Field Approach. arXiv, 1806.01316. Retrieved from https://arxiv.org/abs/1806.01316v3

    - The landscape of the parameter space of DNN is defined by the Fisher information matrix (FIM)
    - We investigate the asymptotic statistics of FIM eigenvalues and reveal that most of them are close to zero while the maximum eigenvalue takes a huge value; so, the landscape of the parameter space of DNN is locally flat in most dimensions, but strongly distorted in others
    - Small eigenvalues that induce flatness are connected to a measure of generalization ability
    - The maximum eigenvalue that induces the distortion enables us to estimate a LR

Kawaguchi, K., Huang, J., & Kaelbling, L. P. (2018). Effect of Depth and Width on Local Minima in Deep Learning. arXiv, 1811.08150. Retrieved from https://arxiv.org/abs/1811.08150v4

    - We study DNNs with squared loss without the strong overparameterization and simplification assumptions
    - We show that the quality of local minima improves toward the global minimum as depth and width increase

Kawaguchi, K., & Bengio, Y. (2018). Depth with Nonlinearity Creates No Bad Local Minima in ResNets. arXiv, 1810.09038. Retrieved from https://arxiv.org/abs/1810.09038v3

    - One can consider a map that takes a classical machine-learning model (a basis-function model with an arbitrary fixed basis or set of features) as input, and outputs a deep version of the classical model. One can then ask what structure this "deepening" map preserves
    - We prove that in a type of deep ResNets, depth with nonlinearity (i.e., the "deepening" map from the set of basis-function models to the set of deep ResNets) does not create "bad" local minima

Kirchhoff, M. D., Parr, T., Palacios, E., Friston, K. J., & Kiverstein, J. (2018). The Markov blankets of life: autonomy, active inference and the free energy principle. J. R. Soc. Interface. Retrieved from https://www.semanticscholar.org/paper/The-Markov-blankets-of-life%3A-autonomy%2C-active-and-Kirchhoff-Parr/e1d3f7eb3bf11a0881d9417df8071ca663eefbaf

#attention #few_shot Lee, J., Lee, Y., Kim, J., Kosiorek, A. R., Choi, S., & Teh, Y. W. (2018). Set Transformer: A Framework for Attention-based Permutation-Invariant Neural Networks. arXiv, 1810.00825. Retrieved from https://arxiv.org/abs/1810.00825v3

    - We want a permutation invariant model to model interactions among elements in the input set
    - We propose encoder-decoder Set Transformer that is a universal approximator of permutation invariant functions
    - This allows to model pairwise- or higher-order interactions between elements in the set
    - Also we reduce computation time of self-attention from quadratic to linear in the number of elements in the set
    - We evaluate on the task of meta-anomaly detection within a set using the CelebA dataset
    - Also we introduce a toy task of counting unique  elements in an input set from the Omniglot dataset

Li, C., Farkhoor, H., Liu, R., & Yosinski, J. (2018). Measuring the Intrinsic Dimension of Objective Landscapes. arXiv, 1804.08838. Retrieved from https://arxiv.org/abs/1804.08838v1

    - We train networks not in their native parameter space, but instead in a smaller, randomly oriented subspace
    - We provide new cartography of the objective landscapes wandered by parameterized models
    - As a result, we see that many problems (datasets) have small intrinsic dimensions
    - It allows quantitative comparison of problem difficulty
    - For example, solving the inverted pendulum problem is 100 times easier than classifying digits from MNIST
    - This allows to obtain an upper bound on the minimum description length of a solution
    - The result is a simple approach for compressing networks, in some cases by more than 100 times

Liang, S., Sun, R., Lee, J. D., & Srikant, R. (2018). Adding One Neuron Can Eliminate All Bad Local Minima. arXiv, 1805.08671. Retrieved from https://arxiv.org/abs/1805.08671v1

    - We study the landscape of NNs for binary classification
    - Under mild assumptions, we prove that after adding one special neuron with a skip connection to the output, or one special neuron per layer, every local minimum is a global minimum
    - So, the distance between any NN and a good NN (with no spurious local minima) is just a neuron away: a class of good NNs is rather “dense” in the class of all NNs
    - This is the first result that no spurious local minimum exists for a wide class of DNNs

Li, C., Farkhoor, H., Liu, R., & Yosinski, J. (2018). Measuring the Intrinsic Dimension of Objective Landscapes. arXiv, 1804.08838. Retrieved from https://arxiv.org/abs/1804.08838v1

#training Lin, T., Stich, S. U., Patel, K. K., & Jaggi, M. (2018). Don't Use Large Mini-Batches, Use Local SGD. arXiv, 1808.07217. Retrieved from https://arxiv.org/abs/1808.07217v6

    - Problem: models trained with large batches often do not generalize well
    - We provide the first comprehensive empirically study of the trade-offs in local SGD for DL - when varying the number of workers, number of local steps and mini-batch sizes
    - We propose post-local SGD to address the generalization issue of large-batch training

#training Liu, J., & Xu, L. (2018). Accelerating Stochastic Gradient Descent Using Antithetic Sampling. arXiv, 1810.03124. Retrieved from https://arxiv.org/abs/1810.03124v1

    - We propose the Antithetic Sampling to reduce the variance of stochastic gradient
    - We make stochastic gradients in a mini-batch negatively correlated as much as possible
    - For this, we just need to calculate the antithetic samples in advance
    - Our stochastic gradient is still an unbiased estimator of full gradient

Martin, C. H., & Mahoney, M. W. (2018). Implicit Self-Regularization in Deep Neural Networks: Evidence from Random Matrix Theory and Implications for Learning. arXiv, 1810.01075. Retrieved from https://arxiv.org/abs/1810.01075v1

    - We analyze the empirical spectral density of DNN layer matrices
    - They show signatures of traditionally-regularized statistical models, even without explicit regularization
    - Based on Random Matrix Theory, we develop a theory to identify 5+1 Phases of Training, corresponding to increasing amounts of Implicit Self-Regularization
    - For small/old DNNs the Implicit Self-Regularization is like traditional Tikhonov regularization
    - For SOTA DNNs we identify a novel form of Heavy-Tailed Self-Regularization, similar to the self-organization seen in the statistical physics  of disordered systems
    - We can cause a small model to exhibit all 5+1 phases of training simply by changing the batch size
    - Large-batch SGD leads to less-well implicitly-regularized models, explaining the the generalization gap
    - In light of our results, we have a much better understanding of why VC theory does not apply to NNs
    - This also suggests why transfer learning is so effective
    - Our practical theory opens the door to address very practical questions

Matthews, A. G. d. G., Rowland, M., Hron, J., Turner, R. E., & Ghahramani, Z. (2018). Gaussian Process Behaviour in Wide Deep Neural Networks. arXiv, 1804.11271. Retrieved from https://arxiv.org/abs/1804.11271v2

    - We study the relationship between deep FCNs and Gaussian processes
    - We empirically study the distance between finite networks and their Gaussian process analogues
    - We compare exact Gaussian process inference and MCMC inference for finite Bayesian NNs
    - A practical recommendation following from our study is that the Bayesian DL community should routinely compare their results to Gaussian processes with the kernels studied in this paper
    - In some scenarios, the Gaussian process behaviour may not be desired because it implies a lack of a hierarchical representation and a Gaussian statistical assumption; we highlight promising ideas to prevent such behaviour

Mei, S., Montanari, A., & Nguyen, P.-M. (2018). A Mean Field View of the Landscape of Two-Layers Neural Networks. arXiv, 1804.06561. Retrieved from https://arxiv.org/abs/1804.06561v2

#uncertainty Nalisnick, E., Matsukawa, A., Teh, Y. W., Gorur, D., & Lakshminarayanan, B. (2018). Do Deep Generative Models Know What They Don't Know? arXiv, 1810.09136. Retrieved from https://arxiv.org/abs/1810.09136v3

    - If we model the density of the input features, can it be used to detect OOD examples?
    - We test flow-based models, VAEs, and PixelCNNs on images
    - We find that this is not true in the reality
    - Do not use density estimates from deep generative models to identify OOD inputs

Neal, B., Mittal, S., Baratin, A., Tantia, V., Scicluna, M., Lacoste-Julien, S., & Mitliagkas, I. (2018). A Modern Take on the Bias-Variance Tradeoff in Neural Networks. arXiv, 1810.08591. Retrieved from https://arxiv.org/abs/1810.08591v4

    - We measure prediction bias and variance of FCNs
    - We find that both bias and variance can decrease as the number of parameters grows
    - We decompose variance into variance due to optimization and variance due to training set sampling
    - Variance due to optimization monotonically decreases with width in the over-parameterized regime

Nouiehed, M., & Razaviyayn, M. (2018). Learning Deep Models: Critical Points and Local Openness. arXiv, 1803.02968. Retrieved from https://arxiv.org/abs/1803.02968v2

Novak, R., Xiao, L., Lee, J., Bahri, Y., Yang, G., Hron, J., ...Sohl-Dickstein, J. (2018). Bayesian Deep Convolutional Networks with Many Channels are Gaussian Processes. arXiv, 1810.05148. Retrieved from https://arxiv.org/abs/1810.05148v4

    - There is a previously identified equivalence between wide FCNs and Gaussian processes
    - We derive an analogous equivalence for multi-layer CNNs both with and without pooling layers
    - We also introduce a Monte Carlo method to estimate the GP corresponding to a given NN architecture
    - Translation equivariance, beneficial in finite channel CNNs, is guaranteed to play no role in the Bayesian treatment of the infinite channel limit
    - We confirm that regular SGD-trained CNNs can significantly outperform their corresponding GPs, suggesting advantages from SGD training compared to fully Bayesian parameter estimation

Papyan, V. (2018). The Full Spectrum of Deepnet Hessians at Scale: Dynamics with SGD Training and Sample Size. arXiv, 1811.07062. Retrieved from https://arxiv.org/abs/1811.07062v2

Parmar, N., Vaswani, A., Uszkoreit, J., Kaiser, Ł., Shazeer, N., Ku, A., & Tran, D. (2018). Image Transformer. arXiv, 1802.05751. Retrieved from https://arxiv.org/abs/1802.05751v3

Perez, E., Strub, F., de Vries, H., Dumoulin, V., & Courville, A. (2018). FiLM: Visual Reasoning with a General Conditioning Layer. AAAI, 32(1). doi: 10.1609/aaai.v32i1.11671

    - We propose FiLM (Feature-wise Linear Modulation), a general-purpose conditioning method for NNs.
    - In the case of visual reasoning, FiLM layers enable a RNN over an input question to influence CNN computation over an image (fig. 3). This process alters the CNN’s behavior as a function of the input question, allowing to carry out a variety of reasoning tasks, from counting to comparing. FiLM also enables the CNN to localize question-referenced objects.
    - FiLM can be thought of as a generalization of Conditional Normalization (which has proven highly successful for image stylization, ASR and VQA).
    - FiLM applies an affine transformation to the network’s intermediate features, based on some input (fig. 2). For CNNs, input modulates the per-feature-map distribution of activations, agnostic to spatial location.
    - FiLM models achieve SOTA across a variety of visual reasoning tasks, and many FiLM model ablations still outperform prior SOTA.
    - FiLM models learn from little data to generalize to more complex and/or substantially different data than seen during training. We also introduce a novel FiLM-based zeroshot generalization method.
    - This is an extended version of short report "Learning Visual Reasoning Without Strong Priors".

#nlp_evaluation Post, M. (2018). A Call for Clarity in Reporting BLEU Scores. arXiv, 1804.08771. Retrieved from https://arxiv.org/abs/1804.08771v2

    - BLEU is a parameterized metric
    - These parameters are often not reported
    - The main culprit is different tokenization and normalization schemes applied to the reference
    - The author provide a new tool, SacreBLEU, to use a common BLEU scheme

Ruthotto, L., & Haber, E. (2018). Deep Neural Networks Motivated by Partial Differential Equations. arXiv, 1804.04272. Retrieved from https://arxiv.org/abs/1804.04272v2

Santurkar, S., Tsipras, D., Ilyas, A., & Madry, A. (2018). How Does Batch Normalization Help Optimization? arXiv, 1805.11604. Retrieved from https://arxiv.org/abs/1805.11604v5

    - We find that in a certain sense BatchNorm does not reduce internal covariate shift
    - We find that BatchNorm makes the optimization landscape significantly more smooth, so we can use larger LR
    - The Lipschitzness of both the loss and the gradients are improved with BatchNorm
    - A number of other natural normalization techniques have a similar (and, sometime, even stronger) effect
    - It could be the case that the smoothening effect of BatchNorm’s encourages converging to more flat minima

Scaman, K., & Virmaux, A. (2018). Lipschitz regularity of deep neural networks: analysis and efficient estimation. arXiv, 1805.10965. Retrieved from https://arxiv.org/abs/1805.10965v2

Shamir, O. (2018). Are ResNets Provably Better than Linear Predictors? arXiv, 1804.06739. Retrieved from https://arxiv.org/abs/1804.06739v4

    - We prove that that ResNet optimization landscape contains NO local minima with value above what can be obtained with a linear predictor (namely a 1-layer network)
    - We use minimal or no assumptions on the network architecture, data distribution, or loss function
    - We show a certain architectural tweak that allows SGD to achieve loss close or better than any linear predictor

#attention #nmt Shaw, P., Uszkoreit, J., & Vaswani, A. (2018). Self-Attention with Relative Position Representations. arXiv, 1803.02155. Retrieved from https://arxiv.org/abs/1803.02155v2

    - We propose Relative Position Representations for transformer

Soudry, D., Hoffer, E., Nacson, M. S., Gunasekar, S., & Srebro, N. (2018). The Implicit Bias of Gradient Descent on Separable Data. Journal of Machine Learning Research, 19(70), 1–57. Retrieved from https://www.jmlr.org/papers/v19/18-188.html

Tarnowski, W., Warchoł, P., Jastrzębski, S., Tabor, J., & Nowak, M. A. (2018). Dynamical Isometry is Achieved in Residual Networks in a Universal Way for any Activation Function. arXiv, 1809.08848. Retrieved from https://arxiv.org/abs/1809.08848v3

    - In ResNets dynamical isometry (https://arxiv.org/abs/1312.6120) is achievable for any activation function
    - We use Free Probability and Random Matrix Theories (FPT & RMT)
    - We study initial and late phases of the learning processes

Thorpe, M., & van Gennip, Y. (2018). Deep Limits of Residual Neural Networks. arXiv, 1810.11741. Retrieved from https://arxiv.org/abs/1810.11741v4

    - We study ResNet as a discretisation of an ODE
    - We study convergence to connect the discrete setting to a continuum problem

Wang, W., Sun, Y., Eriksson, B., Wang, W., & Aggarwal, V. (2018). Wide Compression: Tensor Ring Nets. arXiv, 1802.09052. Retrieved from https://arxiv.org/abs/1802.09052v1

    - We propose Tensor Ring (TR) factorizations to compress existing MLPs and CNNs
    - It compresses with little or no quality degredation on image classification

Wu, Y., & He, K. (2018). Group Normalization. arXiv, 1803.08494. Retrieved from https://arxiv.org/abs/1803.08494v3

    - Problem: Batch Normalization (BN) works poorly with small batches
    - We present Group Normalization (GN) as a simple alternative to BN
    - GN divides the channels into groups and computes within each group the mean and variance for normalization
    - This computation is independent of batch size
    - On ResNet-50 trained in ImageNet with batch size of 2, GN has 10.6% lower error than its BN counterpart
    - When using typical batch sizes, GN is comparably good with BN
    - GN can be naturally transferred from pre-training to fine-tuning (while fine-tuning BN works poorly)

Xiao, L., Bahri, Y., Sohl-Dickstein, J., Schoenholz, S. S., & Pennington, J. (2018). Dynamical Isometry and a Mean Field Theory of CNNs: How to Train 10,000-Layer Vanilla Convolutional Neural Networks. arXiv, 1806.05393. Retrieved from https://arxiv.org/abs/1806.05393v2

    - Are residual connections and batch normalization necessary for very deep nets?
    - No, just use a Delta-Orthogonal initialization and appropriate (in this case, tanh) nonlinearity.
    - This research is based on a mean field theory and dynamical isometry (https://arxiv.org/abs/1312.6120)

Xing, C., Arpit, D., Tsirigotis, C., & Bengio, Y. (2018). A Walk with SGD. arXiv, 1802.08770. Retrieved from https://arxiv.org/abs/1802.08770v4

    - We show the qualitatively different roles of LR and batch-size in DNN optimization and generalization
    - We find that the loss interpolation between parameters before and after each training iteration’s update is roughly convex with a minimum (valley floor) in between for most of the training
    - This means that SGD moves in valley like regions "bouncing between walls at a height"
    - While a large LR maintains a large height from the valley floor, a small batch size injects noise facilitating exploration; this mechanism is crucial for generalization

Yao, Z., Gholami, A., Lei, Q., Keutzer, K., & Mahoney, M. W. (2018). Hessian-based Analysis of Large Batch Training and Robustness to Adversaries. arXiv, 1802.08241. Retrieved from https://arxiv.org/abs/1802.08241v4

    - We study large batch size training through the lens of the Hessian
    - Large batch size training converges to points with noticeably higher Hessian spectrum, and such points have poor robustness to adversarial perturbation
    - We study the connection between robust optimization (https://arxiv.org/abs/1706.06083, that is a min-max optimization problem), and large batch size training
    - Robust optimization is antithetical to large batch training, in the sense that it favors areas with small spectrum (aka flat minimas), and robust training is a saddle-free optimization problem almost everywhere

Yuille, A. L., & Liu, C. (2018). Deep Nets: What have they ever done for Vision? arXiv, 1805.04025. Retrieved from https://arxiv.org/abs/1805.04025v4

Yun, C., Sra, S., & Jadbabaie, A. (2018). Small nonlinearities in activation functions create bad local minima in neural networks. arXiv, 1802.03487. Retrieved from https://arxiv.org/abs/1802.03487v4

    - We prove that for ReLU DNNs and almost all practical datasets (specifically, if linear models cannot perfectly fit the data) there exist infinitely many local minima that are not global
    - We summarize what is known so far, and in our paper we make the least restrictive assumptions
    - We also tackle more general nonlinear activation functions
    - We present some other theoretical results

Yun, C., Sra, S., & Jadbabaie, A. (2018). Efficiently testing local optimality and escaping saddles for ReLU networks. arXiv, 1809.10858. Retrieved from https://arxiv.org/abs/1809.10858v2

    - We provide a theoretical algorithm for checking local optimality and escaping saddles at nondifferentiable points of empirical risks of two-layer ReLU networks
    - Such nondifferentiable points lie in a set of measure zero, so one may be tempted to overlook them as "non-generic", however, when studying critical points we cannot do so, as they are precisely such "non-generic" points

Zaeemzadeh, A., Rahnavard, N., & Shah, M. (2018). Norm-Preservation: Why Residual Networks Can Become Extremely Deep? arXiv, 1805.07477. Retrieved from https://arxiv.org/abs/1805.07477v5

    - We prove that the skip connections facilitate preserving the norm of the gradient, and lead to stable backprop
    - As more residual blocks are stacked, the norm-preservation of the network is enhanced
    - We validate this experimentally
    - We propose Procrustes ResNets: an method to regularize the singular values of the convolution operator and making the ResNet’s transition layers extra norm-preserving
    - This can be used as a guide for training deeper networks and can also inspire new deeper architectures

Zhang, L., & Schaeffer, H. (2018). Forward Stability of ResNet and Its Variants. arXiv, 1811.09885. Retrieved from https://arxiv.org/abs/1811.09885v1

Zhang, J., Liu, T., & Tao, D. (2018). An Information-Theoretic View for Deep Learning. arXiv, 1804.09060. Retrieved from https://arxiv.org/abs/1804.09060v8

    - Question: does it always hold that a deeper network leads to better performance?
    - We derive an upper bound on the expected generalization error depending on depth
    - This shows that as depth increases, the expected generalization error will decrease exponentially
    - Some more results

Zhang, C., Öztireli, C., Mandt, S., & Salvi, G. (2018). Active Mini-Batch Sampling using Repulsive Point Processes. arXiv, 1804.02772. Retrieved from https://arxiv.org/abs/1804.02772v2

## 2019

Allen-Zhu, Z., & Li, Y. (2019). What Can ResNet Learn Efficiently, Going Beyond Kernels? arXiv, 1905.10337. Retrieved from https://arxiv.org/abs/1905.10337v3

    - We prove there are functions that with the same number of training examples, the test error obtained by NNs can be much smaller than any kernel method, including neural tangent kernels (NTK)
    - The main intuition is that DNNs can implicitly perform hierarchical learning reducing the sample complexity

Allen-Zhu, Z., & Li, Y. (2019). Can SGD Learn Recurrent Neural Networks with Provable Generalization? arXiv, 1902.01028. Retrieved from https://arxiv.org/abs/1902.01028v2

    - We discuss optimization and generalization capabilities of RNN
    - We show that RNN with SGD can learn some notable concept class efficiently, meaning that both time and sample complexity scale polynomially in the input length

#ood_generalization Arjovsky, M., Bottou, L., Gulrajani, I., & Lopez-Paz, D. (2019). Invariant Risk Minimization. arXiv, 1907.02893. Retrieved from https://arxiv.org/abs/1907.02893v3

    - Problem: ML models learn spurious correlations stemming from data biases
    - We propose Invariant Risk Minimization (IRM) method
    - It estimates invariant predictors from multiple training environments to enable OOD generalization
    - It tries to find a representation such that the optimal classifier on top of it matches for all environments
    - We propose a tractable algorithm where the classifier is dummy

Arora, S., Du, S. S., Hu, W., Li, Z., Salakhutdinov, R., & Wang, R. (2019). On Exact Computation with an Infinitely Wide Neural Net. arXiv, 1904.11955. Retrieved from https://arxiv.org/abs/1904.11955v2

Arpit, D., Campos, V., & Bengio, Y. (2019). How to Initialize your Network? Robust Initialization for WeightNorm & ResNets. arXiv, 1906.02341. Retrieved from https://arxiv.org/abs/1906.02341v2

    - We propose a novel initialization strategy for weight normalized networks with and without residual connections
    - It is based on mean field approximation
    - It outperforms existing methods in generalization, robustness to hyper-parameters and variance between seeds

Ash, J. T., & Adams, R. P. (2019). On Warm-Starting Neural Network Training. arXiv, 1910.08475. Retrieved from https://arxiv.org/abs/1910.08475v3

    - We discuss warm-starting for contunual learning
    - In practice it seems to yield poorer quality than fresh random initializations as new data arrive, even though the final training losses are similar
    - We provide the "shrink and perturb" trick that overcomes this pathology in several important situations

Bartlett, P. L., Long, P. M., Lugosi, G., & Tsigler, A. (2019). Benign Overfitting in Linear Regression. arXiv, 1906.11300. Retrieved from https://arxiv.org/abs/1906.11300v3

    - Question: why DNNs seem to predict well, even with a perfect fit to noisy training data
    - We study when the same happens in linear regression
    - We show that overparameterization is essential for benign overfitting in linear regression
    - We show that data that lies in a large but finite dimensional space exhibits the benign overfitting phenomenon with a much wider range of covariance properties than data that lies in an infinite dimensional space

Belkin, M., Hsu, D., & Xu, J. (2019). Two models of double descent for weak features. arXiv, 1903.07571. Retrieved from https://arxiv.org/abs/1903.07571v2

    - We show that the double descent curve can be observed in two simple random features models with the least squares/least norm predictor
    - We show that double descent occurs when features are plentiful but individually too "weak"

Bengio, Y., Deleu, T., Rahaman, N., Ke, R., Lachapelle, S., Bilaniuk, O., ...Pal, C. (2019). A Meta-Transfer Objective for Learning to Disentangle Causal Mechanisms. arXiv, 1901.10912. Retrieved from https://arxiv.org/abs/1901.10912v2

Chen, X., Liang, C., Yu, A. W., Zhou, D., Song, D., & Le, Q. V. (2019). Neural Symbolic Reader: Scalable Integration of Distributed and Symbolic Representations for Reading Comprehension. OpenReview. Retrieved from https://openreview.net/forum?id=ryxjnREFwH

    - Discrete operators learned by neural networks, such as addition and sorting, can hardly generalize to inputs of arbitrary size without specialized design. Therefore, integrating neural networks with symbolic reasoning is crucial for solving discrete reasoning tasks. The recent progress on neural semantic parsing is mainly restricted to question answering with structured data sources, e.g., knowledge graphs or tabular databases. Extending it to reading comprehension by parsing the text into structured representations suffers severely from the cascade errors, i.e., the issues of the structured parsing for data preprocessing account for the poor performance of the learned neural model (fig. 1c). A recent line of work usually rely on specialized modules for each type of questions, which is hard to scale to multiple domains or multi-step complex reasoning (fig. 1b).
    - We propose the Neural Symbolic Reader (NeRd) for reading comprehension, which is domain-agnostic (fig. 1a). Given the natural language text including a question and a passage, the reader component (e. g. BERT) encodes each token in the text into an embedding. Then the programmer (e. g. LSTM with attention over the encoded text, and self-attention over the previously generated tokens) takes the output of the reader as input, and decodes a program as a sequence of tokens - programs, which are executed to produce answers. Note that a valid program should satisfy the grammar constraints. Therefore, we constraint decoding to only valid next program tokens, given previously generated tokens. Such a grammar-based decoding process is a common practice in order to ensure the syntactic correctness of the generated programs. Цe find that the greedy decoding is already sufficient to provide good results.
    - To interpret the tokens generated by the programmer as an executable program, we introduce our domain specific language (DSL). To handle discrete reasoning, the DSL includes operators that perform arithmetics (DIFF, SUM), counting (COUNT) and sorting (ARGMAX, ARGMIN, MAX, MIN). A key insight in our DSL design is to introduce the span selection operators, so that all the arithmetics, counting and sorting operators can be applied to text. The only change needed, to apply to a different domain, is to extend the DSL with new operators. For example, MathQA benchmark requires adding more advanced mathematical operations beyond addition and subtraction. A major advantage of our DSL is its compositionality, i.e., complex programs can be generated by compositionally applying the operators.
    - It is often expensive to collect program annotations, so we need weak supervision, i.e., training with question-answer pairs only. We face the cold start problem: the training cannot get started when there isn’t any program available. For example, a question “How many touchdowns did Brady throw” annotated with only an answer “3” cannot be directly used to train our model due to the lack of the target program to optimize on. To solve the cold start problem, we find programs for questions answerable by span selection or arithmetic operations via an exhaustive search. lthough the found programs are noisy, they help bootstrap the training. Throughout the training, we also use the model to decode programs, and add those leading to correct answers into our training set.
    - Another obstacle is the spurious program problem, the phenomenon that a wrong program accidentally predicts a right answer. For example, per arithmetic question in DROP, there are on average 9.8 programs that return correct answers, but usually only one of them is semantically correct. We adopt hard EM, which uses the current model to select the program with the highest model probability among the ones that return the correct answer, and then maximizes the likelihood of the selected program.
    - NeRd outperforms the previous SOTA on DROP. See examples in table 2, 6.
    - IMO it remains not clear how do the programmer predict span start and end. Seems like it does not output indices, but directly substitutes the spans itself into the program.

#attention #interpretation Clark, K., Khandelwal, U., Levy, O., & Manning, C. D. (2019). What Does BERT Look At? An Analysis of BERT's Attention. arXiv, 1906.04341. Retrieved from https://arxiv.org/abs/1906.04341v1

    - We propose methods for analyzing the attention mechanisms of pre-trained models
    - We also propose an attention-based probing classifier
    - We use the tools above to analyze BERT model
    - BERT’s attention heads exhibit patterns such as attending to delimiter tokens, specific positional offsets, or broadly attending over the whole sentence (fig. 1)
    - We find heads that find direct objects of verbs, determiners of nouns, objects of prepositions, and objects of possessive pronouns with >75% accuracy
    - The behavior of the attention heads emerges purely from self-supervised training on unlabeled data

Dean, T., Fan, C., Lewis, F. E., & Sano, M. (2019). Biological Blueprints for Next Generation AI Systems. arXiv, 1912.00421. Retrieved from https://arxiv.org/abs/1912.00421v1

Farquhar, S., & Gal, Y. (2019). A Unifying Bayesian View of Continual Learning. arXiv, 1902.06494. Retrieved from https://arxiv.org/abs/1902.06494v1

Fort, S., & Jastrzebski, S. (2019). Large Scale Structure of Neural Network Loss Landscapes. arXiv, 1906.04724. Retrieved from https://arxiv.org/abs/1906.04724v1

    - We model the NN`s loss surface as a union of n-dimensional manifolds that we call n-wedges
    - We show that common regularizers (learning rate, batch size, L2 regularization, dropout, network width) all influence the optimization trajectory in a similar way
    - We see surprising effects in high dimensions, when our intuition about hills and valleys in 2D often fails us
    - We also critically examine the recently popular Stochastic Weight Averaging (SWA) technique

Fort, S., & Ganguli, S. (2019). Emergent properties of the local geometry of neural loss landscapes. arXiv, 1910.05929. Retrieved from https://arxiv.org/abs/1910.05929v1

    - We discuss 4 previously found local properties of loss landscape in DNNs
    - We develop a simple theoretical model of gradients and Hessians, justified by numerical experiments
    - This model simultaneously accounts for all 4 of these surprising and seemingly unrelated properties
    - This model makes connections with diverse topics in neural networks, random matrix theory, and spin glasses, including the neural tangent kernel, BBP phase transitions, and Derrida’s random energy model

Fort, S., Hu, H., & Lakshminarayanan, B. (2019). Deep Ensembles: A Loss Landscape Perspective. arXiv, 1912.02757. Retrieved from https://arxiv.org/abs/1912.02757v2

    - Why Bayesian neural networks do not perform as well as deep ensembles in practice?
    - We show that the ability of random initializations to explore entirely different modes is unmatched by popular subspace sampling (Bayesian) methods
    - In other words, the functions sampled along a single training trajectory or subspace thereof tend to be very similar in predictions (while potential far away in the weight space), whereas functions sampled from different randomly initialized trajectories tend to be very diverse

Fort, S., Nowak, P. K., Jastrzebski, S., & Narayanan, S. (2019). Stiffness: A New Perspective on Generalization in Neural Networks. arXiv, 1901.09491. Retrieved from https://arxiv.org/abs/1901.09491v3

    - We study generalization through the lens of stiffness: we measure how stiff a neural network is by analyzing how a small gradient step based on one input example affects the loss on another input example
    - We demonstrate the connection between stiffness and generalization, and observe its dependence on LR
    - Some more results

Ghorbani, B., Krishnan, S., & Xiao, Y. (2019). An Investigation into Neural Net Optimization via Hessian Eigenvalue Density. arXiv, 1901.10159. Retrieved from https://arxiv.org/abs/1901.10159v1

    - We develop a tool to study the evolution of the entire Hessian spectrum throughout the optimization process
    - We find interesting details such that rapid appearance of large isolated eigenvalues in the spectrum
    - We analyze how the outlier eigenvalues affect the speed of optimization
    - We believe our tool and style of analysis will open up new avenues of research in optimization

Greff, K., Kaufman, R. L., Kabra, R., Watters, N., Burgess, C., Zoran, D., ...Lerchner, A. (2019). Multi-Object Representation Learning with Iterative Variational Inference. arXiv, 1903.00450. Retrieved from https://arxiv.org/abs/1903.00450v3

#nmt #ood_generalization Gu, J., Wang, Y., Cho, K., & Li, V. O. K. (2019). Improved Zero-shot Neural Machine Translation via Ignoring Spurious Correlations. arXiv, 1906.01181. Retrieved from https://arxiv.org/abs/1906.01181v1

    - Problem: zero-shot translation (translating between language pairs on which a system has never been trained), is an emergent property; however, naive training for zero-shot translation easily fails and is sensitive to hyperparemeters; the performance typically lags far behind the more conventional pivot-based approach which translates twice using a third language as a pivot
    - We show that this issue is a consequence of capturing spurious correlation
    - We propose decoder pre-training and back-translation approaches to improve zero-shot translation

Hastie, T., Montanari, A., Rosset, S., & Tibshirani, R. J. (2019). Surprises in High-Dimensional Ridgeless Least Squares Interpolation. arXiv, 1903.08560. Retrieved from https://arxiv.org/abs/1903.08560v5

    - SOTA NNs appear to be interpolators: estimators that achieve zero training error with low testing error
    - We study minimum L2 norm interpolation in high-dimensional least squares regression
    - We study linear model and one-layer NN
    - We recover several phenomena including the "double descent"

He, F., Liu, T., & Tao, D. (2019). Why ResNet Works? Residuals Generalize. arXiv, 1904.01367. Retrieved from https://arxiv.org/abs/1904.01367v1

    - We prove that skip connections does not increase the hypothesis complexity (expressive power?) of the NNs
    - We study some generalization bounds
    - We conclude that we need to use regularization terms to control the magnitude of the norms of weight matrices not to increase too much, which justifes the standard technique of weight decay

Jiang, A. H., Wong, D. L.-K., Zhou, G., Andersen, D. G., Dean, J., Ganger, G. R., ...Pillai, P. (2019). Accelerating Deep Learning by Focusing on the Biggest Losers. arXiv, 1910.00762. Retrieved from https://arxiv.org/abs/1910.00762v1

    - We propose Selective-Backprop: prioritizing examples with high loss at each iteration, and skipping others
    - This accelerates training by reducing the number of backprops
    - Selective-Backprop converges to target error rates up to 3.5x faster than with standard SGD, and 1.02-1.8x faster than a SOTA importance sampling approach
    - Further acceleration of 26% can be achieved by also skipping forward passes of low priority examples

Jiao, L., & Zhao, J. (2019). A Survey on the New Generation of Deep Learning in Image Processing. IEEE Access, 7, 172231–172263. Retrieved from https://ieeexplore.ieee.org/document/8917633

#uncertainty #ensembling Izmailov, P., Maddox, W. J., Kirichenko, P., Garipov, T., Vetrov, D., & Wilson, A. G. (2019). Subspace Inference for Bayesian Deep Learning. arXiv, 1907.07504. Retrieved from https://arxiv.org/abs/1907.07504v1

    - We construct low-dimensional subspaces of parameter space which contain diverse sets of high performing models
    - We perform Bayesian model averaging over the induced posterior in these subspaces
    - This produces accurate predictions and well calibrated predictive uncertainty for regression and classification

Kawaguchi, K., & Huang, J. (2019). Gradient Descent Finds Global Minima for Generalizable Deep Neural Networks of Practical Sizes. arXiv, 1908.02419. Retrieved from https://arxiv.org/abs/1908.02419v3

    - We prove that GD can find a global minimum for DNNs of sizes commonly encountered in practice
    - This only requires the practical degrees of over-parameterization (several orders of magnitude smaller than that required by the previous theories)
    - Our theory only requires the number of trainable parameters to increase linearly as the training set grows
    - Such DNNs are shown to generalize well to unseen test samples with a natural dataset

Kawaguchi, K., Huang, J., & Kaelbling, L. P. (2019). Every Local Minimum Value is the Global Minimum Value of Induced Model in Non-convex Machine Learning. arXiv, 1904.03673. Retrieved from https://arxiv.org/abs/1904.03673v3

    - We prove, under mild assumptions, that every local minimum achieves the globally optimal value of the perturbable gradient basis model at any differentiable point (what?)

Khrulkov, V., Mirvakhabova, L., Ustinova, E., Oseledets, I., & Lempitsky, V. (2019). Hyperbolic Image Embeddings. arXiv, 1904.02239. Retrieved from https://arxiv.org/abs/1904.02239v2

    - Hyperbolic embeddings as an alternative to Euclidean and spherical embeddings
    - Hyperbolic spaces are more suitable for embedding data with such hierarchical structure
    - Experiments with few-shot learning and person re-identification demonstrate these embeddings are beneficial
    - Propose an approach to evaluate the hyperbolicity of a dataset using Gromov δ-hyperbolicity

Kosiorek, A. R., Sabour, S., Teh, Y. W., & Hinton, G. E. (2019). Stacked Capsule Autoencoders. arXiv, 1906.06818. Retrieved from https://arxiv.org/abs/1906.06818v2

Labach, A., Salehinejad, H., & Valaee, S. (2019). Survey of Dropout Methods for Deep Neural Networks. arXiv, 1904.13310. Retrieved from https://arxiv.org/abs/1904.13310v2

    - We provide an overview of dropout methods, including:
    - We summarize approaches for theoretically explaining the function of dropout methods
    - We describe dropout methods for CNNs
    - We describe dropout for compressing NNs
    - We describe Monte Carlo dropout and related work

Lake, B. M., Salakhutdinov, R., & Tenenbaum, J. B. (2019). The Omniglot challenge: a 3-year progress report. arXiv, 1902.03477. Retrieved from https://arxiv.org/abs/1902.03477v2

Lee, J., Xiao, L., Schoenholz, S. S., Bahri, Y., Novak, R., Sohl-Dickstein, J., & Pennington, J. (2019). Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent. arXiv, 1902.06720. Retrieved from https://arxiv.org/abs/1902.06720v4

    - We build on Neural tangent kernel work and study infinitely-wide NNs
    - Infinitely-wide NNs are governed by a linear model obtained from the first-order Taylor expansion of the network around its initial parameters
    - Gradient-based training of infinitely-wide NNs with a squared loss produces test set predictions drawn from a Gaussian process with a particular compositional kernel

Lezcano-Casado, M., & Martínez-Rubio, D. (2019). Cheap Orthogonal Constraints in Neural Networks: A Simple Parametrization of the Orthogonal and Unitary Group. arXiv, 1901.08428. Retrieved from https://arxiv.org/abs/1901.08428v3

    - A reparametrization to perform unconstrained optimizaion with orthogonal and unitary constraints
    - We apply our results to RNNs with orthogonal recurrent weights, yielding a new architecture called EXPRNN
    - Faster, accurate, and more stable convergence
    - https://github.com/pytorch/pytorch/issues/48144

Liang, S., Sun, R., & Srikant, R. (2019). Revisiting Landscape Analysis in Deep Neural Networks: Eliminating Decreasing Paths to Infinity. arXiv, 1912.13472. Retrieved from https://arxiv.org/abs/1912.13472v1

    - We highlight that even without bad local minima, an optimization may diverge to infinity (extremely big parameter values) if there are paths leading to infinity, along which the loss function decreases
    - If we restrict parameters, this may introduce additional bad local minima on the boundaries
    - We consider a large class of over-parameterized deep neural networks with appropriate regularizers
    - For them, we prove that the loss function has no bad local minima and no decreasing paths to infinity

Liu, T., Chen, M., Zhou, M., Du, S. S., Zhou, E., & Zhao, T. (2019). Towards Understanding the Importance of Shortcut Connections in Residual Networks. arXiv, 1909.04653. Retrieved from https://arxiv.org/abs/1909.04653v3

    - We study a two-layer non-overlapping convolutional ResNet
    - The corresponding optimization problem has a spurious local optimum
    - However, GD with proper normalization avoids it and converges to a global optimum in polynomial time

Liu, L., Jiang, H., He, P., Chen, W., Liu, X., Gao, J., & Han, J. (2019). On the Variance of the Adaptive Learning Rate and Beyond. arXiv, 1908.03265. Retrieved from https://arxiv.org/abs/1908.03265v4

    - Why warmup is essential for adaptive stochastic optimization algorithms like RMSprop and Adam?
    - We show that their variance is problematically large in the early stage
    - We propose Rectified Adam (RAdam) by introducing a term to rectify the variance of the adaptive LR

#lm #compressing Ma, X., Zhang, P., Zhang, S., Duan, N., Hou, Y., Song, D., & Zhou, M. (2019). A Tensorized Transformer for Language Modeling. arXiv, 1906.09777. Retrieved from https://arxiv.org/abs/1906.09777v3

    - We propose Multi-linear attention with Block-Term Tensor Decomposition (BTD)
    - This not only largely compress the model parameters but also obtain performance improvements

Mania, H., Miller, J., Schmidt, L., Hardt, M., & Recht, B. (2019). Model Similarity Mitigates Test Set Overuse. arXiv, 1905.12580. Retrieved from https://arxiv.org/abs/1905.12580v1

    - Popular benchmarks, competitions, industrial scale tuning involve excessive reuse of test data. Models incorporate prior information about the available test data since human analysts choose models in a manner guided by previous results.
    - Theory suggests that for k models chosen independently of n test data points, the holdout method provides valid risk estimates for each of these models up to a deviation on the order of sqrt(log(k)/n). However, adaptivity significantly complicates the theoretical guarantees of the holdout method. A simple adaptive strategy, resembling the practice of selectively ensembling k models, can bias the holdout method by as much as sqrt(k/n). If this bound were attained in practice, holdout data across the board would rapidly lose its value over time. Nonetheless, recent replication studies give evidence that popular benchmarks continue to support progress (on OOD data) despite years of extensive reuse.
    - We show that current ImageNet and CIFAR-10 models exhibit significant agreement in their predictions, well beyond what would follow from their accuracy values alone. We give new generalization bounds that incorporate a measure of similarity.
    - This helps to explain why holdout data has much greater longevity than prior bounds suggest when models are highly similar. In hyperparameter search, such similarity provides a counterweight to the massive number of model evaluations, limiting the amount of overfitting we observe.

McCoy, R. T., Min, J., & Linzen, T. (2019). BERTs of a feather do not generalize together: Large variability in generalization across models with similar test set performance. arXiv, 1911.02969. Retrieved from https://arxiv.org/abs/1911.02969v2

Mei, S., & Montanari, A. (2019). The generalization error of random features regression: Precise asymptotics and double descent curve. arXiv, 1908.05355. Retrieved from https://arxiv.org/abs/1908.05355v5

Millidge, B. (2019). Deep Active Inference as Variational Policy Gradients. arXiv, 1907.03876. Retrieved from https://arxiv.org/abs/1907.03876v1

Nakamura, K., & Hong, B.-W. (2019). Adaptive Weight Decay for Deep Neural Networks. arXiv, 1907.08931. Retrieved from https://arxiv.org/abs/1907.08931v2

    - We propose adaptive weight-decay (AdaDecay) where the gradient norms are normalized within each layer and the degree of regularization for each parameter is proportional to the magnitude of its gradient using the sigmoid
    - We show the effectiveness on MNIST, Fashion-MNIST, and CIFAR-10

#training Nakkiran, P., Kaplun, G., Bansal, Y., Yang, T., Barak, B., & Sutskever, I. (2019). Deep Double Descent: Where Bigger Models and More Data Hurt. arXiv, 1912.02292. Retrieved from https://arxiv.org/abs/1912.02292v1

    - We discuss previously found "double-descent" phenomenon
    - We find that a variety of modern DL tasks exhibit this phenomenon (fig. 1).
    - We define the effective model complexity (EMC) of a training procedure as the maximum number of samples on which it can achieve close to zero training error and hypothesize that double descent occurs as a function of the EMC.
    - We identify certain regimes where increasing the number of train samples actually hurts test performance
    - We also observe epoch-wise double descent, when training randomly imitialized ResNet models on CIFAR-10 with 20% label noise, for sufficiently large models (fig. 1, 9). For “medium sized” models, for which training to completion will only barely reach ≈ 0 error, the test error as a function of training time will follow a classical U-like curve where it is better to stop early. Models that are too small to reach the approximation threshold will remain in the “under parameterized” regime where increasing train time monotonically decreases test error. This is consistent with our unified view of effective model complexity (EMC). Increasing the train time increases the EMC - and thus a sufficiently large model transitions from under- to over-parameterized over the course of training.
    - We identify certain regimes where increasing the number of train samples actually hurts test performance

Neal, B. (2019). On the Bias-Variance Tradeoff: Textbooks Need an Update. arXiv, 1912.08286. Retrieved from https://arxiv.org/abs/1912.08286v1

    - A PhD thesis of Brady Neal
    - We review the history of the bias-variance tradeoff
    - We show a lack of a bias-variance tradeoff in NNs
    - We observe a similar phenomenon in deep RL

Papyan, V. (2019). Measurements of Three-Level Hierarchical Structure in the Outliers in the Spectrum of Deepnet Hessians. arXiv, 1901.08244. Retrieved from https://arxiv.org/abs/1901.08244v1

    - We study a known fact that spectrum of the Hessian of DNNs contains outliers
    - We clarify the source of these outliers, comparing to previous works
    - We find a way to approximate the principal subspace of the Hessian using certain "averaging" operations, avoiding the need for high-dimensional eigenanalysis

Peluchetti, S., & Favaro, S. (2019). Infinitely deep neural networks as diffusion processes. arXiv, 1905.11065. Retrieved from https://arxiv.org/abs/1905.11065v3

    - For deep nets with iid weight init, the dependency on the input vanishes as depth increases to infinity
    - Under some assumptions, infinitely deep ResNets converge to SDEs (diffusion processes)
    - They do not suffer from the above property

Recht, B., Roelofs, R., Schmidt, L., & Shankar, V. (2019). Do ImageNet Classifiers Generalize to ImageNet? arXiv, 1902.10811. Retrieved from https://arxiv.org/abs/1902.10811v2

    - We replicate the dataset creation process for CIFAR-10 and ImageNet, obtaining ImageNetV2 and CIFAR-10.1.
    - A wide range of classification models fail to reach their original accuracy scores. The accuracy drops range from 3% to 15% on CIFAR-10 and 11% to 14% on ImageNet. At the same time, the models with highest accuracy on the original test sets are still the models with highest accuracy on the new test sets (fig. 1).
    - It is possible to recover the original ImageNet accuracies almost exactly if we only include the easiest images from our candidate pool. This suggests that the accuracy scores are sensitive to minutiae of the data cleaning process.

#training Qiao, S., Wang, H., Liu, C., Shen, W., & Yuille, A. (2019). Micro-Batch Training with Batch-Channel Normalization and Weight Standardization. arXiv, 1903.10520. Retrieved from https://arxiv.org/abs/1903.10520v2

    - We propose Weight Standardization (WS) and Batch-Channel Normalization (BCN)
    - This bring two success factors of BatchNorm into micro-batch training:
    - 1) the smoothing effects on the loss landscape
    - 2) the ability to avoid harmful elimination singularities (neuron saturation?)
    - The latter problem is not solved by Layer Normalization or Group Normalization in micro-batch training
    - WS and BCN with micro-batch training is even able to match or outperform BN with large-batch training

Rangamani, A., Nguyen, N. H., Kumar, A., Phan, D., Chin, S. H., & Tran, T. D. (2019). A Scale Invariant Flatness Measure for Deep Network Minima. arXiv, 1902.02434. Retrieved from https://arxiv.org/abs/1902.02434v1

    - Question: flatness of minima empirically provides better generalization, but most measures of sharpness/flatness are not invariant to rescaling of the network parameters
    - We propose a Hessian-based measure for flatness that is invariant to rescaling
    - We confirm that Large-Batch SGD minima are indeed sharper than Small-Batch SGD minima (there was also another work that shows that Large-Batch SGD converges to solution with another Hessian properties)

Shen, X., Tian, X., Liu, T., Xu, F., & Tao, D. (2019). Continuous Dropout. arXiv, 1911.12675. Retrieved from https://arxiv.org/abs/1911.12675v1

    - We extend the traditional binary dropout to continuous dropout inspired by neuroscience
    - We compare it with binary dropout, adaptive dropout, and DropConnect, and show that it performs better

Simsekli, U., Sagun, L., & Gurbuzbalaban, M. (2019). A Tail-Index Analysis of Stochastic Gradient Noise in Deep Neural Networks. arXiv, 1901.06053. Retrieved from https://arxiv.org/abs/1901.06053v1

    - The gradient noise in SGD is often considered to be Gaussian
    - This enables SGD to be analyzed as a stochastic differential equation (SDE) driven by a Brownian motion
    - We show that in deep learning the gradient noise is highly non-Gaussian and admits heavy-tails
    - We investigate this in varying network architectures and sizes, loss functions, and datasets
    - Instead of Brownian motion, we e propose to analyze SGD as an SDE driven by a Lévy motion
    - Such SDEs can incur "jumps", which force the SDE transition from narrow minima to wider minima
    - This sheds more light on the belief that SGD prefers wide minima

Siu, C. (2019). Residual Networks Behave Like Boosting Algorithms. arXiv, 1909.11790. Retrieved from https://arxiv.org/abs/1909.11790v1

    - We show that ResNets with standard training are equivalent to boosting feature representation
    - Inspired by Online Boosting, we modify the ResNet with an additional learnable shrinkage parameter
    - We propose a ResNet-DT (neural decision tree residual network) and test it on the datasets from UCI repository

Sohl-Dickstein, J., & Kawaguchi, K. (2019). Eliminating all bad Local Minima from Loss Landscapes without even adding an Extra Unit. arXiv, 1901.03909. Retrieved from https://arxiv.org/abs/1901.03909v1

    - A one-list paper!
    - We find a way to remove all bad local minima from any loss landscape, so long as the global minimum has a loss of zero (seems like they add two more learnable parameters)
    - Pathologies (diverging go infinity) can continue to exist in losses modified in a such fashion
    - We leave it to the reader to judge whether removing local minima in this fashion is trivial, deep, or both

#attention Sukhbaatar, S., Grave, E., Lample, G., Jegou, H., & Joulin, A. (2019). Augmenting Self-attention with Persistent Memory. arXiv, 1907.01470. Retrieved from https://arxiv.org/abs/1907.01470v1

    - We propose a new model that solely consists of attention layers (no FF layers)
    - We augment the self-attention with persistent memory vectors that play a similar role as the FF layer

#attention #interpretation #see_related_work Tenney, I., Das, D., & Pavlick, E. (2019). BERT Rediscovers the Classical NLP Pipeline. arXiv, 1905.05950. Retrieved from https://arxiv.org/abs/1905.05950v2

    - We analyze how individual sentences are processed by the BERT network, layer-by-layer
    - We find that trained BERT represents the steps of the traditional NLP pipeline
    - Each step appear in the expected sequence: POS tagging, parsing, NER, semantic roles, coreference
    - Also, the model often adjusts this pipeline dynamically, revising lower-level decisions on the basis of disambiguating information from higher-level representations

#nmt #scaling_laws Wang, Q., Li, B., Xiao, T., Zhu, J., Li, C., Wong, D. F., & Chao, L. S. (2019). Learning Deep Transformer Models for Machine Translation. arXiv, 1906.01787. Retrieved from https://arxiv.org/abs/1906.01787v1

    - We show that deep transformer encoders may outperform their wide counterparts
    - The proper use of layer normalization is the key to learning deep encoders
    - We propose an approach based on dynamic linear combination of layers (DLCL) to memorizing the features extracted from all preceding layers
    - We successfully train a 30-layer encoder that is currently the deepest encoder in NMT

Wang, J., Chen, Y., Chakraborty, R., & Yu, S. X. (2019). Orthogonal Convolutional Neural Networks. arXiv, 1911.12207. Retrieved from https://arxiv.org/abs/1911.12207v3

    - We propose orthogonal convolution: filter orthogonality with doubly block-Toeplitz matrix representation
    - It outperforms the kernel orthogonality, learns more diverse and expressive features

Wen, Y., Luk, K., Gazeau, M., Zhang, G., Chan, H., & Ba, J. (2019). An Empirical Study of Large-Batch Stochastic Gradient Descent with Structured Covariance Noise. arXiv, 1902.08234. Retrieved from https://arxiv.org/abs/1902.08234v4

    - We address the problem of improving generalization in large-batch training without elongating training duration
    - We propose to add covariance noise to the gradients
    - We do some theoretical studies

Yang, G., Pennington, J., Rao, V., Sohl-Dickstein, J., & Schoenholz, S. S. (2019). A Mean Field Theory of Batch Normalization. arXiv, 1902.08129. Retrieved from https://arxiv.org/abs/1902.08129v2

    - We show that the batch normalization (BN) itself is the cause of gradient explosion
    - Vanilla BN networks without skip connections are not trainable at large depths for common initialization schemes
    - Gradient explosion can be reduced by tuning the network close to the linear regime
    - It is possible to perform exact Bayesian inference in the case of wide neural networks with BN

Yang, G. (2019). Scaling Limits of Wide Neural Networks with Weight Sharing: Gaussian Process Behavior, Gradient Independence, and Neural Tangent Kernel Derivation. arXiv, 1902.04760. Retrieved from https://arxiv.org/abs/1902.04760v3

    - We introduce a notion of a Tensor Program that can express most neural network computations where all dimensions are large compared to input and output dimensions
    - A Tensor Program is indeed a program over a specific set of functions: matrix transpose, matrix-vector multiplication, linear combination of vectors, and coordinatewise application of any nonlinearity
    - Such tensor programs can express the computation in most NN scenarios, but not all (one example is layer normalization, however we can still deal with it, and the paper shows how)
    - This framework describes the convergence of random NNs (CNN, RNN, ResNet, attention, BN) to Gaussian processes
    - We discuss the applicability of the gradient independence assumption
    - The convergence of the Neural Tangent Kernel is also a part of our framework
    - Our framework is general enough to rederive classical random matrix results as well as recent results in neural network Jacobian singular values
    - We hope our work opens a way toward design of even stronger Gaussian Processess, initialization schemes and deeper understanding of SGD dynamics

Yang, G. (2019). Tensor Programs I: Wide Feedforward or Recurrent Neural Networks of Any Architecture are Gaussian Processes. arXiv, 1910.12478. Retrieved from https://arxiv.org/abs/1910.12478v3

    - It has been shown that wide NNs with random weights are Gaussian processes
    - We show that this extends to most of modern feedforward or recurrent neural networks
    - This work serves as a tutorial on the tensor programs technique (https://arxiv.org/abs/1902.04760)
    - We provide open-source implementations of the Gaussian Process kernels of simple RNN, GRU, transformer, and batchnorm+ReLU network

Yun, C., Sra, S., & Jadbabaie, A. (2019). Are deep ResNets provably better than linear predictors? arXiv, 1907.03922. Retrieved from https://arxiv.org/abs/1907.03922v2

Zhang, H., Dauphin, Y. N., & Ma, T. (2019). Fixup Initialization: Residual Learning Without Normalization. arXiv, 1901.09321. Retrieved from https://arxiv.org/abs/1901.09321v2

    - Normalization layers are believed to stabilize training, enable higher LR, improve generalization
    - We show that none of the perceived benefits is unique to normalization
    - We propose Fixup initialization via properly rescaling a standard initialization
    - Training ResNets with Fixup is as stable as training with normalization
    - Fixup allows to achieve SOTA on image classification and NMT

#training Zhang, J., Karimireddy, S. P., Veit, A., Kim, S., Reddi, S. J., Kumar, S., & Sra, S. (2019). Why are Adaptive Methods Good for Attention Models? arXiv, 1912.03194. Retrieved from https://arxiv.org/abs/1912.03194v2

    - Why are adaptive methods like Clipped SGD/Adam good for attention models?
    - We show that heavy-tails of the noise in stochastic gradients is one cause of SGD’s poor performance
    - We provide tight upper and lower convergence bounds for adaptive gradient methods under heavy-tailed noise
    - Though clipping speeds up SGD, it does not close the gap between SGD and Adam
    - We develop an adaptive coordinate-wise clipping algorithm
    - We experimentally show that it outperforms Adam on BERT training tasks

Zhou, H., Lan, J., Liu, R., & Yosinski, J. (2019). Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask. arXiv, 1905.01067. Retrieved from https://arxiv.org/abs/1905.01067v4

    - 

## 2020

Allen-Zhu, Z., & Li, Y. (2020). Backward Feature Correction: How Deep Learning Performs Deep (Hierarchical) Learning. arXiv, 2001.04413. Retrieved from https://arxiv.org/abs/2001.04413v6

#uncertainty #training Agarwal, C., D'souza, D., & Hooker, S. (2020). Estimating Example Difficulty Using Variance of Gradients. arXiv, 2008.11600. Retrieved from https://arxiv.org/abs/2008.11600v4

    - We propose Variance of Gradients (VoG) as a metric to rank data by difficulty
    - Data points with high VoG scores are far more difficult for the model to learn and may require memorization
    - VoG provides insight into the learning cycle of the model
    - VoG is a valuable and efficient ranking for out-of-distribution detection

Alemi, A. A., Morningstar, W. R., Poole, B., Fischer, I., & Dillon, J. V. (2020). VIB is Half Bayes. arXiv, 2011.08711. Retrieved from https://arxiv.org/abs/2011.08711v1

#uncertainty #ensembling Ashukha, A., Lyzhov, A., Molchanov, D., & Vetrov, D. (2020). Pitfalls of In-Domain Uncertainty Estimation and Ensembling in Deep Learning. arXiv, 2002.06470. Retrieved from https://arxiv.org/abs/2002.06470v4

    - We point out pitfalls of existing metrics for in-domain uncertainty estimation
    - We introduce the deep ensemble equivalent score (DEE)
    - We compare different ensembling techniques and show that many of them are equivalent to an ensemble of only few independently trained networks in terms of test performance

#training Bachlechner, T., Majumder, B. P., Mao, H. H., Cottrell, G. W., & McAuley, J. (2020). ReZero is All You Need: Fast Convergence at Large Depth. arXiv, 2003.04887. Retrieved from https://arxiv.org/abs/2003.04887v2

    - We propose ReZero: a modification of ResNet by adding a learnable (initially zero) multiplier to residual blocks
    - ReZero initializes each layer to perform the identity operation and satisfies initial dynamical isometry
    - ReZero effectively propagates signals through deep network
    - We are the first to train Transformers over 100 layers without LR warm-up, LayerNorm or auxiliary losses

Chan, K. H. R., Yu, Y., You, C., Qi, H., Wright, J., & Ma, Y. (2020). Deep Networks from the Principle of Rate Reduction. arXiv, 2010.14765. Retrieved from https://arxiv.org/abs/2010.14765v1

    - We construct a CNN layer-by-layer by optimizing the rate reduction of learned features
    - We do this one gradient ascent iteration per layer
    - This "white box" network has precise optimization, statistical, and geometric interpretation
    - This framework justifies the role of multi-channel lifting and sparse coding in early stage of CNN
    - So constructed CNN can learn a good discriminative representation even without any backprop training

Chen, M., Bai, Y., Lee, J. D., Zhao, T., Wang, H., Xiong, C., & Socher, R. (2020). Towards Understanding Hierarchical Learning: Benefits of Neural Representations. arXiv, 2006.13436. Retrieved from https://arxiv.org/abs/2006.13436v2

    - Not easy to understand what happens in this paper!
    - Seems like they use randomly initialized neural network as a source of deep features to train a shallow model
    - They demonstrate that intermediate neural representations can be advantageous over raw inputs

Chen, L., Min, Y., Belkin, M., & Karbasi, A. (2020). Multiple Descent: Design Your Own Generalization Curve. arXiv, 2008.01036. Retrieved from https://arxiv.org/abs/2008.01036v7

    - We show a multiple descent: a generalization curve with many peaks can exist for linear regression
    - Locations of those peaks can be explicitly controlled
    - On the other hand, we rarely observe complex generalization curves in practice
    - We conclude that realistic generalization curves arise from specific interactions between data properties and the inductive biases of algorithms

Chen, Z., Deng, L., Wang, B., Li, G., & Xie, Y. (2020). A Comprehensive and Modularized Statistical Framework for Gradient Norm Equality in Deep Neural Networks. arXiv, 2001.00254. Retrieved from https://arxiv.org/abs/2001.00254v1

    - We propose a novel metric called Block Dynamical Isometry, which measures the change of gradient norm
    - We propose a highly modularized statistical framework based on free probability
    - With our metric and framework we analyze extensive initialization, normalization, and network structures
    - Based on our analysis, we:
    - 1) Improve an activation function selection strategy for initialization techniques
    - 2) Propose a new configuration for weight normalization
    - 3) Propose a depth-aware way to derive coefficients in SeLU
    - 4) Propose a second moment normalization, which is theoretically 30% faster than BatchNorm without accuracy loss

#ood_generalization Choe, Y. J., Ham, J., & Park, K. (2020). An Empirical Study of Invariant Risk Minimization. arXiv, 2004.05007. Retrieved from https://arxiv.org/abs/2004.05007v2

    - We empirically investigate IRMv1, which is the first practical algorithm to approximately solve IRM
    - We use ColoredMNIST and Stanford Sentiment Treebank (SST-2) datasets
    - IRMv1 performs better as the gap between training environments grows larger
    - IRMv1 can perform good even when the association between shape and label is only approximately invariant
    - IRMv1 can perform good with spurious token-to-label correlations in text classification tasks

D'Ascoli, S., Refinetti, M., Biroli, G., & Krzakala, F. (2020). Double Trouble in Double Descent : Bias and Variance(s) in the Lazy Regime. arXiv, 2003.01054. Retrieved from https://arxiv.org/abs/2003.01054v2

#fine_tuning Dodge, J., Ilharco, G., Schwartz, R., Farhadi, A., Hajishirzi, H., & Smith, N. (2020). Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping. arXiv, 2002.06305. Retrieved from https://arxiv.org/abs/2002.06305v1

    - Problem: fine-tuning BERT is seed-dependent
    - We examine the effects of random initialization and random training order on fine-tuning BERT
    - We find that both contribute comparably, some weight initializations perform well across all tasks explored
    - Many fine-tuning trials diverge part of the way through training
    - We offer best practices for practitioners to stop training less promising runs early

Domingos, P. (2020). Every Model Learned by Gradient Descent Is Approximately a Kernel Machine. arXiv, 2012.00152. Retrieved from https://arxiv.org/abs/2012.00152v1

    - NNs trained by SGD, regardless of architecture, are approximately equivalent to kernel machines
    - Kernel machine store a subset of the training data points and match them to the query using the kernel
    - So, NNs are effectively a superposition of the training examples
    - This contrasts with the standard view of DL as a method for discovering representations from data
    - Our result also has significant implications for boosting algorithms, probabilistic graphical models and convex optimization
    - Also some discussion here: https://www.reddit.com/r/MachineLearning/comments/k8h01q/r_wide_neural_networks_are_feature_learners_not/?rdt=59640

#thinking Ellis, K., Wong, C., Nye, M., Sable-Meyer, M., Cary, L., Morales, L., ...Tenenbaum, J. B. (2020). DreamCoder: Growing generalizable, interpretable knowledge with wake-sleep Bayesian program learning. arXiv, 2006.08381. Retrieved from https://arxiv.org/abs/2006.08381v1

    - We present DreamCoder that learns to solve problems by writing programs
    - It solves both classic inductive programming tasks and creative tasks such as drawing pictures and building scenes
    - A “wake-sleep” learning algorithm alternately extends the language with new symbolic abstractions and trains the neural network on imagined and replayed problems
    - Concepts are built compositionally from those learned earlier, yielding multilayered symbolic representations that are interpretable and transferrable to new tasks
    - We experimentally investigate DreamCoder on list processing and text editing

Faghri, F., Duvenaud, D., Fleet, D. J., & Ba, J. (2020). A Study of Gradient Variance in Deep Learning. arXiv, 2007.04532. Retrieved from https://arxiv.org/abs/2007.04532v1

Fang, C., Lee, J. D., Yang, P., & Zhang, T. (2020). Modeling from Features: a Mean-field Framework for Over-parameterized Deep Neural Networks. arXiv, 2007.01452. Retrieved from https://arxiv.org/abs/2007.01452v1

    - A new framework to analyze neural network training
    - We capture the evolution of an over-parameterized DNN trained by Gradient Descent
    - Global convergence proof for over-parameterized DNN in the mean-field regime

Fort, S., Dziugaite, G. K., Paul, M., Kharaghani, S., Roy, D. M., & Ganguli, S. (2020). Deep learning versus kernel learning: an empirical study of loss landscape geometry and the time evolution of the Neural Tangent Kernel. arXiv, 2010.15110. Retrieved from https://arxiv.org/abs/2010.15110v1

#ood_generalization Furrer, D., van Zee, M., Scales, N., & Schärli, N. (2020). Compositional Generalization in Semantic Parsing: Pre-training vs. Specialized Architectures. arXiv, 2007.08970. Retrieved from https://arxiv.org/abs/2007.08970v3

    - We evaluate methods on complex compositional tasks
    - Pre-training performs better than specialized architectures proposed to encourage compositional generalization
    - We establish a new SOTA on the CFQ compositional generalization benchmark using MLM pre-training

George, D., Lázaro-Gredilla, M., & Guntupalli, J. S. (2020). From CAPTCHA to Commonsense: How Brain Can Teach Us About Artificial Intelligence. Front. Comput. Neurosci., 14, 554097. Retrieved from https://www.frontiersin.org/articles/10.3389/fncom.2020.554097/full

Gorbunov, E., Danilova, M., & Gasnikov, A. (2020). Stochastic Optimization with Heavy-Tailed Noise via Accelerated Gradient Clipping. arXiv, 2005.10785. Retrieved from https://arxiv.org/abs/2005.10785v2

    - We propose clipped-SSTM: a first-order method for smooth convex stochastic optimization
    - Clipped-SSTM is for heavy-tailed distributed noise in stochastic gradients
    - Clipped-SSTM is based on SGD and and gradient clipping
    - We derive some theoretical bounds

Heckel, R., & Yilmaz, F. F. (2020). Early Stopping in Deep Networks: Double Descent and How to Eliminate it. arXiv, 2007.10099. Retrieved from https://arxiv.org/abs/2007.10099v2

    - Previously Nakkiran et al. in "Deep double descent: Where bigger models and more data hurt" conjectured that epoch-wise double descent occurs because the training time controls the “effective model complexity”.
    - We show a different reason: epoch-wise double descent arises becuse the risk can be decomposed into two U-shaped bias-variance tradeoffs with minima at different epochs/iterations (fig. 1b). So, both under- and overparamterized models can have epoch-wise double descent.
    - In a two-layer NN, the initialization scales and stepsizes of the weights in the first and second layer determine whether double descent occurs or not.
    - For the 5-layer CNN epoch-wise double descent occurs because the convolutional layers are learned faster than the final, fully connected layer, which results in a superposition of bias-variance tradeoffs. For ResNet-18, later layers are learned faster than early layers, which again results in double descent. In both cases, epoch-wise double descent can be eliminated through adjusting the stepsizes (learning rate?) of different coefficients or layers. This allows for early stopping to be more reliable.

Hu, W., Xiao, L., & Pennington, J. (2020). Provable Benefit of Orthogonal Initialization in Optimizing Deep Linear Networks. arXiv, 2001.05992. Retrieved from https://arxiv.org/abs/2001.05992v1

    - Proof that orthogonal initialization speeds up convergence
    - With it, the width for efficient convergence is independent of the depth (without it does not)
    - Is related to the principle of dynamical isometry (https://arxiv.org/abs/1312.6120)

Huang, K., Wang, Y., Tao, M., & Zhao, T. (2020). Why Do Deep Residual Networks Generalize Better than Deep Feedforward Networks? -- A Neural Tangent Kernel Perspective. arXiv, 2002.06262. Retrieved from https://arxiv.org/abs/2002.06262v2

Jastrzebski, S., Szymczak, M., Fort, S., Arpit, D., Tabor, J., Cho, K., & Geras, K. (2020). The Break-Even Point on Optimization Trajectories of Deep Neural Networks. arXiv, 2002.09572. Retrieved from https://arxiv.org/abs/2002.09572v1

    - We show that the key properties of the loss surface are strongly influenced by SGD in the early training phase
    - Using a large LR in the early training phase is beneficial from the optimization perspective
    - It reduces the variance of the gradient, and improves the conditioning of the covariance of gradients
    - Using a low LR in the early training phase results in bad conditioning even with BatchNorm

Kidger, P., Morrill, J., Foster, J., & Lyons, T. (2020). Neural Controlled Differential Equations for Irregular Time Series. arXiv, 2005.08926. Retrieved from https://arxiv.org/abs/2005.08926v2

    - Problem in neural ODEs: no mechanism for adjusting the trajectory based on subsequent observations
    - We demonstrate how this may be resolved through the mathematics of controlled differential equations
    - This is applicable to the partially observed irregularly-sampled multivariate time series
    - SOTA performance against similar (ODE or RNN based) models in empirical studies on a range of datasets
    - Theoretical results demonstrating universal approximation

#inductive_biases #uncertainty Lengerich, B., Xing, E. P., & Caruana, R. (2020). Dropout as a Regularizer of Interaction Effects. arXiv, 2007.00823. Retrieved from https://arxiv.org/abs/2007.00823v2

    - We show that Dropout regularizes against higher-order interactions
    - So, high Dropout is useful when we need stronger regularization against spurious high-order interactions
    - When NNs are trained on data with important interaction effects, the optimal Dropout rate is lower
    - Weight decay and early stopping do not achieve Dropout’s regularization against high-order interactions
    - Caution should be exercised when interpreting Dropout-based uncertainty measures

#training Liu, L., Liu, X., Gao, J., Chen, W., & Han, J. (2020). Understanding the Difficulty of Training Transformers. arXiv, 2004.08249. Retrieved from https://arxiv.org/abs/2004.08249v3

    - It is known that SGD fails to train transformers effectively
    - We show that unbalanced gradients are not the root cause of this problem
    - We discover an effect inside the transformer that amplifies small parameter perturbations
    - We propose Admin (Adaptive model initialization) to stabilize the early stage’s training

Liu, C., Zhu, L., & Belkin, M. (2020). Loss landscapes and optimization in over-parameterized non-linear systems and neural networks. arXiv, 2003.00307. Retrieved from https://arxiv.org/abs/2003.00307v2

    - We ovserve that NN optimization is generally not convex, even locally
    - Convexity is not the right framework for analysis of over-parameterized systems
    - Instead, wide NNs satisfy a variant of the Polyak-Lojasiewicz condition on most of the parameter space
    - This guarantees an efficient convergence by SGD to a global minimum
    - This is closely related to the condition number of the NTK

#uncertainty #ensembling Lobacheva, E., Chirkova, N., Kodryan, M., & Vetrov, D. (2020). On Power Laws in Deep Ensembles. arXiv, 2007.08483. Retrieved from https://arxiv.org/abs/2007.08483v2

    - It was shown (https://arxiv.org/abs/2002.06470) that calibrated negative log-likelihood (CNLL) of a deep ensemble measures its quality of uncertainty estimation
    - We examine CNLL of a deep ensemble as a function of the ensemble size and the member network size
    - Under several conditions it follows a power law w. r. t. ensemble size or member network size
    - We find that one large network may perform worse than an ensemble of several medium-size networks with the same total number of parameters (we call this ensemble a memory split)
    - With these results we can guess the possible gain from the ensembling and the optimal memory split

Mania, H., & Sra, S. (2020). Why do classifier accuracies show linear trends under distribution shift? arXiv, 2012.15483. Retrieved from https://arxiv.org/abs/2012.15483v2

    - The current work is based on our previous work "Model Similarity Mitigates Test Set Overuse", when we have shown that most ImageNet models strongly agree in their predictions.
    - We translate this observation into the following assumption: high accuracy models correctly classify most of the data points correctly classified by lower accuracy models. This assumptions is satisfied by ImageNet and CIFAR-10 models.
    - Under this assumption we show that models must be approximately collinear when evaluated on two distributions, unless the size of the distribution shift is large in a certain sense.
    - To define distributional closeness, we use a specific (δ1, δ2, ν1, ν2)-closeness measure based on triplets of models. We provide empirical motivation for this definition and a more detailed discussion.
    - However, we still do not understand why models are similar. Are models similar due to the training data, the model classes, or something else?

Millidge, B., Tschantz, A., Seth, A. K., & Buckley, C. L. (2020). On the Relationship Between Active Inference and Control as Inference. arXiv, 2006.12964. Retrieved from https://arxiv.org/abs/2006.12964v3

Millidge, B., Tschantz, A., & Buckley, C. L. (2020). Whence the Expected Free Energy? arXiv, 2004.08128. Retrieved from https://arxiv.org/abs/2004.08128v5

Millidge, B., Tschantz, A., & Buckley, C. L. (2020). Predictive Coding Approximates Backprop along Arbitrary Computation Graphs. arXiv, 2006.04182. Retrieved from https://arxiv.org/abs/2006.04182v5

Mundt, M., Hong, Y., Pliushch, I., & Ramesh, V. (2020). A Wholistic View of Continual Learning with Deep Neural Networks: Forgotten Lessons and the Bridge to Active and Open World Learning. arXiv, 2009.01797. Retrieved from https://arxiv.org/abs/2009.01797v3

Muthukumar, V., Narang, A., Subramanian, V., Belkin, M., Hsu, D., & Sahai, A. (2020). Classification vs regression in overparameterized regimes: Does the loss function matter? arXiv, 2005.08054. Retrieved from https://arxiv.org/abs/2005.08054v2

    - We analyze the overparameterized regime under the linear model with Gaussian features
    - In this case every training sample is a support vector
    - Consequently, the outcome of GD optimization is the same whether we use the hinge, square or logistic loss
    - On the other hand, the choice of test loss function results in a significant asymptotic difference: some overparameterized predictors will generalize poorly for square loss but well for 0-1 loss

Nakkiran, P., & Bansal, Y. (2020). Distributional Generalization: A New Kind of Generalization. arXiv, 2009.08092. Retrieved from https://arxiv.org/abs/2009.08092v2

    - We propose studying the entire distribution of classifier outputs on test samples, beyond just its test error.
    - We propose a notion of "distributional generalization", which is a property of trained classifiers (eq. 2) that is met when the train and test behavior of models are close as distributions. If the model fits its train set exactly (is an "interpolating classifier"), then this means that the output (argmax) distribution of the model on test samples is close to the true distribution. This notion is more fine-grained than classical generalization, since it considers the entire distribution of model outputs instead of just the test error.
    - The "agreement Property", informally, is that the test accuracy of a classifier is close to the probability that it agrees with an identically-trained classifier on a disjoint train set. If this is true, the classifier met distributional generalization.
    - In our experiments, we train a pair of classifiers on random disjoint subsets of the train set for a given distribution. The Agreement Property approximately holds for all pairs of identical classifiers, and continues to hold even for "weak" classifiers (e.g. when f1, f2 have high test error).
    - IMO, for example, distributional generalization means that if in binary classification, given some samples subset X, for which the real class probabilities are 30% and 70%, the model should argmax-classify samples from X as the first class in 30% cases and as the second class in 70% cases. This is different from the optimal Bayes classifier, that will argmax-classify all samples from X as the second class. But maybe I am wrong.
    - IMO, the authors seem to incorrectly define "classical generalization", what they say is acrually a "generalization gap". IMO it means that the model correctly models the data distribution, but the train set performance does not matter, and not only average test error matters (the authors say that "classical generalization considers just the test error").
    - IMO, if model is good at estimating p(y|x) and well-callibrated, then to met "distributional generalization" we just need to sample from p(y|x).

Timothy P. Lillicrap, #., Adam Santoro, #., Marris, L., Akerman, C. J., & Hinton, G. (2020). Backpropagation and the brain. Nat. Rev. Neurosci., 32303713. Retrieved from https://pubmed.ncbi.nlm.nih.gov/32303713

Lu, Y., Ma, C., Lu, Y., Lu, J., & Ying, L. (2020). A Mean-field Analysis of Deep ResNet and Beyond: Towards Provable Optimization Via Overparameterization From Depth. arXiv, 2003.05508. Retrieved from https://arxiv.org/abs/2003.05508v2

    - Question: why do ResNets achieve zero training loss, while optimization landscape is highly non-convex?
    - We propose a new continuum limit of deep ResNets with a good landscape where every local minimizer is global
    - We apply existing mean-field analyses of two-layer networks to deep networks
    - We propose several novel training schemes which result in strong empirical performance

Melkman, A. A., Guo, S., Ching, W.-K., Liu, P., & Akutsu, T. (2020). On the Compressive Power of Boolean Threshold Autoencoders. arXiv, 2004.09735. Retrieved from https://arxiv.org/abs/2004.09735v1

Mixon, D. G., Parshall, H., & Pi, J. (2020). Neural collapse with unconstrained features. arXiv, 2011.11619. Retrieved from https://arxiv.org/abs/2011.11619v1

#training Papyan, V., Han, X. Y., & Donoho, D. L. (2020). Prevalence of Neural Collapse during the terminal phase of deep learning training. arXiv, 2008.08186. Retrieved from https://arxiv.org/abs/2008.08186v2

    - One of the standard workflow practices is training beyond zero-error to zero-loss
    - We discuss a Terminal Phase of Training (TPT), when training accuracy is 1, but training loss is still lowering
    - We measure TPT for 3 deep architectures and 7 classification datasets
    - We expose a pervasive inductive bias we call Neural Collapse
    - 1) Last layer class representations collapse to points
    - 2) These points collapse to the vertices of a Simplex Equiangular Tight Frame (fig. 1)
    - Convergence to this simple structure is beneficial: it improves test performance and adversarial robustness

#inductive_biases Pezeshki, M., Kaba, S.-O., Bengio, Y., Courville, A., Precup, D., & Lajoie, G. (2020). Gradient Starvation: A Learning Proclivity in Neural Networks. arXiv, 2011.09468. Retrieved from https://arxiv.org/abs/2011.09468v4

    - We provide a theoretical explanation for Gradient Starvation
    - It arises when loss is minimized by capturing only a subset of relevant features
    - Other predictive features fail to be discovered
    - Such a situation can be expected given certain statistical structure in training data
    - We propose Spectral Decoupling (SD): a regularization method aimed at decoupling feature learning dynamics
    - We experiment on classification and adversarial attack tasks

#nmt Raunak, V., Dalmia, S., Gupta, V., & Metze, F. (2020). On Long-Tailed Phenomena in Neural Machine Translation. arXiv, 2010.04924. Retrieved from https://arxiv.org/abs/2010.04924v1

    - Problem: NMT models struggle with generating low-frequency tokens
    - Penalizing low-confidence predictions hurts beam search performance
    - We propose Anti-Focal loss, a generalization of Focal loss and cross-entropy
    - Anti-Focal loss allocates less relative loss to low-confidence predictions
    - It leads to significant gains over cross-entropy, especially on the generation of low-frequency words

Queiruga, A. F., Erichson, N. B., Taylor, D., & Mahoney, M. W. (2020). Continuous-in-Depth Neural Networks. arXiv, 2008.02389. Retrieved from https://arxiv.org/abs/2008.02389v1

Sankar, A. R., Khasbage, Y., Vigneswaran, R., & Balasubramanian, V. N. (2020). A Deeper Look at the Hessian Eigenspectrum of Deep Neural Networks and its Applications to Regularization. arXiv, 2012.03801. Retrieved from https://arxiv.org/abs/2012.03801v2

    - We study the eigenspectra of the Hessian at each DNN layer
    - We propose a new regularizer: Layerwise Hessian Trace Regularization (HTR)
    - It forces Stochastic Gradient Descent to converge to flatter minima

#nmt #augmentations Shen, D., Zheng, M., Shen, Y., Qu, Y., & Chen, W. (2020). A Simple but Tough-to-Beat Data Augmentation Approach for Natural Language Understanding and Generation. arXiv, 2009.13818. Retrieved from https://arxiv.org/abs/2009.13818v2

    - Problem: adversarial training has been shown effective but requires expensive computation
    - We propose a cutoff: a data augmentation strategy, where part of the input sentence is erased
    - For these samples we use a Jensen-Shannon Divergence consistency loss
    - We apply cutoff to both understanding and generation problems, including GLUE and NMT
    - Cutoff consistently outperforms adversarial training

Sun, R. (2020). Optimization for Deep Learning: An Overview. J. Oper. Res. Soc. China, 8(2), 249–294. doi: 10.1007/s40305-020-00309-6 https://www.ise.ncsu.edu/fuzzy-neural/wp-content/uploads/sites/9/2020/08/Optimization-for-deep-learning.pdf   
  
    - A survey paper. We discuss:  
    - The issue of undesirable spectrum, including gradient explosion/vanishing  
    - Solutions such as careful initialization, normalization, and skip connections  
    - SGD, adaptive gradient methods, and existing theoretical results  
    - Results on global landscape, mode connectivity, lottery ticket hypothesis and NTK

Sun, R., Li, D., Liang, S., Ding, T., & Srikant, R. (2020). The Global Landscape of Neural Networks: An Overview. arXiv, 2007.01429. Retrieved from https://arxiv.org/abs/2007.01429v1   
  
    - A survey paper. We discuss:  
    - That wide NNs may have sub-optimal local minima under certain assumptions  
    - Geometric properties of wide NNs  
    - Some modifications that eliminate sub-optimal local minima and/or decreasing paths to infinity  
    - Visualization and empirical explorations of the loss landscape  
    - Some convergence results  
    - Compared to another survey ("Optimization for Deep Learning: An Overview" from the same author), this article focuses on global landscape and contains formal theorem statements, while they covered many aspects of neural net optimization and did not present formal theorems.

Tschantz, A., Millidge, B., Seth, A. K., & Buckley, C. L. (2020). Reinforcement Learning through Active Inference. arXiv, 2002.12636. Retrieved from https://arxiv.org/abs/2002.12636v1

Tschantz, A., Millidge, B., Seth, A. K., & Buckley, C. L. (2020). Control as Hybrid Inference. arXiv, 2007.05838. Retrieved from https://arxiv.org/abs/2007.05838v1

#attention Vuckovic, J., Baratin, A., & Combes, R. T. d. (2020). A Mathematical Theory of Attention. arXiv, 2007.02876. Retrieved from https://arxiv.org/abs/2007.02876v2

    - We show that attention can be interpreted as system of interacting particles, and we model the evolution of this system as a nonlinear transformation of probability measures
    - We show that attention contains the solution to a maximum entropy problem and a minimum entropy projection, and is therefore characterized by these problems
    - We show that attention is a Lipschitz-continuous operation under suitable assumptions, and give quantitative estimates of the Lipschitz coefficient
    - We apply these theories to study sub-sampled input data; infinitely-deep, weight-sharing self-attention networks

Wang, L., Shen, B., Zhao, N., & Zhang, Z. (2020). Is the Skip Connection Provable to Reform the Neural Network Loss Landscape? arXiv, 2006.05939. Retrieved from https://arxiv.org/abs/2006.05939v1

Wen, Y., Tran, D., & Ba, J. (2020). BatchEnsemble: An Alternative Approach to Efficient Ensemble and Lifelong Learning. arXiv, 2002.06715. Retrieved from https://arxiv.org/abs/2002.06715v2

#uncertainty #ensembling Wilson, A. G., & Izmailov, P. (2020). Bayesian Deep Learning and a Probabilistic Perspective of Generalization. arXiv, 2002.08791. Retrieved from https://arxiv.org/abs/2002.08791v4

    - We propose MultiSWAG to significantly improve Deep Ensembles  
    - MultiSWAG alleviates double descent  
    - Deep Ensembles are not a competing approach to Bayesian inference, but are a mechanism for Bayesian marginalization that provides a better approximation to the Bayesian predictive distribution than standard Bayesian approaches  
    - Fitting random labels (https://arxiv.org/abs/1611.03530) can be understood by reasoning about prior distributions over functions, and are not specific to NNs  
    - Gaussian processes can also perfectly fit images with random labels, yet generalize on the noise-free problem

#training Xiong, R., Yang, Y., He, D., Zheng, K., Zheng, S., Xing, C., ...Liu, T.-Y. (2020). On Layer Normalization in the Transformer Architecture. arXiv, 2002.04745. Retrieved from https://arxiv.org/abs/2002.04745v2

    - We theoretically study why the LR warm-up stage is essential for transformers  
    - We show that in a recently proposed Pre-LN Transformer the gradients are well-behaved at initialization  
    - Pre-LN Transformers without the warm-up stage can reach comparable results with baselines while requiring significantly less training time

Yang, G. (2020). Tensor Programs II: Neural Tangent Kernel for Any Architecture. arXiv, 2006.14548. Retrieved from https://arxiv.org/abs/2006.14548v4

    - We review the tensor programs technique
    - We prove that infinitely-wide NN of any architecture has its NTK converge to a deterministic limit
    - We demonstrate how to calculate this limit
    - We decribe a Simple GIA Check to check gradient independence assumption (GIA) used in NTK
    - When Simple GIA Check fails, we show GIA can result in wrong answers
    - We implement infinite-width NTKs of RNN, transformer, and batch normalization in a repo

Yang, G. (2020). Tensor Programs III: Neural Matrix Laws. arXiv, 2009.10685. Retrieved from https://arxiv.org/abs/2009.10685v3

    - We study intinitely-wide NNs with a random matrix theory and derive the Free Independence Principle (FIP)
    - It justifies the calculation of Jacobian singular value distribution of intinitely-wide NN
    - It gives a new justification of gradient independence assumption used for calculating NTK
    - We generalize the Master Theorems from previous works

Yang, G., & Hu, E. J. (2020). Feature Learning in Infinite-Width Neural Networks. arXiv, 2011.14522. Retrieved from https://arxiv.org/abs/2011.14522v3

    - This is "Tensor Programs IV" (another 11-list version: https://proceedings.mlr.press/v139/yang21c/yang21c.pdf)
    - Using the Tensor Programs we adapt NTK to the case of pre-training and transfer learning such as with BERT
    - We compute an infinite-width limits on Word2Vec and few-shot learning on Omniglot via MAML
    - Such feature-learning limit outperforms both the NTK and the finite-width neural networks
    - We classify a space of NN parametrizations that generalizes standard, NTK, and Mean Field parametrizations
    - https://www.reddit.com/r/MachineLearning/comments/k8h01q/r_wide_neural_networks_are_feature_learners_not/
    - The title really should be something like “To Explain Pretraining and Transfer Learning, Wide Neural Networks Should Be Thought of as Feature Learners, Not Kernel Machines” but that’s really long
    - So, NNs can be kernel machines, but we can understand them better as feature learners
    - More precisely, the same NN can have different infinite-width limits, depending on the parametrization
    - A big contribution of this paper is classifying what kind of limits are possible

#ood_generalization Yang, Z., Yu, Y., You, C., Steinhardt, J., & Ma, Y. (2020). Rethinking Bias-Variance Trade-off for Generalization of Neural Networks. arXiv, 2002.11328. Retrieved from https://arxiv.org/abs/2002.11328v3

    - We study double descent and confirm that variance is unimodal or bell-shaped  
    - This occurs robustly for all models we considered  
    - Accuracy drops on OOD data comes from increased bias  
    - Deeper models decrease bias and increase variance (for both IID and OOD data)  
    - Increasing model depth may help combat the drop in OOD accuracy

#few_shot #fine_tuning Zhao, M., Zhu, Y., Shareghi, E., Vulić, I., Reichart, R., Korhonen, A., & Schütze, H. (2020). A Closer Look at Few-Shot Crosslingual Transfer: The Choice of Shots Matters. arXiv, 2012.15682. Retrieved from https://arxiv.org/abs/2012.15682v2

    - We conduct a largescale study of few-shot crosslingual transfer on diverse NLP tasks and languages
    - We find that the model exhibits a high degree of sensitivity to the selection of few shots
    - We provide an analysis of success and failure cases
    - We find that a straightforward fine-tuning outperforms several SOTA few-shot approaches

Zhao, P., Chen, P.-Y., Das, P., Ramamurthy, K. N., & Lin, X. (2020). Bridging Mode Connectivity in Loss Landscapes and Adversarial Robustness. arXiv, 2005.00060. Retrieved from https://arxiv.org/abs/2005.00060v2

#fine_tuning Zhou, W., Lin, B. Y., & Ren, X. (2020). IsoBN: Fine-Tuning BERT with Isotropic Batch Normalization. arXiv, 2005.02178. Retrieved from https://arxiv.org/abs/2005.02178v2

    - It was shown that isotropic embeddings can significantly improve performance
    - We study how isotropic (unit-variance and uncorrelated) are output [CLS] embeddings of pre-trained language models
    - We find high variance in their standard deviation, and high correlation between different dimensions
    - We propose isotropic batch normalization (IsoBN) regularization that penalizes dominating principal components
    - IsoBN allows to learn more isotropic representations in fine-tuning
    - We achieve improvement on the average of seven NLU tasks

Zhou, X., Nie, Y., Tan, H., & Bansal, M. (2020). The Curse of Performance Instability in Analysis Datasets: Consequences, Source, and Suggestions. arXiv, 2004.13606. Retrieved from https://arxiv.org/abs/2004.13606v2

    - Previous works have shown performance instabilities, for example, different runs of BERT NLI models have large non-negligible variances on the HANS, contrasting sharply with their stable results on standard validation set across multiple seeds (see "BERTs of a feather do not generalize together: Large variability in generalization across models with similar test set performance"). This finding raises concerns regarding the reliability of individual results reported on those datasets and the conclusions made upon these results.
    - To address this problem, we conduct a deep investigation on model instability.
    - We observe that the final results of the same model with different random seeds on several analysis sets are of significantly high variance (more than 27 times of that for standard development set, fig. 2). The results of the same model on analysis sets and on the standard development set have low correlation (fig. 3). Certain datasets have unstable results across different models. The instability exists all along training trajectory (fig. 1).
    - We show that inter-examples correlation within the dataset is the dominating factor causing this performance instability. If there are many examples correlated with each other in the evaluation set, then the change of prediction on one example will influence predictions on all the correlated examples (fig. 4), causing high variances in final accuracy.
    - Our short term suggestion is to report the decomposed variance (Idp Var and Cov). The first number (independent variance, i.e., Idp Var) can be viewed as a metric regarding how stable the model makes one single prediction and this number can be compared across different models. A high Cov indicates that many examples look similar to the model, and the model may be exploiting some common artifacts in this group of examples. A lower Cov usually means that the dataset is diverse and is preferable for evaluation. A more stable model should aim to improve the total variance with more focus on Idp Var.
    - Our long term suggestion is to focus on improving models (including better inductive biases, large-scale pre-training with tasks concerning structure/compositionality) so that they can get high accuracy stably. We also encourage the construction of more diverse datasets (in terms of syntax and lexicon). Datasets from natural real-life sources usually lead to lower covariance between predictions and show better stability. While controlled synthetic datasets are more accurate and effective in evaluating certain linguistic phenomenon, the lack of diversity may increase the model’s ability to guess the answer right and solve only that single pattern/property.

## 2021

Aguilera, M., Millidge, B., Tschantz, A., & Buckley, C. L. (2021). How particular is the physics of the free energy principle? arXiv, 2105.11203. Retrieved from https://arxiv.org/abs/2105.11203v3

Bartlett, P. L., Montanari, A., & Rakhlin, A. (2021). Deep learning: a statistical viewpoint. arXiv, 2103.09177. Retrieved from https://arxiv.org/abs/2103.09177v1

    - A survey paper on deep learning with overparametrization
    - We focus specifically on the linear regime for neural networks
    - We consider benign overfitting with two-layer networks

Belkin, M. (2021). Fit without fear: remarkable mathematical phenomena of deep learning through the prism of interpolation. arXiv, 2105.14368. Retrieved from https://arxiv.org/abs/2105.14368v1

    - A survey paper on the foundations of DL through the prism of interpolation and over-parameterization

Bello, I., Fedus, W., Du, X., Cubuk, E. D., Srinivas, A., Lin, T.-Y., ...Zoph, B. (2021). Revisiting ResNets: Improved Training and Scaling Strategies. arXiv, 2103.07579. Retrieved from https://arxiv.org/abs/2103.07579v1

Benton, G. W., Maddox, W. J., Lotfi, S., & Wilson, A. G. (2021). Loss Surface Simplexes for Mode Connecting Volumes and Fast Ensembling. arXiv, 2102.13042. Retrieved from https://arxiv.org/abs/2102.13042v2

Berariu, T., Czarnecki, W., De, S., Bornschein, J., Smith, S., Pascanu, R., & Clopath, C. (2021). A study on the plasticity of neural networks. arXiv, 2106.00042. Retrieved from https://arxiv.org/abs/2106.00042v2

    - We focus on plasticity, namely the ability of the model to keep learning; when NNs lose this ability?
    - For example, PackNet (Mallya & Lazebnik, 2017) eventually gets to a point where all neurons are frozen and learning is not possible anymore; alternatively, learning might become less data efficient (negative forward transfer)
    - We build on https://arxiv.org/abs/1910.08475, provide a hypothesis about it and study the implications

#training Bingham, G., & Miikkulainen, R. (2021). AutoInit: Analytic Signal-Preserving Weight Initialization for Neural Networks. arXiv, 2109.08958. Retrieved from https://arxiv.org/abs/2109.08958v2

    - A weight initialization algorithm that automatically adapts to different architectures
    - Scales the weights by tracking the mean and variance of signals as they propagate through the network
    - Improves performance of convolutional, residual, and transformer networks

Bond-Taylor, S., Leach, A., Long, Y., & Willcocks, C. G. (2021). Deep Generative Modelling: A Comparative Review of VAEs, GANs, Normalizing Flows, Energy-Based and Autoregressive Models. arXiv, 2103.04922. Retrieved from https://arxiv.org/abs/2103.04922v4

    - A survey paper about deep generative models reserch which has fragmented into various approaches
    - These approaches make trade-offs including run-time, diversity, and architectural restrictions

#training Brock, A., De, S., Smith, S. L., & Simonyan, K. (2021). High-Performance Large-Scale Image Recognition Without Normalization. arXiv, 2102.06171. Retrieved from https://arxiv.org/abs/2102.06171v1

    - Problem: BatchNorm has many undesirable properties (is computationally expensive, perform poorly when the batch size is too small, introduces a train-test discrepancy, is often the cause of subtle implementation errors, cannot be used for some tasks due to interaction between training examples - we discuss it in Appendix B)
    - Problem: Normalizer-Free networks are often unstable for large learning rates or strong data augmentations
    - We propose NFNets: Normalizer-Free networks with an adaptive gradient clipping technique to overcome instabilities
    - With our NFNets, we achieve SOTA on ImageNet, fast convergence, better fine-tuning performance

Cao, S. (2021). Choose a Transformer: Fourier or Galerkin. arXiv, 2105.14995. Retrieved from https://arxiv.org/abs/2105.14995v4

    - We apply self-attention to a data-driven operator learning problem related to PDE
    - We present three operator learning experiments
    - We demonstrate that the softmax normalization is sufficient but not necessary
    - We propose Fourier Transformer (FT) with the Fourier-type encoder, and the Galerkin Transformer (GT) with the Galerkin-type encoder (?) to improve quality in PDE-related operator learning tasks
    - https://scaomath.github.io/blog/galerkin-transformer/

Chen, Y., Huang, W., Nguyen, L. M., & Weng, T.-W. (2021). On the Equivalence between Neural Network and Support Vector Machine. arXiv, 2111.06063. Retrieved from https://arxiv.org/abs/2111.06063v2

Cohen, A.-S., Cont, R., Rossier, A., & Xu, R. (2021). Scaling Properties of Deep Residual Networks. arXiv, 2105.12245. Retrieved from https://arxiv.org/abs/2105.12245v2

    - We investigate the scaling behavior of trained ResNet weights as the number of layers increases
    - We found at least three different scaling regimes
    - In two of these regimes, the properties may be described in terms of a class of ODEs or SDEs

Dar, Y., Muthukumar, V., & Baraniuk, R. G. (2021). A Farewell to the Bias-Variance Tradeoff? An Overview of the Theory of Overparameterized Machine Learning. arXiv, 2109.02355. Retrieved from https://arxiv.org/abs/2109.02355v1

    - A survey paper about the theory of overparameterized ML
    - Compared to other surveys, we take a more elementary signal processing perspective to elucidate these principles

Ding, Z., Chen, S., Li, Q., & Wright, S. (2021). Overparameterization of deep ResNet: zero loss and mean-field analysis. arXiv, 2105.14417. Retrieved from https://arxiv.org/abs/2105.14417v3

    - Study ResNet convergence in the infinite-depth and infinite-width regime
    - GD becomes a gradient flow for a probability distribution that is characterized by a PDE
    - Results suggest that the training of the large enough ResNet gives a near-zero loss
    - Estimates of the depth and width needed to reduce the loss below a given threshold

Entezari, R., Sedghi, H., Saukh, O., & Neyshabur, B. (2021). The Role of Permutation Invariance in Linear Mode Connectivity of Neural Networks. arXiv, 2110.06296. Retrieved from https://arxiv.org/abs/2110.06296v2

    - We conjecture that by taking permutation invariance into account, the loss landscape of NNs can be simplified significantly, resulting in linear mode connectivity (LMC) between SGD solutions trained from different initializations.
    - Our experiments fall short of refuting this hypothesis and end up as supporting evidence for it.
    - We investivate how LMC is affected bywidth, depth and task difficulty for FCNs and CNNs.
    - Our conjecture has implications for lottery ticket hypothesis, distributed training and ensemble methods.

Federici, M., Tomioka, R., & Forré, P. (2021). An Information-theoretic Approach to Distribution Shifts. arXiv, 2106.03783. Retrieved from https://arxiv.org/abs/2106.03783v2

    - We introduce a novel information-theoretical framework for the problem of distribution shift
    - We analyze four main families of objectives (bottleneck, independence, sufficiency, and separation) and describe some of their guarantees and assumptions
    - The effectiveness of these criteria is determined by the structure of the underlying data-generating process

Fortuin, V. (2021). Priors in Bayesian Deep Learning: A Review. arXiv, 2105.06868. Retrieved from https://arxiv.org/abs/2105.06868v3

    - A review paper
    - We highlight the importance of prior choices for Bayesian DL
    - We hope to motivate practitioners to think more carefully about the prior specification
    - We present an overview of different priors for (deep) Gaussian processes, VAE, and Bayesian NN
    - We outline different methods of learning priors for these models from data

Gilmer, J., Ghorbani, B., Garg, A., Kudugunta, S., Neyshabur, B., Cardoze, D., ...Firat, O. (2021). A Loss Curvature Perspective on Training Instability in Deep Learning. arXiv, 2110.04369. Retrieved from https://arxiv.org/abs/2110.04369v1

    - We study the evolution of the loss Hessian during training
    - We analyze the effects of initialization, architecture, heuristics such as gradient clipping and LR warmup
    - It is desirable to allow the early optimization trajectory to avoid (or navigate out of) regions of high curvature and go into flatter regions that tolerate a higher LR
    - We show that LR warmup can improve training stability just as much as batch normalization, layer normalization, MetaInit, GradInit, and Fixup initialization

Hinton, G. (2021). How to represent part-whole hierarchies in a neural network. arXiv, 2102.12627. Retrieved from https://arxiv.org/abs/2102.12627v1

    - How can a NN with a fixed architecture parse an image into a part-whole hierarchy (different for each image)?
    - The idea is simply to use islands of identical vectors to represent the nodes in the parse tre
    - We propose an imaginary system called GLOM that should significantly improve the interpretability
    - The GLOM architecture is composed of a large number of columns: stacks of spatially local autoencoders
    - The GLOM can be trained end-to-end to reconstruct images with missing regions, but the objective function also includes two regularizers that encourage islands of near identical vectors at each level

Han, X. Y., Papyan, V., & Donoho, D. L. (2021). Neural Collapse Under MSE Loss: Proximity to and Dynamics on the Central Path. arXiv, 2106.02073. Retrieved from https://arxiv.org/abs/2106.02073v4

Hua, T., Wang, W., Xue, Z., Ren, S., Wang, Y., & Zhao, H. (2021). On Feature Decorrelation in Self-Supervised Learning. arXiv, 2105.00470. Retrieved from https://arxiv.org/abs/2105.00470v2

#bayesian_nn #ensembling #ood_generalization Izmailov, P., Nicholson, P., Lotfi, S., & Wilson, A. G. (2021). Dangers of Bayesian Model Averaging under Covariate Shift. arXiv, 2106.11905. Retrieved from https://arxiv.org/abs/2106.11905v2

    - It was shown (https://arxiv.org/abs/2104.14421v1) that BNN provide shockingly poor OOD performance
    - We show that Bayesian model averaging perform poorly for data corruption, domain shift, spurious correlations
    - We propose priors for improving generalization under covariate shift

#bayesian_nn #ensembling #ood_generalization Izmailov, P., Vikram, S., Hoffman, M. D., & Wilson, A. G. (2021). What Are Bayesian Neural Network Posteriors Really Like? arXiv, 2104.14421. Retrieved from https://arxiv.org/abs/2104.14421v1

    - We study BNN posteriors with full-batch Hamiltonian Monte Carlo (HMC)
    - HMC can take tens of thousands of training epochs to produce a single sample from the posterior. To address this challenge, we parallelize the computation over hundreds of TPU devices
    - We show that BNNs can achieve significant performance gains over standard training and deep ensembles
    - Posterior tempering is not needed, with little evidence for a "cold posterior" effect
    - BMA performance is robust to the choice of prior and its scale
    - While BNN have good performance for OOD detection, they show surprisingly poor generalization under domain shift
    - We compare predictive distributions for HMC and cheaper alternatives such as deep ensembles and SGMCMC

Jiang, Y., Nagarajan, V., Baek, C., & Kolter, J. Z. (2021). Assessing Generalization of SGD via Disagreement. arXiv, 2106.13799. Retrieved from https://arxiv.org/abs/2106.13799v2

    - We empirically show that the test error of DNNs can be estimated by training the same architecture on the same training set but with two different runs of SGD ( different random orders and/or initializations), and then measuring the disagreement rate between the two networks on unlabeled test data. Under certain training conditions, these properties even hold on many kinds of out-of-distribution data, albeit not on all kinds.
    - Our result is a stronger version of "Distributional Generalization: A New Kind of Generalization", which requires the runs to be on separate training sets.
    - This provides a simple empirical measure to directly predict the test error using unlabeled test data. Such unsupervised accuracy estimation is valuable for real-time evaluation of models when test labels are costly or unavailable to due to privacy considerations.
    - Note that there are also many other known measures that correlate with generalization without access to even unlabeled test data. Disagreement, however, provides a direct estimate, and works even with a black-box model, which makes it practically viable when the inner details of the model are unavailable due to privacy concerns.
    - SGD-trained deep network ensembles are known to be naturally calibrated in practice. Informally, a well-calibrated model is one whose output probability for a particular class (i.e., the model’s “confidence”) is indicative of the probability that the ground truth class is indeed that class. There are many ways in which calibration can be formalized. We provide a particular formalism called class-wise calibration. The ensemble model h satisfies class-wise calibration on D if for any confidence value q ∈ [0, 1] and for any class k, p(y = k|h_k(X) = q) = q (how does q is calculated?). Next, we show that if the ensemble is class-wise calibrated on the distribution D, then Generalization Disagreement Equality (GDE) does hold on D.
    - We also show that GDE holds under a more relaxed notion of calibration, called class-aggregated calibration. We fornally define the class-aggregated calibration for ensemble (eq. 5, where h~ is the proportion of models in the ensemble that predict a given class by taking argmax, see eq. 2). Next, we define the Class Aggregated Calibration Error (CACE) of an ensemble (eq. 6), that is different from the known “expected calibration error”, which is concerned only with the confidence level of the top predicted class for each point. While top-class calibration is weaker than class-wise calibration, it is neither stronger nor weaker than class-aggregated calibration. In Appendix B.5, we discuss the relation between this new notion of calibration to existing definitions.
    - IMO, a question: how is class-wise calibration related to the correctness of p(y|x) obtained from ensmble? if we have perfectly calibrated model (or ensemble), can we obtain perfect predictions from it? If yes, then the ensembles may not be perfectly calibrated, because it is known that they do not achieve perfect accuracy even with infinite models.

#fine_tuning Kanavati, F., & Tsuneki, M. (2021). Partial transfusion: on the expressive influence of trainable batch norm parameters for transfer learning. arXiv, 2102.05543. Retrieved from https://arxiv.org/abs/2102.05543v1

    - It is typically recommended to fine-tune the model with the BatchNorm layers kept in inference mode
    - We find that fine-tuning only the scale and shift weights of the BatchNorm leads to similar performance as to fine-tuning all of the weights, with the added benefit of faster convergence

Lafon, M., & Thomas, A. (2021). Understanding the Double Descent Phenomenon in Deep Learning. arXiv, 2403.10459. Retrieved from https://arxiv.org/abs/2403.10459v1

Lanillos, P., Meo, C., Pezzato, C., Meera, A. A., Baioumy, M., Ohata, W., ...Tani, J. (2021). Active Inference in Robotics and Artificial Agents: Survey and Challenges. arXiv, 2112.01871. Retrieved from https://arxiv.org/abs/2112.01871v1

Larsen, B. W., Fort, S., Becker, N., & Ganguli, S. (2021). How many degrees of freedom do we need to train deep networks: a loss landscape perspective. arXiv, 2107.05802. Retrieved from https://arxiv.org/abs/2107.05802v2

Liu, F., Suykens, J. A. K., & Cevher, V. (2021). On the Double Descent of Random Features Models Trained with SGD. arXiv, 2110.06910. Retrieved from https://arxiv.org/abs/2110.06910v6

Liu, M., Chen, L., Du, X., Jin, L., & Shang, M. (2021). Activated Gradients for Deep Neural Networks. arXiv, 2107.04228. Retrieved from https://arxiv.org/abs/2107.04228v1

Lobacheva, E., Kodryan, M., Chirkova, N., Malinin, A., & Vetrov, D. (2021). On the Periodic Behavior of Neural Network Training with Batch Normalization and Weight Decay. arXiv, 2106.15739. Retrieved from https://arxiv.org/abs/2106.15739v3

    - We show that combined batch normalization and weight decay may lead to periodic behavior of optimization dynamics
    - Periodic behavior can be regarded as a generalization of two previously opposing perspectives on training with batch normalization and weight decay, namely the equilibrium presumption and the instability presumption

Meunier, L., Delattre, B., Araujo, A., & Allauzen, A. (2021). A Dynamical System Perspective for Lipschitz Neural Networks. arXiv, 2110.12690. Retrieved from https://arxiv.org/abs/2110.12690v2

Miller, J., Taori, R., Raghunathan, A., Sagawa, S., Koh, P. W., Shankar, V., ...Schmidt, L. (2021). Accuracy on the Line: On the Strong Correlation Between Out-of-Distribution and In-Distribution Generalization. arXiv, 2107.04649. Retrieved from https://arxiv.org/abs/2107.04649v2

    - We investigate linear trends between ID and OOD accuracy (previously shown in "Do ImageNet Classifiers Generalize to ImageNet?") oт many models on multiple distribution shifts (fig. 1). We observe that linear trends apply to a wide range of model families and holds under variation in architecture, hyperparameters, and training duration, and on many datasets: multiple popular image classification benchmarks, a pose estimation testbed and two distribution shifts derived from concrete applications of image classification: satellite imagery and wildlife photos.
    - The linear trends are more precise after applying a probit or logit scaling on both axes of the scatter plotes.
    - Classical methods follow the same linear trend as more recent deep learning architectures.
    - ImageNet pre-trained models on CIFAR-10 and FMoW-WILDS follow the same linear trends as models trained only on CIFAR-10 / FMoW-WILDS. In other cases (e.g., iWildCam-WILDS), pre-training yields clearly different relationships between ID and OOD accuracies.
    - Linear trends do not occur or are less regular on some of the synthetic distribution shifts in CIFAR-10-C (e.g., Gaussian noise), the Camelyon17-WILDS shift of tissue slides from different hospitals, and a version of the iWildCam-WILDS wildlife with a different in-distribution train-test split.
    - As a theoretical models for linear fits, we identify a simple Gaussian setting that showed linear fits across a large range of models (linear models, nearest neighbors and random features, fig. 6).
    - When the direction which determines the distribution shift is chosen by an adversary with knowledge of the tested models, the ID-OOD relationship can be highly non-linear. This is reminiscent of adversarial robustness.
    - When a practitioner encounters a linear trend between ID and OOD performance, it is reasonable to expect that similar distribution shifts will also exhibit a linear trend. In this case, selecting the best model under these distribution shifts reduces to selecting the best model on the ID test set, and existing work on improving ID performance already improves the robustness of a model (understood as OOD performance) without explicitly targeting robustness. To better compare new training techniques to prior work in terms of robustness, we recommend that papers illustrate the effect of their technique with a scatter plot of relevant models and report both ID and OOD performance.
    - It is currently unclear whether improving ID performance is the only way, or even the best way, to improve OOD performance.
    - We provice a detailed discussion of related work in appendix.

Millidge, B., Seth, A., & Buckley, C. L. (2021). Predictive Coding: a Theoretical and Experimental Review. arXiv, 2107.12979. Retrieved from https://arxiv.org/abs/2107.12979v4

Nado, Z., Gilmer, J. M., Shallue, C. J., Anil, R., & Dahl, G. E. (2021). A Large Batch Optimizer Reality Check: Traditional, Generic Optimizers Suffice Across Batch Sizes. arXiv, 2102.06356. Retrieved from https://arxiv.org/abs/2102.06356v3

Ørebæk, O.-E., & Geitle, M. (2021). Exploring the Hyperparameters of XGBoost Through 3D Visualizations. AAAI Spring Symposium Combining Machine Learning with Knowledge Engineering. Retrieved from https://www.semanticscholar.org/paper/Exploring-the-Hyperparameters-of-XGBoost-Through-3D-%C3%98reb%C3%A6k-Geitle/4b5895a52efa17c60b6bbd693f80594c0e10440c

Ortiz, J., Evans, T., & Davison, A. J. (2021). A visual introduction to Gaussian Belief Propagation. arXiv, 2107.02308. Retrieved from https://arxiv.org/abs/2107.02308v1

Roberts, D. A., Yaida, S., & Hanin, B. (2021). The Principles of Deep Learning Theory. arXiv, 2106.10165. Retrieved from https://arxiv.org/abs/2106.10165v2

    - A research monograph in the style of a textbook about the DL theory
    - Is for everyone with knowledge of linear algebra, multivariable calculus, and informal probability theory
    - Much of the material is novel and appears for the first time in this book
    - The infinite-width limit of a NN will be introduced only as a starting point
    - A large part of the book is focused on deep multilayer perceptrons
    - But we expect that many of our results have a broad applicability
    - The first chapters consist of whirlwind introductions to Gaussian integration and perturbation theory
    - As a warm-up, we also discuss deep linear networks
    - Goal in §4 and §5 will be to understand how an ensemble of NNs at initialization behaves as a function of data
    - In §6 and §7 we introduce Bayesian and gradient-based learning
    - As we detail in §10, §11, and §∞, understanding the NTK for a given NN architecture will enable us to effectively describe gradient-based learning for that model
    - Further according to citations:
    - The authors study properties of large-but-finite neural nets
    - Finite width corrections to the NTK theory have been studied by the authors
    - The work develops a theory of an evolving NTK. They apply it in detail to fully-connected networks of depth L, demonstrate the relevance of L/N as an expansion parameter, and develop an effective model for the dynamics
    - The authors develop the perturbative formalism that captures the flow of pre-activation distributions to deeper layers and studies the finite-width effect on Bayesian inference

Rudman, W., Gillman, N., Rayne, T., & Eickhoff, C. (2021). IsoScore: Measuring the Uniformity of Embedding Space Utilization. arXiv, 2108.07344. Retrieved from https://arxiv.org/abs/2108.07344v2

Sander, M. E., Ablin, P., Blondel, M., & Peyré, G. (2021). Momentum Residual Neural Networks. arXiv, 2102.07870. Retrieved from https://arxiv.org/abs/2102.07870v3

Sander, M. E., Ablin, P., Blondel, M., & Peyré, G. (2021). Sinkformers: Transformers with Doubly Stochastic Attention. arXiv, 2110.11773. Retrieved from https://arxiv.org/abs/2110.11773v2

    - We propose a Sinkformer that enhances model accuracy in vision (3D shape classification) and NLP tasks
    - In a classical SoftMax, the exp attention matrix gets row-wise normalized
    - Empirically, during the learning process columns of this matrix also get nearly normalized
    - We propose to normalize this matrix row-wise and column-wise
    - This leads to a doubly stochastic matrix (i.e., whose rows and columns both sum to 1)
    - This makes it possible to understand the iterations of self-attention modules as a discretized gradient-flow for the Wasserstein metric
    - In the infinite number of samples limit that, when rescaling both attention matrices and depth, Sinkformers operate a heat diffusion

#attention Schlag, I., Irie, K., & Schmidhuber, J. (2021). Linear Transformers Are Secretly Fast Weight Programmers. arXiv, 2102.11174. Retrieved from https://arxiv.org/abs/2102.11174v3

    - We show the equivalence of (recently proposed) linearised selfattention mechanisms and the Fast Weight Controllers or Fast Weight Programmers (FWPs) from the ’90s
    - This view allows us to derive a limitation of the memory capacity of linear Transformers and similar models
    - When the sequence length exceeds storage capacity, the model may end up in an overcapacity regime
    - We introduce an improved programming instruction akin to the famous error-correcting delta-rule
    - We demonstrate the benefits of the proposed methods on real datasets for NMT and language modeling

Schneider, F., Dangel, F., & Hennig, P. (2021). Cockpit: A Practical Debugging Tool for the Training of Deep Neural Networks. arXiv, 2102.06604. Retrieved from https://arxiv.org/abs/2102.06604v2

Schölkopf, B., Locatello, F., Bauer, S., Ke, N. R., Kalchbrenner, N., Goyal, A., & Bengio, Y. (2021). Towards Causal Representation Learning. arXiv, 2102.11107. Retrieved from https://arxiv.org/abs/2102.11107v1

Shang, Y., Duan, B., Zong, Z., Nie, L., & Yan, Y. (2021). Lipschitz Continuity Guided Knowledge Distillation. arXiv, 2108.12905. Retrieved from https://arxiv.org/abs/2108.12905v1

#training Shleifer, S., Weston, J., & Ott, M. (2021). NormFormer: Improved Transformer Pretraining with Extra Normalization. arXiv, 2110.09456. Retrieved from https://arxiv.org/abs/2110.09456v2

    - Problem: in Pre-LayerNorm transformer gradients at early layers are much larger than at later layers
    - We propose NormFormer, which adds three normalization operations to each layer
    - This improves pretraining perplexity and downstream task performance for both generation and MLM

Silver, D., Singh, S., Precup, D., & Sutton, R. (2021). Reward is enough. Artif. Intell. Retrieved from https://web.eecs.umich.edu/~baveja/Papers/RewardIsEnough.pdf

Smith, S. L., Dherin, B., Barrett, D. G. T., & De, S. (2021). On the Origin of Implicit Regularization in Stochastic Gradient Descent. arXiv, 2101.12176. Retrieved from https://arxiv.org/abs/2101.12176v1

Tolstikhin, I., Houlsby, N., Kolesnikov, A., Beyer, L., Zhai, X., Unterthiner, T., ...Dosovitskiy, A. (2021). MLP-Mixer: An all-MLP Architecture for Vision. arXiv, 2105.01601. Retrieved from https://arxiv.org/abs/2105.01601v4

    - We present MLP-Mixer for vision, an architecture based exclusively on multi-layer perceptrons (MLPs)
    - While convolutions and attention are both sufficient for good performance, neither of them are necessary
    - Channel-mixing MLPs are applied independently to image patches (i.e. “mixing” the per-location features)
    - Token-mixing MLPs are applied across patches (i.e. “mixing” spatial information)
    - Other components include: skip-connections, dropout, and layer norm on the channels
    - Despite its simplicity, Mixer attains competitive results, when pre-trained on large datasets
    - Similar to ViT, it falls slightly short of specialized CNN architectures when pre-training on models scale data

#compression Yin, M., Sui, Y., Liao, S., & Yuan, B. (2021). Towards Efficient Tensor Decomposition-Based DNN Model Compression with Optimization Framework. arXiv, 2107.12422. Retrieved from https://arxiv.org/abs/2107.12422v1

    - Problem: compressing CNNs with Tensor train (TT) and Tensor ring (TR) suffers significant accuracy loss
    - A new approach requires a specific training procedure
    - very high compression performance with high accuracy
    - Also works for RNNs

Zhao, M., Liu, Z., Luan, S., Zhang, S., Precup, D., & Bengio, Y. (2021). A Consciousness-Inspired Planning Agent for Model-Based Reinforcement Learning. arXiv, 2106.02097. Retrieved from https://arxiv.org/abs/2106.02097v3

Zhu, Z., Ding, T., Zhou, J., Li, X., You, C., Sulam, J., & Qu, Q. (2021). A Geometric Analysis of Neural Collapse with Unconstrained Features. arXiv, 2105.02375. Retrieved from https://arxiv.org/abs/2105.02375v1

Ziyin, L., Li, B., Simon, J. B., & Ueda, M. (2021). SGD with a Constant Large Learning Rate Can Converge to Local Maxima. arXiv, 2107.11774. Retrieved from https://arxiv.org/abs/2107.11774v4

## 2022

Ahn, K., Zhang, J., & Sra, S. (2022). Understanding the unstable convergence of gradient descent. arXiv, 2204.01050. Retrieved from https://arxiv.org/abs/2204.01050v2

Amid, E., Anil, R., Kotłowski, W., & Warmuth, M. K. (2022). Learning from Randomly Initialized Neural Network Features. arXiv, 2202.06438. Retrieved from https://arxiv.org/abs/2202.06438v1

Arjevani, Y., & Field, M. (2022). Annihilation of Spurious Minima in Two-Layer ReLU Networks. arXiv, 2210.06088. Retrieved from https://arxiv.org/abs/2210.06088v1

Bai, Q., Rosenberg, S., & Xu, W. (2022). A Geometric Understanding of Natural Gradient. arXiv, 2202.06232. Retrieved from https://arxiv.org/abs/2202.06232v3

Barak, B., Edelman, B. L., Goel, S., Kakade, S., Malach, E., & Zhang, C. (2022). Hidden Progress in Deep Learning: SGD Learns Parities Near the Computational Limit. arXiv, 2207.08799. Retrieved from https://arxiv.org/abs/2207.08799v3

Christof, C., & Kowalczyk, J. (2022). On the Omnipresence of Spurious Local Minima in Certain Neural Network Training Problems. arXiv, 2202.12262. Retrieved from https://arxiv.org/abs/2202.12262v2

Cohen, J. M., Ghorbani, B., Krishnan, S., Agarwal, N., Medapati, S., Badura, M., ...Gilmer, J. (2022). Adaptive Gradient Methods at the Edge of Stability. arXiv, 2207.14484. Retrieved from https://arxiv.org/abs/2207.14484v1

    - We empirically demonstrate that in daptive full-batch gradient methods, the sharpness (maximum eigenvalue of the preconditioned Hessian) typically equilibrates at a certain numerical value — the stability threshold of a GD
    - Thus we extent previous works where the same was confirmed for non-adaptive methods
    - However, behavior of adaptive gradient methods in this regime differs in a significant way from that of non-adaptive methods in the classical EoS regime: we find that adaptive gradient methods at the “Adaptive Edge of Stability” can and do enter high-curvature regions
    - Thus, adaptive gradient methods sometimes lack the implicit inductive bias that blocks non-adaptive methods from converging to high-curvature solutions

Courtois, A., Morel, J.-M., & Arias, P. (2022). Can neural networks extrapolate? Discussion of a theorem by Pedro Domingos. arXiv, 2211.03566. Retrieved from https://arxiv.org/abs/2211.03566v1

    - We discuss a theorem proved in https://arxiv.org/abs/2012.00152
    - We extend the proof to the discrete setting and the multi-dimensional case
    - It is unclear how the interpretability of the NTK would apply to high dimensional settings such as images
    - Our experiments seem to confirm Domingos’ interpretation of his theorem

D'Amour, A., Heller, K., Moldovan, D., Adlam, B., Alipanahi, B., Beutel, A., ...Sculley, D. (2022). Underspecification Presents Challenges for Credibility in Modern Machine Learning. Journal of Machine Learning Research, 23(226), 1–61. Retrieved from https://www.jmlr.org/papers/v23/20-1335.html

Engel, A., Wang, Z., Sarwate, A. D., Choudhury, S., & Chiang, T. (2022). TorchNTK: A Library for Calculation of Neural Tangent Kernels of PyTorch Models. arXiv, 2205.12372. Retrieved from https://arxiv.org/abs/2205.12372v1

#attention #see_related_work Ergen, T., Neyshabur, B., & Mehta, H. (2022). Convexifying Transformers: Improving optimization and understanding of transformer networks. arXiv, 2211.11052. Retrieved from https://arxiv.org/abs/2211.11052v1

    - We propose an alternative formulation to the standard self-attention mechanism and study the regularized training problem of attention/transformer networks with it
    - We propose Convex attention layer and therefore enable finding a globally optimal solution without requiring any nonconvex optimization heuristic, e.g., layer normalization and skip connections
    - We demonstrate the effectiveness of our convex reformulation via various experimental results

Fedus, W., Dean, J., & Zoph, B. (2022). A Review of Sparse Expert Models in Deep Learning. arXiv, 2209.01667. Retrieved from https://arxiv.org/abs/2209.01667v1

Fort, S., Cubuk, E. D., Ganguli, S., & Schoenholz, S. S. (2022). What does a deep neural network confidently perceive? The effective dimension of high certainty class manifolds and their low confidence boundaries. arXiv, 2210.05546. Retrieved from https://arxiv.org/abs/2210.05546v1

Friston, K. J., Ramstead, M. J. D., Kiefer, A. B., Tschantz, A., Buckley, C. L., Albarracin, M., ...René, G. (2022). Designing Ecosystems of Intelligence from First Principles. arXiv, 2212.01354. Retrieved from https://arxiv.org/abs/2212.01354v2

Friston, K., Da Costa, L., Sajid, N., Heins, C., Ueltzhoffer, K., Pavliotis, G., & Parr, T. (2022). The free energy principle made simpler but not too simple. Phys. Rep. Retrieved from https://www.semanticscholar.org/paper/The-free-energy-principle-made-simpler-but-not-too-Friston-Costa/e54427100b2de8187fe3b96303653b6220aaad44

Isomura, T., Shimazaki, H., & Friston, K. J. (2022). Canonical neural networks perform active inference. Commun. Biol., 5(55), 1–15. Retrieved from https://www.nature.com/articles/s42003-021-02994-2

Ivanova, A. A., Schrimpf, M., Anzellotti, S., Zaslavsky, N., Fedorenko, E., & Isik, L. (2022). Beyond linear regression: mapping models in cognitive neuroscience should align with research goals. arXiv, 2208.10668. Retrieved from https://arxiv.org/abs/2208.10668v1

Hafner, D., Lee, K.-H., Fischer, I., & Abbeel, P. (2022). Deep Hierarchical Planning from Pixels. arXiv, 2206.04114. Retrieved from https://arxiv.org/abs/2206.04114v1

Hinton, G. (2022). The Forward-Forward Algorithm: Some Preliminary Investigations. arXiv, 2212.13345. Retrieved from https://arxiv.org/abs/2212.13345v1

    - As a model of how cortex learns, backpropagation (especially through time) remains implausible
    - A further serious limitation of backpropagation is that it requires perfect knowledge of the computation performed in the forward pass in order to compute the correct derivatives (impossible with black-box forward pass)
    - In the absence of a perfect model of the forward pass, it is always possible to resort to one of the many forms of reinforcement learning, but it suffer from high variance: it is hard to see the effect of perturbing one variable when many other variables are being perturbed at the same time
    - The Forward-Forward algorithm replaces the forward and backward passes of backpropagation by two forward passes, one with positive (i.e. real) data and the other with negative data which could be generated by the network itself
    - Each layer has an objective to have high goodness for positive data and low goodness for negative data
    - The forward-forward algorithm is somewhat slower than backpropagation and does not generalize quite as well on several of the toy problems investigated in this paper
    - The two areas in which the forward-forward algorithm may be superior to backpropagation are as a model of learning in cortex and as a way of making use of very low-power analog hardware without resorting to reinforcement learning

Juneja, J., Bansal, R., Cho, K., Sedoc, J., & Saphra, N. (2022). Linear Connectivity Reveals Generalization Strategies. arXiv, 2205.12411. Retrieved from https://arxiv.org/abs/2205.12411v5

Kirsch, A., & Gal, Y. (2022). A Note on "Assessing Generalization of SGD via Disagreement". arXiv, 2202.01851. Retrieved from https://arxiv.org/abs/2202.01851v2

    - We show that the previously proposed Generalization Disagreement Equality (that estimates the average test error via the prediction disagreement of ensemble) might be impractical. A deep ensemble’s calibration can deteriorate as prediction disagreement increases. Calibration must be measured on the data distribution we want to evaluate, but this reintroduces the need for labels on the unlabeled dataset.
    - We also simplify the theoretical statements and proofs of "Assessing generalization of SGD via disagreement", showing them to be straightforward within a probabilistic context.
    - TODO read, a paper with good survey.

Kusupati, A., Bhatt, G., Rege, A., Wallingford, M., Sinha, A., Ramanujan, V., ...Farhadi, A. (2022). Matryoshka Representation Learning. arXiv, 2205.13147. Retrieved from https://arxiv.org/abs/2205.13147v4

    - We propose Matryoshka Representation Learning (MRL) to learn coarse-to-fine representations
    - This allows shorter embeddings and improvements for long-tail few-shot classification
    - This is a flexible representation that can adapt to multiple downstream tasks with varying computational resources

Li, Y. (2022). A Short Survey of Systematic Generalization. arXiv, 2211.11956. Retrieved from https://arxiv.org/abs/2211.11956v1

#transformers_ffn Li, Z., You, C., Bhojanapalli, S., Li, D., Rawat, A. S., Reddi, S. J., ...Kumar, S. (2022). The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in Transformers. arXiv, 2210.06313. Retrieved from https://arxiv.org/abs/2210.06313v2

    - We show that in Transformers, the intermediate output of the MLPs after a ReLU activation function is sparse
    - This means that on average very few entries (e.g., 3.0% for T5-Base and 6.3% for ViT-B16) are nonzero
    - Moreover, larger Transformers with more layers and wider MLP hidden dimensions are sparser
    - Emergence of sparsity occurs for both NLP and vision tasks, on both training and evaluation data, for various configurations, at all depth levels, as well as for other architectures including MLP-mixers and 2-layer MLPs
    - Sparsity is not a result of a specific family of datasets: it also emerges using training datasets with random labels, or with random inputs, or with infinite amount of data
    - Sparsity of activation map in trained Transformers implies that a large proportion of the computation during inference is spent on multiplying values by zero. Hence, FLOPs can be drastically reduced by avoiding all such computations
    - We introduce Top-k Transformer: we found that enforcing an even sparser activation with a Top-k thresholding brings less sensitivity to noisy training data, more robustness to input corruptions, and better calibration for their prediction confidence

Liu, Z., Michaud, E. J., & Tegmark, M. (2022). Omnigrok: Grokking Beyond Algorithmic Data. arXiv, 2210.01117. Retrieved from https://arxiv.org/abs/2210.01117v2

#fine_tuning Malladi, S., Wettig, A., Yu, D., Chen, D., & Arora, S. (2022). A Kernel-Based View of Language Model Fine-Tuning. arXiv, 2210.05643. Retrieved from https://arxiv.org/abs/2210.05643v4

    - Why fine-tuning a model with >10^8 parameters on a couple dozen training points does not result in overfitting?
    - We use Tensor Programs to characterize conditions under which the NTK may describe fine-tuning of pre-trained LMs
    - We use NTKs to mathematically formalize the general intuition that fine-tuning pretrained language models to solve downstream tasks requires only a “small change.”
    - Extensive experiments on 14 NLU tasks demonstrate that including a meaningful prompt often causes FT to exhibit kernel behavior and that kernel dynamics describe prompt-based FT on tasks that the eNTK can solve
    - Our experiments are limited to few-shot classification and a single MLM with specific prompts
    - Our experiments show some tasks do not induce kernel behavior during FT

Millidge, B., Salvatori, T., Song, Y., Bogacz, R., & Lukasiewicz, T. (2022). Predictive Coding: Towards a Future of Deep Learning beyond Backpropagation? arXiv, 2202.09467. Retrieved from https://arxiv.org/abs/2202.09467v1

Mohamadi, M. A., Bae, W., & Sutherland, D. J. (2022). A Fast, Well-Founded Approximation to the Empirical Neural Tangent Kernel. arXiv, 2206.12543. Retrieved from https://arxiv.org/abs/2206.12543v3

Novak, R., Sohl-Dickstein, J., & Schoenholz, S. S. (2022). Fast Finite Width Neural Tangent Kernel. arXiv, 2206.08720. Retrieved from https://arxiv.org/abs/2206.08720v1

Power, A., Burda, Y., Edwards, H., Babuschkin, I., & Misra, V. (2022). Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets. arXiv, 2201.02177. Retrieved from https://arxiv.org/abs/2201.02177v1

    - The datasets we consider are binary operation tables of the form a ◦ b = c where a, b, c are discrete symbols
    - Training NN on a proper subset of all possible equations amounts to filling in the blanks of the binary op table
    - We show that NNs are capable of generalizing to the empty slots in a variety of binary op tables
    - Long after severely overfitting, validation accuracy sometimes suddenly begins to increase from chance level toward perfect generalization. We call this phenomenon ‘grokking’

Rangwani, H., Aithal, S. K., Mishra, M., & Babu, R. V. (2022). Escaping Saddle Points for Effective Generalization on Class-Imbalanced Data. arXiv, 2212.13827. Retrieved from https://arxiv.org/abs/2212.13827v1

Ramesh, A., Kirsch, L., van Steenkiste, S., & Schmidhuber, J. (2022). Exploring through Random Curiosity with General Value Functions. arXiv, 2211.10282. Retrieved from https://arxiv.org/abs/2211.10282v1

Rissanen, S., Heinonen, M., & Solin, A. (2022). Generative Modelling With Inverse Heat Dissipation. arXiv, 2206.13397. Retrieved from https://arxiv.org/abs/2206.13397v7

    - Inspired by coarse-to-fine modelling, we propose an inverse heat dissipation model (IHDM) (fig. 2) that generates images through stochastically reversing the heat equation, a PDE that locally erases fine-scale information when run over the 2D plane of the image
    - The intuition is that as the original image information content is erased in the forward process, a corresponding stochastic reverse process produces multiple plausible reconstructions, defining a generative model
    - Our model shows emergent properties not seen in standard diffusion models, such as disentanglement of overall colour and shape in images
    - Spectral analysis reveals an implicit coarse-to-fine inductive bias

Radhakrishnan, A., Beaglehole, D., Pandit, P., & Belkin, M. (2022). Mechanism of feature learning in deep fully connected networks and kernel machines that recursively learn features. arXiv, 2212.13881. Retrieved from https://arxiv.org/abs/2212.13881v3

    - How do deep MLPs learn features?
    - We show that neural feature learning occurs by implementing the average gradient outer product to up-weight features strongly related to model output
    - This sheds light on the emergence of spurious features, simplicity biases, lottery ticket hypothesis
    - The mechanism identified in our work leads to a backpropagation-free method for feature learning; we use it to enable feature learning in kernel machines
    - The resulting Recursive Feature Machines achieve SOTA performance on tabular data

Ramstead, M. J. D., Sakthivadivel, D. A. R., Heins, C., Koudahl, M., Millidge, B., Da Costa, L., ...Friston, K. J. (2022). On Bayesian Mechanics: A Physics of and by Beliefs. arXiv, 2205.11543. Retrieved from https://arxiv.org/abs/2205.11543v4

Sander, M. E., Ablin, P., & Peyré, G. (2022). Do Residual Neural Networks discretize Neural Ordinary Differential Equations? arXiv, 2205.14612. Retrieved from https://arxiv.org/abs/2205.14612v2

    - Are discrete dynamics defined by a ResNet close to the continuous one of a Neural ODE?
    - Several theoretical results
    - A simple technique to train ResNets without storing activations
    - Recover the approximated activations during the backward pass by using a reverse-time Euler scheme
    - Fine-tuning very deep ResNets without memory consumption in the residual layers

Sutton, R. S., Bowling, M., & Pilarski, P. M. (2022). The Alberta Plan for AI Research. arXiv, 2208.11173. Retrieved from https://arxiv.org/abs/2208.11173v3

    - The Alberta Plan is a long-term plan for the next 5–10 years oriented toward basic understanding of computational intelligence
    - Overall, the Alberta Plan characterizes the problem of AI as the online maximization of reward via continual sensing and acting, with limited computation, and potentially in the presence of other agents
    - We emphasize on ordinary experience, as opposed to special training sets, human assistance, or access to the internal structure of the world
    - We pursue temporal uniformity: all times are the same with respect to the algorithms running on the agent. There are no special training periods when training information is available or when rewards are counted more or less than others
    - We should prioritize methods that scale extensively with computer power
    - We focus on the special case in which the environment includes other intelligent agents

Sutton, R. S., Machado, M. C., Holland, G. Z., Szepesvari, D., Timbers, F., Tanner, B., & White, A. (2022). Reward-Respecting Subtasks for Model-Based Reinforcement Learning. arXiv, 2202.03466. Retrieved from https://arxiv.org/abs/2202.03466v4

Vanchurin, V., Wolf, Y. I., Katsnelson, M. I., & Koonin, E. V. (2022). Toward a theory of evolution as multilevel learning. Proc. Natl. Acad. Sci. U.S.A., 119(6), e2120037119. Retrieved from https://doi.org/10.1073/pnas.2120037119

Wang, H., Ma, S., Huang, S., Dong, L., Wang, W., Peng, Z., ...Wei, F. (2022). Foundation Transformers. arXiv, 2210.06423. Retrieved from https://arxiv.org/abs/2210.06423v2

    - We call for developing Foundation Transformers for true general-purpose modeling
    - We introduce a Transformer variant, named MAGNETO, with Sub-LayerNorm for good expressivity, and the initialization strategy theoretically derived from DeepNet for stable scaling up
    - We demonstrate its superior performance and better stability than the de facto Transformer variants

#training Wang, H., Ma, S., Dong, L., Huang, S., Zhang, D., & Wei, F. (2022). DeepNet: Scaling Transformers to 1,000 Layers. arXiv, 2203.00555. Retrieved from https://arxiv.org/abs/2203.00555v1

    - We introduce a new normalization function (DeepNorm) to modify the residual connection in Transformer
    - It is accompanied with theoretically derived initialization
    - Our method combines the best of two worlds, i.e., good performance of Post-LN and stable training of Pre-LN
    - We successfully scale Transformers up to 1000 layers
    - Our 200-layer model with 3.2B parameters significantly outperforms the 48-layer SOTA model with 12B parameters, which indicates a promising scaling direction

#fine_tuning Wei, A., Hu, W., & Steinhardt, J. (2022). More Than a Toy: Random Matrix Models Predict How Real-World Neural Representations Generalize. arXiv, 2203.06176. Retrieved from https://arxiv.org/abs/2203.06176v1

    - We apply this random matrix theory to explore neural representations: Why do pretrained models generalize better than randomly initialized ones? And what factors govern the rates observed in neural scaling laws?
    - Pretrained representations perform better than random representations due to better alignment - how easy it is to represent the ground truth function in the eigenbasis
    - We provide methods to estimate the alignment and eigenvalue decay
    - In addition to our scientific contribution, we develop a library for computing large-scale empirical NTKs

Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., ...Zhou, D. (2022). Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. arXiv, 2201.11903. Retrieved from https://arxiv.org/abs/2201.11903v6

    - We propose chain-of-thought prompting that enables any off-the-shelf LLM to tackle complex arithmetic, commonsense, and symbolic reasoning tasks. A chain of thought is a series of intermediate natural language reasoning steps that lead to the final output. We provide few in-context examples of triples: (input, chain of thought, output) (fig. 1, 3).
    - Chain-of-thought outperforms standard few-shot prompting, sometimes to a striking degree.
    - Chain-of-thought provides an interpretable window into the behavior of the model (although fully characterizing a model’s computations that support an answer remains an open question).
    - For symbolic reasoning, chain-of-thought prompting facilitates OOD generalization to examples with more steps than those in the exemplars.

Weng, Lilian. (Sep 2022). Some math behind neural tangent kernel. Lil’Log. https://lilianweng.github.io/posts/2022-09-08-ntk/.

    - A blogpost review on a small number of core papers in NTK
    - The goal is to show all the math behind NTK in a clear and easy-to-follow format

#ensembling #ood_generalization Wortsman, M., Ilharco, G., Gadre, S. Y., Roelofs, R., Gontijo-Lopes, R., Morcos, A. S., ...Schmidt, L. (2022). Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. arXiv, 2203.05482. Retrieved from https://arxiv.org/abs/2203.05482v3

    - We propose "model soups": averaging the weights of multiple models finetuned with different hyperparameters
    - This often improves accuracy and robustness and does not incur additional inference or memory costs
    - When fine-tuning CLIP, ALIGN or ViT-G, our soup recipe provides significant improvements over the best model
    - Model soup also improves OOD performance and zero-shot performance on new downstream tasks, compared to the best individual fine-tuned model.
    - Greedy soups, where models are sequentially added to the soup if they improve accuracy on held-out data, outperforms uniform averaging. Greedy soups avoid adding in models which may lie in a different basin of the error landscape, which could happen if, for example, models are fine-tuned with high learning rates.
    - We relate the performance similarity of weight-averaging and logit ensembling to flatness of the loss and confidence of the predictions

Yang, G., Hu, E. J., Babuschkin, I., Sidor, S., Liu, X., Farhi, D., ...Gao, J. (2022). Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer. arXiv, 2203.03466. Retrieved from https://arxiv.org/abs/2203.03466v2

#training Zhou, J., You, C., Li, X., Liu, K., Liu, S., Qu, Q., & Zhu, Z. (2022). Are All Losses Created Equal: A Neural Collapse Perspective. arXiv, 2210.02192. Retrieved from https://arxiv.org/abs/2210.02192v2

    - We extend a previously studied Neural Collapse (NC) phenomenon under CE and MSE losses
    - We theoretically show that the NC solution is the only global optimal solution to a broad family of loss functions
    - Many loss functions including label smoothing (LS) and focal loss (FL) exhibits neural collapse
    - All losses (i.e., CE, LS, FL, MSE) lead to largely identical features on training data and largely identical performance on test data by large DNNs and sufficiently many iterations
    - We also provide a global landscape analysis
    - Our conclusion implies that the better performance with particular choices of loss functions comes as a result that the training does not produce a globally optimal (i.e., NC) solution
    - We note that our conclusion is based on natural accuracy, rather than model transferability or robustness

Zhou, J., Li, X., Ding, T., You, C., Qu, Q., & Zhu, Z. (2022). On the Optimization Landscape of Neural Collapse under MSE Loss: Global Optimality with Unconstrained Features. arXiv, 2203.01238. Retrieved from https://arxiv.org/abs/2203.01238v2

#robustness Zhu, Z., Liu, F., Chrysos, G. G., & Cevher, V. (2022). Robustness in deep learning: The good (width), the bad (depth), and the ugly (initialization). arXiv, 2209.07263. Retrieved from https://arxiv.org/abs/2209.07263v4

    - Our results suggest that the width (good) helps average robustness in the over-parameterized regime but the depth (bad) can help only under certain initializations (ugly)

## 2023

Abbas, Z., Zhao, R., Modayil, J., White, A., & Machado, M. C. (2023). Loss of Plasticity in Continual Deep Reinforcement Learning. arXiv, 2303.07507. Retrieved from https://arxiv.org/abs/2303.07507v1

#ensembling #loss_landscape Altintas, G. S., Bachmann, G., Noci, L., & Hofmann, T. (2023). Disentangling Linear Mode-Connectivity. arXiv, 2312.09832. Retrieved from https://arxiv.org/abs/2312.09832v1

    - We study Linear mode-connectivity (LMC) that lacks theoretical understanding
    - MLP with one hidden layer is the minimal non-linear setting when LMC can be robustly observed over different optimization schemes and datasets
    - While locality preserves LMC, weight-sharing breaks it
    - Adam breaks connectivity more than SGD, while it can be recovered by modifying LR and adding warm-up
    - LMC can be more easily broken under more complex datasets

Andriushchenko, M., Croce, F., Müller, M., Hein, M., & Flammarion, N. (2023). A Modern Look at the Relationship between Sharpness and Generalization. arXiv, 2302.07011. Retrieved from https://arxiv.org/abs/2302.07011v2

Araujo, A., Havens, A., Delattre, B., Allauzen, A., & Hu, B. (2023). A Unified Algebraic Perspective on Lipschitz Neural Networks. arXiv, 2303.03169. Retrieved from https://arxiv.org/abs/2303.03169v2

Arbel, J., Pitas, K., Vladimirova, M., & Fortuin, V. (2023). A Primer on Bayesian Neural Networks: Review and Debates. arXiv, 2309.16314. Retrieved from https://arxiv.org/abs/2309.16314v1

    - We present a systematic introduction to the fundamental concepts of neural networks and Bayesian inference
    - We delve into the practical considerations associated with training and inference in BNNs
    - The adoption of Bayesian DL in the industry remains limited because of the considerable computational burden

Barabási, D. L., Bianconi, G., Bullmore, E., Burgess, M., Chung, S., Eliassi-Rad, T., ...Buzsáki, G. (2023). Neuroscience needs Network Science. arXiv, 2305.06160. Retrieved from https://arxiv.org/abs/2305.06160v2

Bombari, S., Kiyani, S., & Mondelli, M. (2023). Beyond the Universal Law of Robustness: Sharper Laws for Random Features and Neural Tangent Kernels. arXiv, 2302.01629. Retrieved from https://arxiv.org/abs/2302.01629v2

Irie, K., Csordás, R., & Schmidhuber, J. (2023). Automating Continual Learning. arXiv, 2312.00276. Retrieved from https://arxiv.org/abs/2312.00276v1

    - We propose Automated Continual Learning (ACL) to tackle the problem of catastrophic forgetting
    - ACL encodes good performance on both old and new tasks into its meta-learning objectives
    - ACL requires training settings similar to those of few-shot/meta learning problems
    - We demonstrate that ACL effectively solves "in-context catastrophic forgetting"

Isomura, T. (2023). Bayesian mechanics of self-organising systems. arXiv, 2311.10216. Retrieved from https://arxiv.org/abs/2311.10216v1

#attention #graph #see_related_work Cai, C., Hy, T. S., Yu, R., & Wang, Y. (2023). On the Connection Between MPNN and Graph Transformer. arXiv, 2301.11956. Retrieved from https://arxiv.org/abs/2301.11956v4

    - We study MPNN (Message Passing Neural Network) and more recently proposed Graph Transformer (GT) for processing graph-structured data
    - MPNN enjoys linear complexity, but GT is more suitable for long-range modeling tasks where the label of one node depends on features of nodes far away
    - In MPNN, one can resort to using of an additional virtual node (VN) that connects to all input graph nodes
    - We study the representation power of MPNN + VN compared to GT
    - Compared to early results showing GT can approximate MPNN, we draw the connection from the inverse direction
    - We demonstrate that MPNN + VN remains a surprisingly strong baseline
    - We still lack good benchmark datasets where GT can outperform MPNN by a large margin

#training Chen, L., Lukasik, M., Jitkrittum, W., You, C., & Kumar, S. (2023). It's an Alignment, Not a Trade-off: Revisiting Bias and Variance in Deep Models. arXiv, 2310.09250. Retrieved from https://arxiv.org/abs/2310.09250v1

    - We directly measure per-sample bias and variance for DL classification models
    - For a variety of architectures and datasets we observe the bias-variance alignment (not the trade-off):
    - 1) or correctly classified sample points, the bias and variance align closely along the line of Bias^2 = Variance
    - 2) For incorrectly classified samples, we observe Bias^2 > Variance
    - So, Bias^2 >= Variance at every sample
    - This provides an explanation for why deep models have limited variance, and in effect, good generalization
    - Theoretically, we prove the bias-variance alignment under the assumption that the model is well-calibrated
    - We show that the neural collapse theory predicts the approximate bias-variance alignment

Cirone, N. M., Lemercier, M., & Salvi, C. (2023). Neural signature kernels as infinite-width-depth-limits of controlled ResNets. arXiv, 2303.17671. Retrieved from https://arxiv.org/abs/2303.17671v2

    - We consider randomly initialized controlled ResNets defined as Euler-discretizations of neural controlled differential equations, a unified architecture which enconpasses both RNNs and ResNets
    - Study convergence in the infinite-depth regime

Dang, H., Tran, T., Osher, S., Tran-The, H., Ho, N., & Nguyen, T. (2023). Neural Collapse in Deep Linear Networks: From Balanced to Imbalanced Data. arXiv, 2301.00437. Retrieved from https://arxiv.org/abs/2301.00437v5

#ssl #evaluation Dubois, Y., Hashimoto, T., & Liang, P. (2023). Evaluating Self-Supervised Learning via Risk Decomposition. arXiv, 2302.03068. Retrieved from https://arxiv.org/abs/2302.03068v3

    - Yet SSL models are typically evaluated using a single metric: linear probing on ImageNet
    - We decompose four sources of errors (and computationally efficient estimators for them):
    - 1) approximation errors due to the encoder’s architecture not having the capacity to perform the task
    - 2) representation usability errors (are large when SSL algorithm fails to produce linearly separable representations that can be used to predict desired tasks)
    - 3) probe generalization errors due to finite training data
    - 4) encoder generalization errors due to pretraining the encoder on finite data
    - Using those estimators, we analyze 169 pretrained SSL models and the effect of 30 design choices
    - The most important source of error used to be representation usability but now is the probe generalization
    - Some design choices improve all error components simultaneously, but others tradeoff components

Fu, S., & Wang, D. (2023). Theoretical Analysis of Robust Overfitting for Wide DNNs: An NTK Approach. arXiv, 2310.06112. Retrieved from https://arxiv.org/abs/2310.06112v2

Gardner, J., Popovic, Z., & Schmidt, L. (2023). Benchmarking Distribution Shift in Tabular Data with TableShift. arXiv, 2312.07577. Retrieved from https://arxiv.org/abs/2312.07577v3

Gao, P., Xu, Q., Wen, P., Shao, H., Yang, Z., & Huang, Q. (2023). A Study of Neural Collapse Phenomenon: Grassmannian Frame, Symmetry and Generalization. arXiv, 2304.08914. Retrieved from https://arxiv.org/abs/2304.08914v2

#attention Geshkovski, B., Letrouit, C., Polyanskiy, Y., & Rigollet, P. (2023). A mathematical perspective on Transformers. arXiv, 2312.10794. Retrieved from https://arxiv.org/abs/2312.10794v3

    - We observe that Transformers are flow maps on the space of probability measures over R^d and evolve as a mean-field interacting particle system (particles are tokens)
    - This allows one to draw concrete connections to established topics in mathematics, including nonlinear transport equations, Wasserstein gradient flows, collective behavior models, and optimal configurations of points on spheres, among other
    - Our main observation is that particles tend to cluster under these dynamics
    - This indicates a small number of possible outcomes in learning tasks such as next-token prediction
    - The Transformer flow appears to possess two different time-scales: in a first phase, tokens quickly form a few clusters, while in a second (much slower) phase, through the process of pairwise merging of clusters, all tokens finally collapse to a single point

#fine_tuning Giannou, A., Rajput, S., & Papailiopoulos, D. (2023). The Expressive Power of Tuning Only the Normalization Layers. arXiv, 2302.07937. Retrieved from https://arxiv.org/abs/2302.07937v2

    - We show that for random ReLU MLPs, fine-tuning only its normalization layers can reconstruct any target network that is O(sqrt(width)) times smaller

Johnson, R., & Zhang, T. (2023). Inconsistency, Instability, and Generalization Gap of Deep Neural Network Training. arXiv, 2306.00169. Retrieved from https://arxiv.org/abs/2306.00169v2

    - We propose a bound of generalization gap that depends on "inconsistency" and "instability" of model outputs.
    - 1) We say training procedure P has high inconsistency of model outputs if the predictions made by the models trained with P using the same training data are very different from one another in expectation over the underlying (but unknown) data distribution.
    - 2) We say training procedure P has high instability of model outputs if different sampling of training data changes the expected predictions a lot over the underlying data distribution.
    - Both inconsistency and instability of model outputs can be estimated on unlabeled data.
    - If a specific condition - low randomness in the final state - is met, inconsistency alone is almost as predictive as inconsistency and instability combined. We thus focused on inconsistency. We observed that both loss sharpness reduction and inconsistency reduction lead to better generalization. However, there are a number of cases where inconsistency and generalization gap are reduced and yet loss sharpness remains relatively high. So, we condlude that inconsistency has a stronger (and perhaps more essential) connection with generalization gap than loss sharpness.
    - Our results provide a theoretical basis for existing methods such as co-distillation and ensemble.

Isomura, T., Kotani, K., Jimbo, Y., & Friston, K. J. (2023). Experimental validation of the free-energy principle with in vitro neural networks. Nat. Commun., 14(4547), 1–15. Retrieved from https://www.nature.com/articles/s41467-023-40141-z

Jacobs, B., & Stein, D. (2023). Pearl's and Jeffrey's Update as Modes of Learning in Probabilistic Programming. arXiv, 2309.07053. Retrieved from https://arxiv.org/abs/2309.07053v2

Jordan, K. (2023). On the Variance of Neural Network Training with respect to Test Sets and Distributions. arXiv, 2304.01910. Retrieved from https://arxiv.org/abs/2304.01910v4

    - Problem: typical NN trainings have substantial variance in test-set performance between repeated runs
    - This impedes hyperparameter comparison and training reproducibility
    - We show that although training runs have substantial variance on a test set, they have little variance in performance on the underlying test distribution. Random seeds which are “lucky” with respect to one set of test data perform no better than average with respect to a second set. Disjoint splits of test data are nearly decorrelated, in the sense that networks which randomly perform well on one split do not perform better than average on the other. This phenomenon also extends to individual examples
    - We prove that variance in test-set accuracy between runs of training is an inevitable consequence of the fact that ensembles of trained networks approximately satisfy the class-wise calibration property of "Assessing generalization of SGD via disagreement".

Kreisler, I., Nacson, M. S., Soudry, D., & Carmon, Y. (2023). Gradient Descent Monotonically Decreases the Sharpness of Gradient Flow Solutions in Scalar Networks and Beyond. arXiv, 2305.13064. Retrieved from https://arxiv.org/abs/2305.13064v1

Laurent, O., Aldea, E., & Franchi, G. (2023). A Symmetry-Aware Exploration of Bayesian Neural Network Posteriors. arXiv, 2310.08287. Retrieved from https://arxiv.org/abs/2310.08287v1

#architectures #see_related_work Lee, J. H., & Vijayan, S. (2023). Having Second Thoughts? Let's hear it. arXiv, 2311.15356. Retrieved from https://arxiv.org/abs/2311.15356v2

    - We hypothesize that incorporating top-down signal processing may make DL models more robust
    - Selective attention that involves bottom-up and top-down processing should lead to more reliable decisions (the searchlight hypothesis): tn the first stage, bottom-up sensory processing detects a stimuli, and in the second stage, top-down processing directs low-order sensory areas to find certain types of information related to the stimuli to confirm or reject the presence of the stimuli initially surmised
    - We propose to mimics selective attention with second-thought certification (STCert): we obtain an original prediction from a DL model and use it as a text prompt to identify the region of interest (ROI), which is used to confirm or reject the original prediction
    - STCert can enhance the robustness of DL and warn adversarially manipulated inputs at high accuracy

Li, W., Peng, Y., Zhang, M., Ding, L., Hu, H., & Shen, L. (2023). Deep Model Fusion: A Survey. arXiv, 2309.15698. Retrieved from https://arxiv.org/abs/2309.15698v1

    - Deep model fusion/merging merges the parameters or predictions of multiple DL models into a single one
    - We categorize existing deep model fusion methods into:
    - 1) Mode connectivity, which connects the solutions in weight space via a path of non-increasing loss, in order to obtain better initialization for model fusion
    - 2) Alignment, which matches units between neural networks to create better conditions for fusion
    - 3) Weight average, which is a classical model fusion method, that averages the weights of multiple models
    - 4) Ensemble learning, which combines the outputs of diverse models

#training Liu, Z., Xu, Z., Jin, J., Shen, Z., & Darrell, T. (2023). Dropout Reduces Underfitting. arXiv, 2303.01500. Retrieved from https://arxiv.org/abs/2303.01500v2

    - We see the exploding growth of available training data, making it increasingly difficult to overfit, so the drop rate has generally been decreasing over the years. As a result, we may soon be confronting more problems with underfitting instead of overfitting. Would dropout lose its relevance?
    - We show that dropout can mitigate underfitting when used at the start of training
    - Models equipped with early dropout achieve lower final training loss than their counterparts without dropout
    - This is because dropout reduces the directional variance of gradients across mini-batches
    - Additionally, we explore a symmetric technique for regularizing overfitting models - late dropout

#training Lyu, K., Jin, J., Li, Z., Du, S. S., Lee, J. D., & Hu, W. (2023). Dichotomy of Early and Late Phase Implicit Biases Can Provably Induce Grokking. arXiv, 2311.18817. Retrieved from https://arxiv.org/abs/2311.18817v1

    - Why the "grokking" transition from memorization to generalization is often sharp, instead of gradual?
    - When training homogeneous NNs (a broad class of neural nets that include commonly used MLPs and CNNs with homogeneous activatio) with large initialization and small weight decay, we prove that the training process gets trapped at a solution corresponding to a kernel predictor (related to the Neural Tangent Kernel) for a long time
    - Then a very sharp transition to min-norm/max-margin predictors occurs
    - Thus, we attrubute grokking to the dichotomy of early and late phase implicit biases
    - Based on this, we are able to construct examples where the early phase implicit bias leads to a generalizable solution, and the late phase bias leads to overfitting. This gives rise to a new phenomenon which we call “misgrokking”: a NN first achieves perfect training and test accuracies, but training for a longer time leads to a sudden big drop in test accuracy

Marion, P., Wu, Y.-H., Sander, M. E., & Biau, G. (2023). Implicit regularization of deep residual networks towards neural ODEs. arXiv, 2309.01213. Retrieved from https://arxiv.org/abs/2309.01213v2

    - Proof that if ResNet is initialized as a discretization of a neural ODE, then it holds throughout training

#training Merrill, W., Tsilivis, N., & Shukla, A. (2023). A Tale of Two Circuits: Grokking as Competition of Sparse and Dense Subnetworks. arXiv, 2303.11873. Retrieved from https://arxiv.org/abs/2303.11873v1

    - We find that grokking phase transition corresponds to the emergence of a sparse subnetwork that dominates model predictions: a small subset of neurons undergoes rapid norm growth, the others decay slowly
    - Thus, grokking emerges from the competition of dense and sparse subnetworks

Pan, L., & Cao, X. (2023). Towards Understanding Neural Collapse: The Effects of Batch Normalization and Weight Decay. arXiv, 2309.04644. Retrieved from https://arxiv.org/abs/2309.04644v2

#transformers_ffn Peng, Z., Qi, L., Shi, Y., & Gao, Y. (2023). A Theoretical Explanation of Activation Sparsity through Flat Minima and Adversarial Robustness. arXiv, 2309.03004. Retrieved from https://arxiv.org/abs/2309.03004v4

    - We investigate activation sparsity in MLP transformer blocks
    - We propose the notions of gradient sparsity, effective gradient sparsity and implicit adversarial robustness
    - We explain activation sparsity with flat minima and implicit adversarial robustness
    - We propose two plug-and-play and orthogonal architectural modifications (DB-MLP and J-SquaredReLU), which brings more training and test sparsity

Schaeffer, R., Khona, M., Robertson, Z., Boopathy, A., Pistunova, K., Rocks, J. W., ...Koyejo, O. (2023). Double Descent Demystified: Identifying, Interpreting & Ablating the Sources of a Deep Learning Puzzle. arXiv, 2303.14151. Retrieved from https://arxiv.org/abs/2303.14151v1

Shen, E., Farhadi, A., & Kusupati, A. (2023). Are "Hierarchical" Visual Representations Hierarchical? arXiv, 2311.05784. Retrieved from https://arxiv.org/abs/2311.05784v2

    - We create HierNet, 12 datasets spanning 3 kinds of hierarchy from the BREEDs subset of ImageNet
    - We evaluate Hyperbolic and Matryoshka Representations
    - We conclude that they do not capture hierarchy better than the standard representations
    - But they can assist in other aspects like search efficiency and interpretability

#attention #transformer_ffn Shen, K., Guo, J., Tan, X., Tang, S., Wang, R., & Bian, J. (2023). A Study on ReLU and Softmax in Transformer. arXiv, 2302.06461. Retrieved from https://arxiv.org/abs/2302.06461v1

    - It is known that both FFN and attention in transformers can be viewed as key-value memories, however, with different activation functions (ReLU and Softmax respectively), which makes them not equivalent
    - We conduct extensive studies on ReLU and Softmax; they use different normalization methods over elements which lead to different variances of results, and ReLU is good at dealing with a large number of key-value slots
    - We propose a full ReLU architecture named ReLUFormer, which performs better than the baseline Transformer on long sequence tasks such as document translation

Simon, J. B., Karkada, D., Ghosh, N., & Belkin, M. (2023). More is Better in Modern Machine Learning: when Infinite Overparameterization is Optimal and Overfitting is Obligatory. arXiv, 2311.14646. Retrieved from https://arxiv.org/abs/2311.14646v2

Súkeník, P., Mondelli, M., & Lampert, C. (2023). Deep Neural Collapse Is Provably Optimal for the Deep Unconstrained Features Model. arXiv, 2305.13165. Retrieved from https://arxiv.org/abs/2305.13165v1

Vasilescu, M. A. O. (2023). Causal Deep Learning: Causal Capsules and Tensor Transformers. arXiv, 2301.00314. Retrieved from https://arxiv.org/abs/2301.00314v1

    - NNs and tensor factorization methods may perform causal inference, or simply perform regression
    - A new deep neural network composed of a hierarchy of autoencoders
    - This results in a hierarchy of kernel tensor factor models
    - Forward causal questions (what if?) and inverse causal questions (why?) are addressed

Verwimp, E., Aljundi, R., Ben-David, S., Bethge, M., Cossu, A., Gepperth, A., ...van de Ven, G. M. (2023). Continual Learning: Applications and the Road Forward. arXiv, 2311.11908. Retrieved from https://arxiv.org/abs/2311.11908v2

Wang, M., Pan, Y., Xu, Z., Yang, X., Li, G., & Cichocki, A. (2023). Tensor Networks Meet Neural Networks: A Survey and Future Perspectives. arXiv, 2302.09019. Retrieved from https://arxiv.org/abs/2302.09019v2

    - Tensor networks (TNs) were introduced to solve the curse of dimensionality in large-scale tensors
    - We refer to the combinations of NNs and TNs as tensorial neural networks (TNNs)
    - Three primary aspects: network compression, information fusion, and quantum circuit simulation
    - Methods for improving TNNs, implementing TNNs, future directions

#attention Wortsman, M., Lee, J., Gilmer, J., & Kornblith, S. (2023). Replacing softmax with ReLU in Vision Transformers. arXiv, 2309.08586. Retrieved from https://arxiv.org/abs/2309.08586v2

    - Previous research observed accuracy degradation when replacing the attention softmax with a ReLU
    - In vision transformers, this degradation is mitigated when dividing by sequence length, and ReLU-attention can approach or match the performance of softmax-attention

Wu, J., Yang, T., Hao, X., Hao, J., Zheng, Y., Wang, W., & Taylor, M. E. (2023). PORTAL: Automatic Curricula Generation for Multiagent Reinforcement Learning. AAMAS '23: Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems. International Foundation for Autonomous Agents and Multiagent Systems. Retrieved from https://dl.acm.org/doi/10.5555/3545946.3598967

Xie, L., Wei, L., Zhang, X., Bi, K., Gu, X., Chang, J., & Tian, Q. (2023). Towards AGI in Computer Vision: Lessons Learned from GPT and Large Language Models. arXiv, 2306.08641. Retrieved from https://arxiv.org/abs/2306.08641v1

Xu, Z., Wang, Y., Frei, S., Vardi, G., & Hu, W. (2023). Benign Overfitting and Grokking in ReLU Networks for XOR Cluster Data. arXiv, 2310.02541. Retrieved from https://arxiv.org/abs/2310.02541v1

Yang, G., & Littwin, E. (2023). Tensor Programs IVb: Adaptive Optimization in the Infinite-Width Limit. arXiv, 2308.01814. Retrieved from https://arxiv.org/abs/2308.01814v2

Yang, G., Yu, D., Zhu, C., & Hayou, S. (2023). Tensor Programs VI: Feature Learning in Infinite-Depth Neural Networks. arXiv, 2310.02244. Retrieved from https://arxiv.org/abs/2310.02244v5

Ye, J., Zhu, Z., Liu, F., Shokri, R., & Cevher, V. (2023). Initialization Matters: Privacy-Utility Analysis of Overparameterized Neural Networks. arXiv, 2310.20579. Retrieved from https://arxiv.org/abs/2310.20579v1

Zhao, M., Alver, S., van Seijen, H., Laroche, R., Precup, D., & Bengio, Y. (2023). Consciousness-Inspired Spatio-Temporal Abstractions for Better Generalization in Reinforcement Learning. arXiv, 2310.00229. Retrieved from https://arxiv.org/abs/2310.00229v4

Zheng, C., Wu, G., Bao, F., Cao, Y., Li, C., & Zhu, J. (2023). Revisiting Discriminative vs. Generative Classifiers: Theory and Implications. arXiv, 2302.02334. Retrieved from https://arxiv.org/abs/2302.02334v2

#training Zhu, L., Liu, C., Radhakrishnan, A., & Belkin, M. (2023). Catapults in SGD: spikes in the training loss and their impact on generalization through feature learning. arXiv, 2306.04815. Retrieved from https://arxiv.org/abs/2306.04815v2

    - Why do spikes in the training loss of SGD occur during training, and how do the spikes relate to generalization?
    - We show that they are “catapults”, originally observed in GD with large LR in https://arxiv.org/abs/2003.02218
    - These catapults occur in a low-dimensional subspace spanned by the top eigenvectors of the tangent kernel
    - Catapults help generalization because they promote feature learning by increasing alignment with the Average Gradient Outer Product (AGOP, https://arxiv.org/abs/2212.13881)
    - We demonstrate that a smaller batch size in SGD induces a larger number of catapults, thereby improving AGOP alignment and test performance

## 2024

#ssl Ayromlou, S., Afkanpour, A., Khazaie, V. R., & Forghani, F. (2024). Can Generative Models Improve Self-Supervised Representation Learning? arXiv, 2403.05966. Retrieved from https://arxiv.org/abs/2403.05966v2

    - The existing SSL techniques often rely on a limited set of simple transformations
    - We enrich the SSL paradigm by utilizing generative models to produce semantically consistent image augmentations
    - Our method enables the generation of diverse augmentations while maintaining the semantics
    - Our framework significantly enhances the quality of learned visual representations

#training Guo, L., Ross, K., Zhao, Z., Andriopoulos, G., Ling, S., Xu, Y., & Dong, Z. (2024). Cross Entropy versus Label Smoothing: A Neural Collapse Perspective. arXiv, 2402.03979. Retrieved from https://arxiv.org/abs/2402.03979v2

    - Why label smoothing (LS) provides better convergence and performance than cross-entropy (CE) loss?
    - We show that models trained with label smoothing converge faster to neural collapse (NC) solutions
    - At the same level of NC1 ("The learned features of the samples within the same class approach their respective class means"), models under label smoothing loss exhibit intensified NC2 ("the collapsed features from different classes and the classification weight vectors approach the vertices of a simplex equiangular tight frame")
    - We demonstrate an excessive level of NC1 can make the model overconfident in incorrect predictions
    - So, models trained under LS loss exhibit improved calibration
    - We also conduct a theoretical analysis of the optimization landscape

Hirono, Y., Tanaka, A., & Fukushima, K. (2024). Understanding Diffusion Models by Feynman's Path Integral. arXiv, 2403.11262. Retrieved from https://arxiv.org/abs/2403.11262v1

Huang, Q., Wake, N., Sarkar, B., Durante, Z., Gong, R., Taori, R., ...Gao, J. (2024). Position Paper: Agent AI Towards a Holistic Intelligence. arXiv, 2403.00833. Retrieved from https://arxiv.org/abs/2403.00833v1

#robustness #training Humayun, A. I., Balestriero, R., & Baraniuk, R. (2024). Deep Networks Always Grok and Here is Why. arXiv, 2402.15555. Retrieved from https://arxiv.org/abs/2402.15555v2

    - We demonstrate that grokking is actually widespread
    - We introduce a concept of delayed robustness, whereby a DNN groks adversarial examples long after interpolation and/or generalization; we make this observation for a number of training settings in CV and NLP
    - We develop an analytical explanation for both delayed generalization and delayed robustness based on a "progress measure": local complexity (the density of so-called “linear regions”) of a DNN’s input-output mapping

Lu, C., Lu, C., Lange, R. T., Foerster, J., Clune, J., & Ha, D. (2024). The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery. arXiv, 2408.06292. Retrieved from https://arxiv.org/abs/2408.06292v2

    - We present The AI Scientist: the first framework for fully automatic scientific discovery, which generates novel research ideas, writes code, executes experiments, visualizes results, describes its findings by writing a full scientific paper.
    - We also design an automated reviewer, which achieves near-human-level performance across multiple evaluation metrics (e.g. 65% vs. 66% balanced accuracy) when evaluated on ICLR 2022 OpenReview data.
    - The AI Scientist can produce papers that exceed the acceptance threshold at a top machine learning conference as judged by our automated reviewer.
    - We provide The AI Scientist with a starting code template that reproduces a lightweight baseline training run from a popular model or benchmark. Given a starting template, The AI Scientist first “brainstorms” a diverse set of novel research directions. Each idea comprises a description, experiment execution plan, and (self-assessed) numerical scores of interestingness, novelty, and feasibility. We iteratively grow an archive of ideas using LLMs as the mutation operator. We use multiple rounds of chain-of-thought and self-reflection. For each run, we provide 1-2 basic seed ideas as examples (e.g. modifying the learning rate or batch size) and have it generate another 50 new ideas. Then we filter ideas by connecting the LM with the Semantic Scholar API. This allows The AI Scientist to discard any idea that is too similar to existing literature. We visualize an example progression of proposed ideas in Appendix C.
    - Given an idea and a template, The AI Scientist uses Aider (https://github.com/paul-gauthier/aider) to first plan a list of experiments to run and then executes them in order. We return any errors upon a failure or time-out (e.g. experiments taking too long to run) to Aider to fix the code and re-attempt up to four times. After the completion of each experiment, Aider is then given the results and told to take notes in the style of an experimental journal. Currently, it only conditions on text but in future versions, this could include data visualizations or any modality. Conditional on the results, it then re-plans and implements the next experiment. This process is repeated up to five times. Upon completion of experiments, Aider is prompted to edit a plotting script to create figures for the paper using Python. At all steps, Aider sees its history of execution.
    - The recorded notes and plots are passed to Aider, which is prompted to fill in a blank conference template section by section. We include brief tips and guidelines on what each section should include, based on the popular “How to ML Paper” guide. At each step of writing, Aider is prompted to only use real experimental results in the form of notes and figures generated from code, and real citations to reduce hallucination. Each section is initially refined with one round of self-reflection.
    - Then The AI Scientist is allowed 20 rounds to poll the Semantic Scholar API looking for the most relevant sources to compare and contrast the near-completed paper against for the related work section.
    - After the previous stages, The AI Scientist has a completed first draft, but can often be overly verbose and repetitive. We perform one final round of self-reflection section-by-section, aiming to remove any duplicated information and streamline the arguments of the paper.
    - Once the LaTeX template has been filled in with all the appropriate results, this is fed into a LaTeX compiler. We use a LaTeX linter and pipe compilation errors back into Aider so that it can automatically correct any issues.
    - We design a GPT-4o-based agent to conduct paper reviews based on the NeurIPS conference review guidelines. It processes the raw text of the PDF manuscript using the PyMuPDF parsing library. The output contains numerical scores (soundness, presentation, contribution, overall, confidence), lists of weaknesses and strengths as well as a preliminary binary decision (accept or reject). The AI Scientist’s reviewing procedure achieves 70% accuracy when combining 5 rounds of self-reflection, 5 ensembled reviews, and a 1-shot review example taken from the ICLR 2022 review guidelines. This number is lower than the 73% accuracy that was reported for humans in the NeurIPS 2021 consistency experiment (when submissions were reviewed by two independent program committees to quantify the randomness in the review process https://blog.neurips.cc/2021/12/08/the-neurips-2021-consistency-experiment/). Overall, across all metrics, the results suggest that LLM-based reviews can not only provide valuable feedback but also align more closely with the average human reviewer score than individual human reviewers align with each other. The False Negative Rate is much lower than the human baseline (0.39 vs. 0.52). Hence, the LLM-based review agent rejects fewer high-quality papers. The False Positive Rate (FNR), on the other hand, is higher than the human baseline (0.31 vs. 0.17). Unlike standard reviewers, the automated reviewer is unable to ask questions to the authors in a rebuttal phase, although this could readily be incorporated into our framework.
    - We additionally compared the reviewing performance of various other foundation models. The performance of Claude Sonnet 3.5 and GPT4o-mini was substantially worse. Moreover, we had to threshold scores at 8 for Sonnet 3.5 to obtain calibrated results, due to persistent over-optimism bias. Llama 3.1 405B struggled to follow the reviewer output template consistently.
    - We further discuss a paper “Adaptive Dual-Scale Denoising” generated by Claude Sonnet 3.5 (available in Appendix D.1). The idea in the selected paper was proposed in the 6th iteration of the algorithm and aims to improve the ability of diffusion models to capture both global structure and local details in a 2D dataset, by proposing two branches in the standard denoiser network. This is a well-motivated direction, and to the best of our knowledge has not been widely studied. The AI Scientist generates an impressive experimental plan that includes the proposed code modification, comparison to baselines, evaluation metrics, and the design of additional plots. The “novel” flag at the end indicates The AI Scientist believes the idea is novel after searching for related papers using the Semantic Scholar API. We display the generated code diff (deletions are in red, and additions are in green) (see page 8). The particularly impressive things in the paper are precise mathematical description of the algorithm, comprehensive write-up of experiments, good empirical results, new visualizations, interesting future work section. However, The AI Scientist made a subtle error in upscaling network, hallucinated that V100 GPUs were used (in reality, H100 GPUs were used), guessed the PyTorch version without checking. The paper tends to take a positive spin even on its negative results, which leads to slightly humorous outcomes: while lower KL is better, change from 0.090 to 0.093 was called an "improvement".
    - The automated reviewer points out valid concerns: it recognizes the experiments were with simple, 2D datasets only, computational cost is significantly higher, insufficient ablation studies etc.
    - Drawing from our domain knowledge in diffusion modeling - we present our overall opinions on the paper generated. The AI Scientist correctly identifies an interesting and well-motivated direction. While the paper’s idea improves performance and the quality of generated diffusion samples, the reasons for its success may not be as explained in the paper. In particular, there is no obvious inductive bias beyond an upscaling layer (effectively just an additional linear layer) for the splitting of global or local features. However, we do see progression in weights (and thus a preference for the global or local branch) across diffusion timesteps which suggests that something non-trivial is happening. Our interpretation is instead that the network that the idea resembles a mixture-of-expert (MoE), which could indeed lead to the diffusion model learning separate branches for global and local features, as the paper claims, but this statement requires more rigorous investigation.
    - Overall, we judge the performance of The AI Scientist to be about the level of an early-stage ML researcher who can competently execute an idea but may not have the full background knowledge to fully interpret the reasons behind an algorithm’s success.
    - From manual inspection, we find that Claude Sonnet 3.5 consistently produces the highest quality papers, with GPT-4o coming in second. We provide a link to all papers, run files, and logs in our GitHub repository. For the open-weight models, DeepSeek Coder is significantly cheaper but often fails to correctly call the Aider tools. Llama-3.1 405b performed the worst overall but was the most convenient to work with, as we were frequently rate-limited by other providers. Both DeepSeek Coder and Llama-3.1 405b often had missing sections and results in their generated papers.
    - Shortcomings are the following. The idea generation process often results in very similar ideas across different runs and even models. Aider fails to implement a significant fraction of the proposed ideas. The AI Scientist may incorrectly implement an idea, which can be difficult to catch. The AI Scientist ofren make deceptive or inaccurate conclusions and sometimes struggles to find and cite the most relevant papers. Importantly, The AI Scientist occasionally makes critical errors when writing and evaluating results. For example, it struggles to compare the magnitude of two numbers, which is a known pathology with LLMs. Rarely, The AI Scientist can hallucinate entire results.

Pagliardini, M., Mohtashami, A., Fleuret, F., & Jaggi, M. (2024). DenseFormer: Enhancing Information Flow in Transformers via Depth Weighted Averaging. arXiv, 2402.02622. Retrieved from https://arxiv.org/abs/2402.02622v2

Paolo, G., Gonzalez-Billandon, J., & Kégl, B. (2024). A call for embodied AI. arXiv, 2402.03824. Retrieved from https://arxiv.org/abs/2402.03824v1

Peters, B., DiCarlo, J. J., Gureckis, T., Haefner, R., Isik, L., Tenenbaum, J., ...Kriegeskorte, N. (2024). How does the primate brain combine generative and discriminative computations in vision? arXiv, 2401.06005. Retrieved from https://arxiv.org/abs/2401.06005v1

Sohl-Dickstein, J. (2024). The boundary of neural network trainability is fractal. arXiv, 2402.06184. Retrieved from https://arxiv.org/abs/2402.06184v1

Song, Y., Millidge, B., Salvatori, T., Lukasiewicz, T., Xu, Z., & Bogacz, R. (2024). Inferring neural activity before plasticity as a foundation for learning beyond backpropagation. Nat. Neurosci., 27, 348–358. Retrieved from https://www.nature.com/articles/s41593-023-01514-1

## Fine-tuning

Li, Z., & Hoiem, D. (2016). Learning without Forgetting. arXiv, 1606.09282. Retrieved from https://arxiv.org/abs/1606.09282v3

    - Currently, there are three common approaches for learning new tasks: feature extraction (training only output layer), full fine-tuning, and joint training (training on both new and old tasks simultaneously). Feature extraction typically underperforms on the new task, fine-tuning degrades performance on previously learned tasks, and joint training becomes increasingly cumbersome as more tasks are learned and is not possible if the training data for previously learned tasks is unavailable (fig. 1).
    - In this paper, we expand on our previous work, Learning without Forgetting (LwF). Our method is similar to joint training, except that our method does not need the old task’s images and labels. The performance of joint training may be seen as an upper bound of what our proposed method can achieve.
    - First, we record responses on each new task image from the original network for outputs on the old tasks ( label probabilities for classification). We add new head for a new task, while keeping old head, and first train the new head to convergence (warm-up step). Then, we jointly train all weights (full network with both heads), while the targets for the old head is the record responses from the pre-trained network (fig. 2).
    - IMO, this looks like distillation + learning the new task.

Reimers, N., & Gurevych, I. (2017). Reporting Score Distributions Makes a Difference: Performance Study of LSTM-networks for Sequence Tagging. arXiv, 1707.09861. Retrieved from https://arxiv.org/abs/1707.09861v1

    - We show that reporting a single performance score is insufficient to compare non-deterministic approaches.
    - We propose to compare score distributions based on multiple executions.
    - For two recent systems for NER, we observe an absolute difference of 1% F1 score, depending on the seed, making these systems perceived either as SOTA or mediocre. Fixing the random seed value would solve the issue with the reproducibility, however, there is no justification for choosing one seed value over another seed value.
    - We present LSTM architectures that produce both superior performance as well as are more stable with respect to the remaining hyperparameters.

Wiese, G., Weissenborn, D., & Neves, M. (2017). Neural Domain Adaptation for Biomedical Question Answering. arXiv, 1706.03610. Retrieved from https://arxiv.org/abs/1706.03610v2

    - We focus on extractive question answering (QA), when the correct answers can be represented as spans in the relevant documents.
    - Creating large-scale QA datasets for specific domains, such as the biomedical, would be very expensive because of the need for domain experts. Are the capabilities of trained models transferable to another domain via domain adaptation techniques?
    - We employ various domain adaptation techniques on a small BioASQ dataset for extractive QA task.
    - We improve SOTA with a forgetting cost regularization to avoid catastrophic forgetting (see "Representation Stability as a Regularizer for Improved Text Analytics Transfer Learning").
    - We also add an L2 loss term which penalizes deviations from the base model’s parameters.
    - IMO, this looks like a simplified version of Elastic Weight Consolidation.

Li, X., Grandvalet, Y., & Davoine, F. (2018). Explicit Inductive Bias for Transfer Learning with Convolutional Networks. International Conference on Machine Learning. PMLR. Retrieved from https://proceedings.mlr.press/v80/li18a.html

    - We evaluate fine-tuning regularizers based on the L2, Lasso and Group-Lasso penalties for the weight difference between pre-trained and fine-tuned models. We call them L2-SP, L1-SP and GL-SP, where SP means "starting point", that is pre-trained weights. We also try L2-SP-Fisher and GL-SP-Fisher motivated by Elastic Weight Consolidation (see "Overcoming catastrophic forgetting in neural networks"), when we use the estimated Fisher information to define the distance between models.
    - We find that L2-SP is much more effective than the standard L2 penalty that is commonly used in fine-tuning. We thus believe that this simple L2-SP scheme should be considered as the standard baseline.
    - None of the L1 or Group-L1 options seem to be valuable in our setting, and using the Fisher information with L2-SP does not improve accuracy on the target task.

Dror, R., Shlomov, S., & Reichart, R. (2019). Deep Dominance - How to Properly Compare Deep Neural Models. ACL Anthology, 2773–2785. doi: 10.18653/v1/P19-1266

    - We need to compare DNNs score distributions, rather than between single evaluation scores. This is because the loss functions of these models are non-convex, making the solution to which they converge sensitive to random weight initialization and the order of training examples.
    - We build on a previously proposed Stochastic Order relation between two random variables: a random variable X is stochastically larger than a random variable Y if if CDF_X(a) ≤ CDF_Y(a) for all a, with a strict inequality for some values of a. We can relax this criterion. We introduce two violation sets: V_X = {a: CDF_X(a) > CDF_Y(a)},  V_Y = {a: CDF_X(a) < CDF_Y(a)}. Intuitively, the variable with the smaller violation set should be declared superior and the ratio between these sets should define the gap between the distributions. To implement this idea, del Barrio et al. (2018) defined the concept of almost stochastic dominance (sec. 4), which we propose to employ. This is a test for statistical significance under very minimal assumptions on the distribution.
    - To test for almost stochastic dominance, we formulate the hypothesis testing problem (see H0, H1 in sec. 4, and eq. 4 for details).
    - (Seems like) random variable is a metric for a single training run. "As an example setup we analyze the comparison between the NER models when running both algorithms multiple times, changing only the random seed fed into the random number generator".

Hao, Y., Dong, L., Wei, F., & Xu, K. (2019). Visualizing and Understanding the Effectiveness of BERT. arXiv, 1908.05620. Retrieved from https://arxiv.org/abs/1908.05620v1

    - We visualize fine-tuning BERT on specific datasets by showing: 1) loss curve obtained by linear interpolation from starting to final weights, 2) two-dimensional loss surface with the projection of the optimization trajectory, obtained in the same way, when one direction is the difference between starting and final weights, and the second direction is difference between starting and final weights fine-tuned on another dataset. Experiments show that these directions are divergent and orthogonal to each other.
    - BERT pretraining reaches a good initial point across downstream tasks, which leads to wider optima on the 2D loss landscape (comparing to random initialization).
    - We give evidence that the pre-training-then-fine-tuning paradigm is robust to overfitting.
    - We show that the lower layers of BERT are more invariant across tasks. We verify the point by visualizing the loss landscape with respect to different groups of layers.
    - IMO, when comparing the width of optima, we need to measure width in absolute and nor relative (percents of the full plot) numbers. I have found nothing about this in the paper (sec. 5.1).

Lee, C., Cho, K., & Kang, W. (2019). Mixout: Effective Regularization to Finetune Large-scale Pretrained Language Models. arXiv, 1909.11299. Retrieved from https://arxiv.org/abs/1909.11299v2

    - Finetuning pre-trained LM on a small dataset is prone to degenerate performance.
    - Dropout and its variants, such as Gaussian dropout, variational dropout, and drop-connect, can be understanded as an adaptive L2-penalty toward the all zero parameters (see "Regularization of neural networks using dropconnect" and "On Dropout and Nuclear Norm Regularization").
    - It has been shown earlier (see "Train longer, generalize better: closing the generalization gap in large batch training of neural networks") that the distance between pre-trained and fine-tuned model grows logarithmically during fine-tuning steps. Also, it has been shown (see "Neural Domain Adaptation for Biomedical Question Answering") that weight decay towards the pre-trained model is effective to avoid catastrophic forgetting during finetuning.
    - We propose mixout regularization by generalizing gropout by considering some target model parameters (instead of all zero parameters). During forward pass, it replaces all outgoing parameters from randomly selected neurons to the corresponding parameters of original pre-trained model (fig. 1). thus regularizing learning to minimize the deviation between models.
    - The earlier work "Regularization techniques for fine-tuning in neural machine translation" is a special case of mixout.
    - IMO, this paper is a good introduction to dropout variants.
    - IMO, it is not clearly stated how are the gradients applied in mixout. Probably, the gradients over the pre-trained weights are calculated, used on further backprop, but not applied.

Aghajanyan, A., Zettlemoyer, L., & Gupta, S. (2020). Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning. arXiv, 2012.13255. Retrieved from https://arxiv.org/abs/2012.13255v1

    - For pre-trained LMs, intrinsic dimensionality means how many free parameters are required to closely approximate the fine-tuning optimization problem.
    - We show that the process of pre-training itself implicitly minimizes the intrinsic dimension of later tuning for different NLP tasks. Common NLP tasks within the context of pre-trained representations have an intrinsic dimension several orders of magnitudes less than the full parameterization. For example, 200 parameters (randomly projected back into the full parameter space) are enough to represent the problem of tuning a RoBERTa model to within 90% of the performance of the full model.
    - We show that number of parameters strongly inversely correlates with intrinsic dimensionality, across a large set of recently developed pre-training methods.
    - We propose a new interpretation of intrinsic dimension as the downstream fine-tuning task’s minimal description length within the framework of the pre-trained model. The process of pre-training implicitly optimizes the description length over the average of NLP tasks, without having direct access to those same tasks.
    - We show that compression based generalization bounds can be applied to our intrinsic dimension framework to provide generalization bounds for large pre-trained models independent of the pre-trained model parameter count.
    - IMO needed ablations to check if pre-training on random noise also reduces the intrinsic dimension (of later tuning for different NLP tasks).
    - TODO read

Dodge, J., Ilharco, G., Schwartz, R., Farhadi, A., Hajishirzi, H., & Smith, N. (2020). Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping. arXiv, 2002.06305. Retrieved from https://arxiv.org/abs/2002.06305v1

    - It is known that fine-tuning performance can vary substantially across different training episodes, even with fixed hyperparameter values.
    - We confirm this: changing only training data order and the weight initialization of the fine-tuning layer - we find substantial variance in performance across trials. For BERT model, it leads to substantial improvements over previously published validation results with the same model and experimental setup (top rows), on four tasks from the GLUE benchmark. On some tasks, BERT even becomes competitive with more modern models (XLNet, RoBERTa, ALBERT).
    - We present how the performance of the bestfound model changes as a function of the number of trials (fig. 1).
    - Some seeds are consistently better than others in a given dataset for both weight initializations and data orders. Some weight initializations for output layer perform well across all studied tasks (binary classification).
    - Worse performing models can often be distinguished from better ones early in training. We show that better performance can be achieved with the same computational resources by using early stopping algorithms that stop the least promising trials early in training: start many, stop early, continue some.

Gromov, V. A., & Migrina, A. M. (2020). A Language as a Self-organized Critical System. . doi: 10.37247/PACS.1.2020.4

    - The authors hypothesize that a natural language is a self-organized critical system (SOC) (see P. Bak, "How Nature Works: The Science of Self-Organized Criticality"). In SOC, we consider some space space of elements able to be in two states, active and passive, and the “avalanches” are chain reactions of elements’ state changes triggered by changes of other elements. A power-law distribution governing avalanche sizes. SOC-systems usually exhibit long periods of slow evolution as opposed to short periods of fast evolution when the system space is changed drastically; similar phenomenon is reported to take place for evolving language systems.
	- In natural language, we formalize the space as a cooccurrence graph: vertices correspond to words and an edge is present if and only if the words associated with its incident vertices occur simultaneously in the same text of the sample involved, once or more. An avalanche, in this context, is a text of a language.
	- The hypothesis that sizes of avalanches obey a power-law distribution forms the subject of the present study. The present work is focused on the study (and comparison) of statistical characteristics of texts sets for English and Russian languages being considered in their synchronic and diachronic aspects.
	- Poetry distributions appear closest to a canonical power-law and therefore poetry may be treated as a kind of supporting column of a language (fig. 1).

Mosbach, M., Andriushchenko, M., & Klakow, D. (2020). On the Stability of Fine-tuning BERT: Misconceptions, Explanations, and Strong Baselines. arXiv, 2006.04884. Retrieved from https://arxiv.org/abs/2006.04884v3

    - The two common hypotheses for fine-tuning instability are catastrophic forgetting and small size of the fine-tuning datasets. We argue that none of them has a causal relationship with fine-tuning instability.
    - By fine-tuning RoBERTa and ALBERT on RTE task, we select three successful and three failed fine-tuning runs (when the accuracy at the end of training is less or equal to that of a majority classifier on the respective dataset) and evaluate their MLM perplexity on the test set of the WikiText-2 language modeling benchmark. We sequentially substitute the top-k layers of the network varying k from 0 (i.e. all layers are from the fine-tuned model) to 24 (i.e. all layers are from the pre-trained model). We observe that catastrophic forgetting affects only the top layers of the network (often around 10 of 24 layers), and the same is however also true for the successfully fine-tuned models, except for a much smaller increase in perplexity. Another important aspect of our experiment is that catastrophic forgetting typically requires that the model at least successfully learns how to perform the new task, however, this is not the case for the failed fine-tuning runs.
    - We also test if having a small training dataset inherently leads to fine-tuning instability. We show that this is only because we usually do less training steps with a small dataset. What is crucial is rather the number of training iterations.
    - We observe that the failed runs have practically constant training loss throughout the training. We plot the L2 gradient norms of the loss function with respect to different layers of BERT, and for the failed run we see large enough gradients only for the top layers and vanishing gradients for the bottom layers (fig. 4).
    - This is harder to resolve than the standard vanishing gradient problem, since we cannot modify weight intialization in pre-trained model. We observe that Adam's bias correction may help here. An alternative solution is to simply train longer with a smaller LR, which also leads to much more stable fine-tuning (as also shown in the concurrent work "Revisiting Few-sample BERT Fine-tuning").
    - We provide loss surface visualizations of failed and successful runs when fine-tuning BERT (fig. 7). For failed fine-tuning runs gradient descent converges to a “bad” valley with a sub-optimal training loss. Moreover, this bad valley is separated from the local minimum (to which the successfully trained run converged) by a barrier. We observe a highly similar geometry for all three datasets.
    - We find that despite achieving close to zero training loss overfitting is not an issue during fine-tuning: dev set accuracy does not degrade even when the training loss is as low as 1e−5.
    - We propose to 1) use small LR (such as 2e-5) with bias correction to avoid vanishing gradients early in training, and 2) increase the number of iterations for small datasets considerably and train to (almost) zero training loss.
    - Our method leads to a much more stable fine-tuning performance on all three datasets, comparing to Mixout  (fig. 1, "Lee") and original BERT fine-tuning (fig. 1, "Devlin").

Radiya-Dixit, E., & Wang, X. (2020). How fine can fine-tuning be? Learning efficient language models. International Conference on Artificial Intelligence and Statistics. PMLR. Retrieved from https://proceedings.mlr.press/v108/radiya-dixit20a.html

    - We find (as expected) that the finetuned models are close to the pre-trained one (both L1-close and angular-close in parameter space), with the closeness varying from layer to layer.
    - We then show that it suffices to finetune only the most critical layers.
    - We also attempt to learn a task by sparsifying the pre-trained weights. We find that a specific task can be learned by simply masking anywhere between 1% to 40% of the pretrained weights to zero. To do this, we reparemetrize a binary mask as continous sigmoid mask with bernoulli sampling. Control over the final sparsity is exerted by initialization of sigmoid mask for fine-tuning.

Taori, R., Dave, A., Shankar, V., Carlini, N., Recht, B., & Schmidt, L. (2020). Measuring Robustness to Natural Distribution Shifts in Image Classification. arXiv, 2007.00644. Retrieved from https://arxiv.org/abs/2007.00644v2

    - Some studies implicitly assume that hat robustness to synthetic distribution shifts (l_p-adversarial examples, noise corruptions, etc.) will lead to models that also perform more reliably on natural distribution shifts.
    - Our paper is a meta-study of current robustness research.
    - We evaluate on seven natural distribution shifts that we classify into three categories. 1) Consistency shifts: robustness to small changes across video frames. Here we evaluate the "accuracy under distribution shift" by choosing the worst frame from each frame set for a classifier; this is a known "pm-k" metric (see "Do image classifiers generalize across time?"). 2) Dataset shifts: shifts between datasets that are collected in a different manner with a compatible set of classes. 3) Adversarially filtered shifts  with an adversarially collected dataset ImageNet-A by collecting images from Flickr, DuckDuckGo, iNaturalist, and selecting the subset that was misclassified by a ResNet-50 model.
    - We evaluate robustness of 204 ImageNet models in 213 different test conditions and show that current methods (adversarial training, various forms of data augmentation, etc.) tested on synthetic robustness measures do not imply natural robustness. In particular, the accuracy on the original test set alone almost perfectly predicts the accuracy on the test sets with distribution shift, if using a linear predictor trained on standard models (fig. 1).
    - Based on this linear trend, we propose the "effective robustness" metric. In our experiments, we always have two evaluation settings: the “standard” test set, and the test set with distribution shift. We aim to disentangle the robustness of a model from its accuracy on the standard test set, so that the latter no longer acts as a confounder. We would like to understand if a model B offers higher accuracy on the shifted test set beyond what is expected from having higher accuracy on the original test set. We call this notion of robustness beyond a baseline "effective robustness" (see sec. 2). Graphically, effective robustness corresponds to a model being above the linear trend (red line) by our testbed of standard models that were simply trained on the same distribution as the “standard” test set.
    - Effective robustness alone does not imply that a robustness intervention is useful, since a robustness intervention may decrease both in-distribution and OOD accuracies. Such a robustness intervention would offer no benefits. So, we also introduce relative robustness, which is simply an OOD accuracy gain from applying the intervention. Overall, a useful robustness intervention should obtain both positive effective and relative robustness.
    - We colcude that no models achieve both large effective and relative robustness. In many existing work the "improved robustness" can be explained by the model performing better on the standard, unperturbed test set. While training more accurate models is clearly useful, it is important to separate accuracy improvements from robustness improvements when interpreting the results.
    - The only intervention that slightly improves the effective robustness to multiple natural distribution shifts is training with a more diverse dataset. At the same time, we find some models that are trained on 100X more data than the standard ImageNet but do not provide any robustness.
    - There is an earlier version of this paper called "When Robustness Doesn’t Promote Robustness: Synthetic vs. Natural Distribution Shifts on ImageNet". Some of the text above is from this version.

Zhang, T., Wu, F., Katiyar, A., Weinberger, K. Q., & Artzi, Y. (2020). Revisiting Few-sample BERT Fine-tuning. arXiv, 2006.05987. Retrieved from https://arxiv.org/abs/2006.05987v3

    - It is known that fine-tuning of BERT-large on small datasets is unstable. Identical fine-tunings with different random seeds often result in significantly different and sometimes degenerate models. As a result, practitioners resort to multiple random trials for model selection.
    - We show that:
    - 1) The most commonly used optimizer for fine-tuning BERT is BERT-ADAM, a modified version of the ADAM that omits a bias correction step. This non-standard implementation is widely used in both industry and research, however, it introduces bias in the gradient estimation.
    - 2) The top layers of the pre-trained BERT provide a detrimental initialization for fine-tuning and delay learning. Simply re-initializing these layers not only speeds up learning but also leads to better model performance.
    - 3) The common three-epochs practice for BERT fine-tuning is sub-optimal for small datasets, and allocating more training time can stabilize fine-tuning.
    - We construct a baseline with all these modifications. Comparing to this baseline, the recently proposed methods (Mixout, Weight decay towards the pre-trained model and Transferring via an Intermediate Task) still perform favorably, but their benefits are less pronounced. This indicates that these methods potentially ease the optimization difficulty brought by the debiasing omission in BERT-ADAM, and when we add the debiasing, the positive effects are reduced.

Andreassen, A., Bahri, Y., Neyshabur, B., & Roelofs, R. (2021). The Evolution of Out-of-Distribution Robustness Throughout Fine-Tuning. arXiv, 2106.15831. Retrieved from https://arxiv.org/abs/2106.15831v1

    - It was shown across a wide range of models that there is a clear linear relationship between a model’s final performance on ID and OOD data (see "Do ImageNet Classifiers Generalize to ImageNet?"). However, even the highest-performing models will still have a gap between ID and OOD accuracy. Models which lie above the linear fit are said to exhibit effective robustness (ER), which measures the model’s OOD accuracy relative to the fit. Models with high ER (>1%) are exceedingly rare.
    - (For mode details about ER see sec. 3 or the original paper "When robustness doesn't promote robustness: Synthetic vs. natural distribution shifts on imagenet").
    - We find that throughout fine-tuning, pre-trained models exhibit ER that vanishes at convergence (fig. 1c, orange), while randomly initialized models do not (fig. 1c, red). So, when approaching convergence during fine-tuning, pre-trained models gradually lose their ER, even as both the ID and OOD accuracies of the model simultaneously increase.
    - We try several solutions to mitigate this problem, but none of them were able to maintain high ER at high ID accuracy.
    - We find that effectively robust models make remarkably dissimilar predictions compared to standard models, and are able to correctly classify 10% of the examples that no other model gets correct.

He, J., Zhou, C., Ma, X., Berg-Kirkpatrick, T., & Neubig, G. (2021). Towards a Unified View of Parameter-Efficient Transfer Learning. arXiv, 2110.04366. Retrieved from https://arxiv.org/abs/2110.04366v3

    - It was show that besides parameter savings, parameter-efficient tuning (PEFT) makes it possible to quickly adapt to new tasks without catastrophic forgetting and often exhibits superior robustness in out-of-distribution evaluation.
    - We show that existing PEFT methods (adapters, prefix-tuning, LoRA, BitFit) still lag behind full fine-tuning in text summarization, machine translation, text classification, and general language understanding (fig. 2).
    - We derive an alternative form of prefix tuning that reveals its close connections with adapters. Prefix tuning essentially applies a position-wise modification to the original head attention output through linear interpolation.
    - We devise a unified framework that frames the PEFT methods as different ways to modify the hidden representations of frozen PLMs.
    - This framework allows us to transfer design choices across approaches to propose new variants: parallel adapter and scaled parallel adapter (fig. 3). The proposed variant uses less parameters than existing methods while being more effective, matching full fine-tuning results on all four aforementioned tasks (fig. 2).

Hua, H., Li, X., Dou, D., Xu, C.-Z., & Luo, J. (2021). Noise Stability Regularization for Improving BERT Fine-tuning. arXiv, 2107.04835. Retrieved from https://arxiv.org/abs/2107.04835v1

    - It was shown that noise injected at the lower layers has very little effect on the higher layers for neural networks with good generalizability. However, for a well pre-trained BERT, we find that the higher layers are still very sensitive to the lower layer’s perturbation (by injecting a scaled Gaussian noise, fig. 1). This phenomenon coincides with the observation that transferring the top pre-trained layers of BERT slows down learning and hurts performance.
    - We propose Layer-wise Noise Stability Regularization (LNSR) to improve fine-tuning on NLP tasks, which injects Gaussian noise into some layer (usually at the first layer) and penalize the discrepancy for all subsequent layers.
    - Theoretically, LNSR encourages the local Lipschitz continuity and/or imposing a Tikhonov regularizer.
    - Our method also demonstrates advantages over L2-SP, Mixout and SMART.

Lester, B., Al-Rfou, R., & Constant, N. (2021). The Power of Scale for Parameter-Efficient Prompt Tuning. arXiv, 2104.08691. Retrieved from https://arxiv.org/abs/2104.08691v2

    - We explore “prompt tuning”, a simple yet effective mechanism for learning “soft prompts” to condition frozen LMs to perform specific downstream tasks. We freeze the entire pre-trained model and only allow an additional k tunable tokens per downstream task to be prepended to the input text.
    - We show that prompt tuning alone (with no intermediate-layer prefixes or task-specific output layers) is sufficient to be competitive with model tuning, in contrast to what's shown in "Prefix-Tuning: Optimizing Continuous Prompts for Generation". They used GPT-2 and BART, and we use T5, which is larger. They also rely on a reparameterization of the prefix to stabilize learning, which adds a large number of parameters during training, whereas our configuration does not require this reparameterization.
    - Prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method “closes the gap” and matches the strong performance of fill parameter tuning.
    - We show that “prompt ensembling”, learning multiple prompts for the same task, can boost quality and is more efficient than classic model ensembling. To process one example, rather than computing forward passes of N different models, we can execute a single forward pass with a batch size of N , replicating the example across the batch and varying the prompt.
    - For most model sizes, increasing prompt length beyond a single token is critical to achieve good performance. Across all models, increasing beyond 20 tokens only yields marginal gains. Going past 100 tokens appears mildly detrimental for larger models.
    - Prompt tuning outperforms model tuning on the majority of out-of-domain datasets.

Li, X. L., & Liang, P. (2021). Prefix-Tuning: Optimizing Continuous Prompts for Generation. arXiv, 2101.00190. Retrieved from https://arxiv.org/abs/2101.00190v1

    - We propose prefix-tuning, a lightweight alternative to fine-tuning for NLG tasks, inspired by prompting. It keeps LM parameters frozen, but optimizes a small continuous task-specific vector (called the prefix). Unlike prompting, the prefix consists entirely of free parameters which do not correspond to real tokens.
    - Prefix-tuning optimizes all layers of the prefix. We also tried to optimize only the continuous embeddings of the “virtual tokens” (embedding-only ablation), and the performance drops significantly, suggesting that tuning only the embedding layer is not sufficiently expressive.
    - In the context of personalization, we could have a separate prefix for each user trained only on that user’s data. Moreover, the prefix-based architecture enables us to even process examples from multiple users/tasks in a single batch. In contrast, we can’t batch across different users in adapter-tuning, which has personalized adapters between shared Transformer layers. (IMO maybe we can?)
    - In our experiments, with full datasets, prefix-tuning and fine-tuning are comparable for table-to-text, while prefix-tuning suffers a small degradation for summarization. In low-data settings, prefix-tuning on average outperforms fine-tuning on both tasks. Prefix-tuning also extrapolates better to tables (for table-to-text) and articles (for summarization) with unseen topics (we split the existing datasets so that training and test cover different topics).
    - Performance increases as the prefix length increases up to a threshold (200 for summarization, 10 for table-to-text) and then a slight performance drop occurs (fig. 4). Empirically, longer prefixes have a negligible impact on inference speed.
    - We can also place the trainable activations between x and y (i.e. [x; INFIX; y]) and call this infix-tuning. It slightly underperforms prefix-tuning. We believe this is because prefix-tuning can affect the activations of x and y whereas infix-tuning can only influence the activations of y.
    - We find that how the prefix is initialized has a large impact in low-data settings. Random initialization leads to low performance with high variance. Initializing with task relevant words such as “summarization” and “table-to-text” obtains slightly better performance than task irrelevant words such as “elephant” and “divide”, but using real words is still better than random.
    - There is a concurrent work "The Power of Scale for Parameter-Efficient Prompt Tuning".
    

Qin, Y., Wang, X., Su, Y., Lin, Y., Ding, N., Yi, J., ...Zhou, J. (2021). Exploring Universal Intrinsic Task Subspace via Prompt Tuning. arXiv, 2110.07867. Retrieved from https://arxiv.org/abs/2110.07867v3

    - We show that the adaptations of PLMs to various few-shot tasks can be reparameterized as optimizing only a few free parameters in a unified low-dimensional intrinsic task subspace.
    - We propose intrinsic prompt tuning (IPT) to find such a subspace, which consists of two phases: multitask subspace finding (MSF) and intrinsic subspace tuning (IST) (fig. 2). During MSF, we first obtain trained soft prompts for multiple tasks via prompt (prefix) tuning, and then learn an auto-encoder by first projecting them into the desired low-dimensional subspace and then reconstructing them with a back-projection. During IST, to adapt the PLM to unseen data and tasks, we only train the few free parameters in the lowdimensional subspace found by MSF through a fixed back-projection.
    - We find that in a 250-dimensional subspace found with 100 tasks, by only tuning 250 free parameters, we can recover 97% of the full prompt tuning performance for 100 seen tasks (using different training data) and 83% of the full prompt tuning performance for 20 unseen tasks.
    - We analyze the effect of training task types, the number of training tasks, and training data scales for IPT.
    - From this perspective, the PLMs serve as general compression frameworks, which compress the learning complexity of various tasks from very high dimensionalities to low dimensionalities.

Summers, C., & Dinneen, M. J. (2021). Nondeterminism and Instability in Neural Network Optimization. International Conference on Machine Learning. PMLR. Retrieved from https://proceedings.mlr.press/v139/summers21a.html

    - We isolate the effects of a variety of sources of nondeterminism.
    - We find they all have similar effects on measures of model diversity. Each source (including nondeterminism in low-level libraries like cuDNN) produces models of similar diversity, as measured by correlations between model predictions.
    - We explain this by showing that even changing the initialization of a single weight by the smallest possible amount within machine precision (∼6 · 10−11) produces nearly as much variability as all other sources combined.
    - We show that Snapshot Ensembling (which uses a cyclic LR schedule, creating the members of an ensemble out of models where the LR is 0 in the cyclic LR schedule) and test-time augmentation reduce model variability without any increase in model training time.

Zaken, E. B., Ravfogel, S., & Goldberg, Y. (2021). BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models. arXiv, 2106.10199. Retrieved from https://arxiv.org/abs/2106.10199v5

    - We introduce BitFit method: freezing most of the network and fine-tuning only the bias-terms is surprisingly effective.
    - With small-to-medium training data, applying BitFit on pre-trained BERT models is competitive with (and sometimes better than) fine-tuning the entire model. For larger data, the method is competitive with other sparse fine-tuning methods.
    - This supports the hypothesis that finetuning is mainly about exposing knowledge induced by LM pre-training, rather than learning new task-specific linguistic knowledge.
    - Moreover, if we allow a small degradation in performance, we can fine-tune only two bias components (the “query” and “middle-of-MLP” bias terms: in total only 0.04% of all model parameters).
    - This opens the way to trainable hardware implementations in which most of the parameters are fixed.
    - Out work is close to "One for many: Transfer learning for building hvac control" when the authors show that bias-only fine-tuning is effective for adaptation of pre-trained CV models.

Chen, G., Liu, F., Meng, Z., & Liang, S. (2022). Revisiting Parameter-Efficient Tuning: Are We Really There Yet? arXiv, 2202.07962. Retrieved from https://arxiv.org/abs/2202.07962v2

    - We found the problematic validation and testing practice of parameter-efficient fine-tuning methods (PEFT) in current studies (for example, the dev set is used for both early stopping and reporting results, and not reporting statistical significance, not accounting for the results instability). When accompanied by the instability nature of PEFT, this has led to unreliable conclusions.
    - We conduct the first comprehensive investigation into PEFT under a truly fair evaluation protocol.
    - We found that:
    - 1) Fine-tuning cannot be fully replaced so far, since there is no PEFT method that can consistently outperform fine-tuning across all tasks and settings. We conclude that PETuning may be more suitable for low-resource tasks, but struggle on medium-resource tasks and fall behind finetuning across the board on high-resource tasks (fig. 1).
    - 2) All the PEFT methods unanimously show instability across different random seeds similar to finetuning.
    - 3) Prompt-tuning lags far behind finetuning, which is a very different conclusion from previous studies. It is highly unstable and cannot consistently re-produce its reported competitive performance.
    - 4) Within each PEFT method, reducing the size of trainable parameters is likely to yield better stability (but not necessary to yield better or poorer performance).
    - 5) The stability of PEFT methods is proportional to the scale of training data and the number of training iterations.
    - IMO, the Appendix A is a good short intro to PEFT methods.

Ding, N., Qin, Y., Yang, G., Wei, F., Yang, Z., Su, Y., ...Sun, M. (2022). Delta Tuning: A Comprehensive Study of Parameter Efficient Methods for Pre-trained Language Models. arXiv, 2203.06904. Retrieved from https://arxiv.org/abs/2203.06904v2

    - We propose the term "delta tuning" for parameter efficient tuning, since it only fine-tunes a small portion of the model parameters. We review and divide existing delta tuning methods into three groups: addition-based (introduce extra trainable paramters), specification-based (specify certain parameters become trainable, including learnable specifications), and reparameterization-based methods (reparameterize existing parameters to a parameter-efficient form by transformation) (fig. 4).
    - Based on the intrinsic low dimension in a large pre-trained language model, we show that delta tuning is essentially a subspace optimization method with respect to the solution space or functional space. We also interpret delta tuning as seeking optimal controllers for PLMs, and propose an optimal control framework that unifies different delta tuning approaches.
    - We develop OpenDelta toolkit to efficiently and flexibly implement delta tuning on pre-trained LMs.
    - We perform comprehensive performance comparison of existing delta tuning methods on 100 NLP tasks.
    - We find that existing delta tuning methods are still no match for the conventional fine-tuning either in performance or convergence. Among delta tuning methods, no single algorithm predominantly outperforms the others. Combining multiple delta tuning methods is more effective than a single method under most cases, despite that the optimal combination may vary for different PLM backbones, downstream tasks, and data scales.
    - We discuss the applications of delta tuning: fast training and shareable checkpoints, multi-task learning, catastrophic forgetting mitigation, and in-batch parallel computing.
    - TODO read

Fu, Z., Yang, H., So, A. M.-C., Lam, W., Bing, L., & Collier, N. (2022). On the Effectiveness of Parameter-Efficient Fine-Tuning. arXiv, 2211.15583. Retrieved from https://arxiv.org/abs/2211.15583v1

    - Many of the parameter-efficient fine-tuning (PEFT) methods are actually sparse fine-tuned models. So, the fine-tuned model can be written as theta_pretrain + M * delta_theta, when M is a matrix with low L0 norm. Many existing methods define a fixed matrix M (not optimizing it). Based on different strategies for choosing M, the methods can be divided into three categories: random approaches (Mixout), rule-based approaches (BitFit, MagPruning, Adapter, and LoRA), and projection-based approaches (DiffPruning, ChildPruning).
    - We show that sparsity implies a regularization of the original model. Then, we prove that if a model is a sparse fine-tuned model, the model stability can benefit from the sparsity. We also show that sparsity contributes to reducing the generalization error.
    - It stillremains a problem of how to choose the tunable parameters. We propose a novel Second-order Approximation Method (SAM) for this. Using this method, we directly get the optimal solution for the parameter mask M.

Träuble, F., Goyal, A., Rahaman, N., Mozer, M., Kawaguchi, K., Bengio, Y., & Schölkopf, B. (2022). Discrete Key-Value Bottleneck. arXiv, 2207.11240. Retrieved from https://arxiv.org/abs/2207.11240v3

    - We tackle the problem of catastrophic forgetting during fine-tuning
    - We propose an architecture built upon a discrete bottleneck of separate and learnable key-value codes
    - The encoded input is used to select the nearest keys, and the corresponding values are fed to the decoder
    - This enables localized and context-dependent model updates
    - It allows learning under distribution shifts and reduces the complexity of the hypothesis class
    - We evaluate on class-incremental learning scenarios wide variety of pre-trained models, outperforming baselines

Li, M., Gururangan, S., Dettmers, T., Lewis, M., Althoff, T., Smith, N. A., & Zettlemoyer, L. (2022). Branch-Train-Merge: Embarrassingly Parallel Training of Expert Language Models. arXiv, 2208.03306. Retrieved from https://arxiv.org/abs/2208.03306v1

    - We present Branch-Train-Merge (BTM), a communication-efficient algorithm for parallel training of LLMs. BTM learns a set of independent EXPERT LMs (ELMs), each specialized to a different textual domain, such as scientific or legal text, eliminating the massive multi-node synchronization. These ELMs can be simply averaged to collapse back to a single LM ("ELM Forest") for efficient inference.
    - To update data coverage, new ELMs can be learned by branching from (mixtures of) ELMs in the current set, training on the new domain, and then merging the resulting model back into the set for future use.
    - When evaluated in- and out-of-domain, ELMFORESTs trained with BTM outperform monolithic GPTstyle transformer LMs (GPT-LMs) and a previous domain-specialized MoE baseline across a range of computational budgets.
    - We show that these results require expert domain specialization; LM ensembles with random data splits do not perform well.
    - We present a study of scaling BTM into a corpus of 64 domains. This suggests more aggressive parallelism could be used to efficiently train LLMs in future work.

Lu, P., Kobyzev, I., Rezagholizadeh, M., Rashid, A., Ghodsi, A., & Langlais, P. (2022). Improving Generalization of Pre-trained Language Models via Stochastic Weight Averaging. arXiv, 2212.05956. Retrieved from https://arxiv.org/abs/2212.05956v2

    - We adapt Stochastic Weight Averaging (SWA) to fine-tuning pre-trained LMs on text classification, question answering, and generation. This improves the generalization without extra computation cost.
    - We outline several practical recipes that are different from the ones in CV. We noticed that a high constant LR performes better than a cyclical LR. Then, since the number of epochs is small, we average over steps instead of epochs. We also start averaging earlier: after 50% of the training instead of 75% in the original method. This could be because out initial model is not randomly initialized, but already pre-trained.
    - We show that SWA able to outperform the SOTA knowledge distillation methods for compact models.

Ramé, A., Kirchmeyer, M., Rahier, T., Rakotomamonjy, A., Gallinari, P., & Cord, M. (2022). Diverse Weight Averaging for Out-of-Distribution Generalization. arXiv, 2205.09739. Retrieved from https://arxiv.org/abs/2205.09739v2

    - We propose Diverse Weight Averaging (DiWA), which averages weights obtained from several independent training runs that share the same initialization. Weights are ranked in decreasing order of validation accuracy and sequentially added only if they improve DiWA’s validation accuracy.
    - We average runs with different hyperparameters. Indeed, weights obtained from extremely different hyperparameters may not be linearly connectable, we thus use the mild search space.
    - DiWA consistently improves the SOTA on DomainBed benchmark without inference overhead.
    - We show the limitations of this flatness-based analysis for weight averaging. Instead, we motivate the need for diversity by a new bias-variance-covariance locality decomposition of the expected error. It contains four terms: 1) the bias that increases under shift in label posterior distributions (i.e., correlation shift); 2) the variance that we show increases under shift in input marginal distributions (i.e., diversity shift); 3) the covariance that decreases when models are diverse; 4) a locality condition on the weights of averaged models. This decomposition highlights that WA succeeds when the variance term dominates, which we show occurs when the marginal distribution changes at test time. So, our motivation is that individually trained models are more diverse than those obtained along a single run.
    - IMO this is the same method as in the concurrent work "Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time".

Wortsman, M., Gururangan, S., Li, S., Farhadi, A., Schmidt, L., Rabbat, M., & Morcos, A. S. (2022). lo-fi: distributed fine-tuning without communication. arXiv, 2210.11948. Retrieved from https://arxiv.org/abs/2210.11948v2

    - We investigate completely local fine-tuning ("lo-fi"), when in distributed setting each node fine-tunes independently without any communication. Then, the weights are averaged across nodes.
    - We show that lo-fi matches accuracy in-distribution and improves OOD accuracy when fine-tuning DeiT-base and DeiT-large on ImageNet, compared to the baseline which communicates gradients at each step.
    - Also lo-fi matches the baseline’s performance when fine-tuning OPT language models (up to 1.3B parameters) on Common Crawl.
    - Lo-fi is a natural extension of previous work: it is just a variant of model soup (see "Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time") and is similar but simpler than branch-train-merge (see "Branch-Train-Merge: Embarrassingly Parallel Training of Expert Language Models").

Yuan, H., Yuan, Z., Tan, C., Huang, F., & Huang, S. (2022). HyPe: Better Pre-trained Language Model Fine-tuning with Hidden Representation Perturbation. arXiv, 2212.08853. Retrieved from https://arxiv.org/abs/2212.08853v2

    - Problem: fine-tuning LMs poses problems such as over-fitting or representation collapse
    - We propose HyPe fine-tuning technique: we inject random noise between transformer layers
    - This outperforms vanilla fine-tuning with negligible computational overheads

Du, Y., & Nguyen, D. (2023). Measuring the Instability of Fine-Tuning. arXiv, 2302.07778. Retrieved from https://arxiv.org/abs/2302.07778v2

    - Problem: fine-tuning pre-trained LMs is sensitive to random seed, especially on small datasets
    - Most previous studies measure only the standard deviation of performance scores between runs
    - We analyze six other measures quantifying instability at different levels of granularity
    - We reassess existing instability mitigation methods

Goyle, V., Krishnaswamy, P., Ravikumar, K. G., Chattopadhyay, U., & Goyle, K. (2023). Neural Machine Translation For Low Resource Languages. arXiv, 2304.07869. Retrieved from https://arxiv.org/abs/2304.07869v2

    - In low-resouce NMT, there is no comprehensive survey done to identify what approaches work well
    - We take mBART as a baseline
    - We apply techniques like transfer learning, back translation and focal loss, and verify their effect

Gueta, A., Venezian, E., Raffel, C., Slonim, N., Katz, Y., & Choshen, L. (2023). Knowledge is a Region in Weight Space for Fine-tuned Language Models. arXiv, 2302.04863. Retrieved from https://arxiv.org/abs/2302.04863v3

    - We find that different finetuning runs from the same pre-trained checkpoint on the same data tend to converge on similar points in weight space rather than dispersed points. These finetuned models define a connected basin of low loss, and every point within it performs well: points on the line between any pair of these models attain similar or even lower loss. These regions are relatively tight: extrapolating (rather than interpolating) can quickly produce a poorly performing model. We replicate the findings in all the granularities: models finetuned 1) on the same task and 2) on general language tasks also cluster together (fig. 1).
    - The best models may lie closer to region's center, while models found via finetuning typically lie on the boundaries of these regions and are often suboptimal. Practically, one can average models from the same region and cautiously expect the resulting model to perform better. This already has been used in practice (see "Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time").
    - For each target dataset, we choose the centroid of all models excluding ones finetuned on this dataset, and use this point as initialization for fine-tuning on the target dataset, using BitFit method. This results in a better performing model than starting from a pretrained model in 9 cases, performs on par in 2 cases, and underperforming in 1 case. In a few-shot scenario limiting the training examples to 1K, this effect is even stronger.
    - IMO, this is also very similar to DiWA ("Diverse Weight Averaging for Out-of-Distribution Generalization").
    - IMO, the another dimension should be the number of training data, what is not covered here. The models trained on 1) small data, 2) large data, 3) infinite data should have different properties.

Lv, K., Yang, Y., Liu, T., Gao, Q., Guo, Q., & Qiu, X. (2023). Full Parameter Fine-tuning for Large Language Models with Limited Resources. arXiv, 2306.09782. Retrieved from https://arxiv.org/abs/2306.09782v2

    - Can we use SGD (less memory consuming) instead of Adam to optimize LLMs? How to further reduce memory usage?
    - Prior works often discuss three challenges of SGD: 1) large curvature loss surface, 2) local optimum, and 3) saddle points. However, there are empirical results and theoretical analyses supporting that the parameter space of large LMs is quite smooth. Also, a local optimum is often a good enough solution for fine-tuning, because we do not want to push the model to a global optimum, which may be far away from pre-trained weights. Also we argue that the initial point of LLMs should be in a valley, not a saddle point. We show theoretically that utilizing SGD optimizer over a smooth loss surface could imply a larger batch size. A larger batch size indicates stronger training stability, so we believe that finetuning process of LLMs with the SGD optimizer is stable. This also explains why SGD failed on small models but worked for large models.
    - We propose a new optimizer, LOw-Memory Optimization (LOMO), which fuses the gradient computation and the parameter update in one step (fig. 1). The key idea is to update the parameter immediately when its gradient is computed so that we do not store gradient tensor in memory. The parameter update process is still equivalent to SGD.
    - LOMO optimizer leads to a substantial reduction in memory footprint from 102.20GB to 14.58GB, when compared to the AdamW optimizer, and from 51.99GB to 14.58GB, when compared to SGD, when training the LLaMA-7B model.
    - The training of a 7B model with 512×8 tokens in one batch demands a substantial amount of memory for activations. LOMO is compatible with activation checkpointing: the memory footprint can be reduced from 45.61GB to 1.79GB.
    - Our approach cannot directly compute the gradient norm because we update parameters along with the backpropagation. Instead, we propose to clipp gradient tensors by its values rather than the norm. The main concern of clipping by values is that truncating some gradient elements could change the direction of the gradient tensor. We show that clipping by values performs well for medium and small LRs (less than 1e-3).
    - If we still want to clip by gradient norm, then we may perform an additional pass to compute and accumulate each parameter’s gradient norm. The memory usage leaves unchanged but sacrifices the speed.
    - To mitigate the degradation in precision in mixed-precision training, we utilize dynamic loss scaling and transition certain computations to full precision. This is crucial in preventing underflows during FP16 training, magnifying the loss with a specific factor prior to the backward pass and diminishing the gradients by the same factor. LOMO dynamically adjusts this scaling factor.
    - LOMO generally outperforms LoRA in most experiments. However, in some cases, LOMO performs worse than LoRA. One possible reason is the relatively small training set we use, which may not be sufficient for full-parameter fine-tuning of large models. Actually, these two methods are not conflicting or mutually exclusive. We find that LOMO consistently enhances the performance of LoRA on subset of the SuperGLUE benchmark.

Lv, K., Yan, H., Guo, Q., Lv, H., & Qiu, X. (2023). AdaLomo: Low-memory Optimization with Adaptive Learning Rate. arXiv, 2310.10195. Retrieved from https://arxiv.org/abs/2310.10195v3

    - While the recently proposed LOMO reduces memory footprint, it is sensitive to hyper-parameters and fails to match AdamW in fine-tuning LLMs.
    - We found that, compared to momentum, the adaptive LR is more critical for bridging the gap. Through experiments we found that second-order moment estimation has a significantly greater impact on convergence than the first-order moment estimation (fig. 1, when "variance" is the second-order moment). It is particularly effective for handling sparse data, allowing parameters that are infrequently updated to receive larger update steps. So, we decided to incorporate a second-order moment estimation and discard the first-order moment. Furthermore, the second-order moment in the optimizer’s state has been proven to be decomposable or compressible to reduce memory usage (in Adafactor optimizer).
    - We introduce AdaLomo: the low-memory optimization with adaptive LR, that achieves results on par with AdamW. For memory efficiency, we applied non-negative matrix factorization to the second-order moment, inspired by Adafactor.
    - We use a grouped gradient normalization instead of global gradient normalization, which nearly doubles the training speed of AdaLomo (since it no more requires two backward passes) while maintaining its performance.

Pecher, B., Srba, I., & Bielikova, M. (2023). On the Effects of Randomness on Stability of Learning with Limited Labelled Data: A Systematic Literature Review. arXiv, 2312.01082. Retrieved from https://arxiv.org/abs/2312.01082v1

    - Learning with limited labelled data (such as few-shot learning, in-context learning, meta-learning or transfer learning) is known to be excessively sensitive to the effects of non-determinism in the training process (such as random initialisation of model parameters, or data shuffling). This leads to large variance in results across training runs with the same setup and hyperparameters.
    - This can unintentionally, but unfortunately also intentionally (by cherry-picking), create an imaginary perception of research progress. For example, changing only order of data in in-context learning can lead the model from SOTA predictions to random guesses. Or choosing different set of adaptation data in meta-learning can lead to difference between minimum and maximum performance being up to 90%.
    - We provide a comprehensive overview of 134 papers addressing these effects.
    - We view the stability to be a subset of the reproducibility of the model (which is also impacted by other factors such as selective reporting, biases, data and source code availability etc.).
    - Most existing works about learning instability focus on random seeds, model initialisation, sample choice and order. However, data split is considered the most important factor, while being investigated only in few papers. Another mostly ignored factors are the non-deterministic implementation and hardware, and choice of which samples are labelled.
    - A significant drawback for almost all investigation papers is that the effects of randomness are not taken into consideration when searching for hyper-parameters. The papers often do not even mention how the hyper-parameters are chosen.
    - Almost all papers investigate the effects of randomness on in-distribution settings.
    - Many papers come to contradictory findings in the same setting. Some papers observe decrease in variance when the size of the model increases, while others found no obvious connection between size of model and the amount of observed variance. When it comes to datasets, some papers observed consistent results, with the same factor being most significant across different datasets, while others observed different factors being important on different datasets. The most consistent finding from the investigation is that increasing the number of samples reduces variance. Although there are also papers that find that increasing number of samples leads to increase in variance in specific settings.
    - The observed origins of randomness can be summarised as follows: 1) Poor choice of samples (sec 6.1), 2) Overfitting that causes catastrophic forgetting, and models to focus on shallow features (sec 6.2), 3) Under-specification, where multiple local minima with same performance are present in training data, which are not consistent with testing data (sec 6.3), 4) Highly unbalanced output label distribution (sec 6.4), 5) Optimisation problems caused by poor choices in hyperparameters etc (sec 6.5).
    - The only way to deal with any origin of randomness is to aggregate over enough repeats of the training runs (called multi-run mitigation strategy). In existing papers, it is mostly used in combination with the data split randomness factor, as it can be viewed as a modification of the cross-validation.
    - Comparisons are sensitive to the number of training runs, as the distribution of results can be objectively determined only with larger number of runs.
    - TODO need to read this paper in more details.

Talman, A., Celikkanat, H., Virpioja, S., Heinonen, M., & Tiedemann, J. (2023). Uncertainty-Aware Natural Language Inference with Stochastic Weight Averaging. arXiv, 2304.04726. Retrieved from https://arxiv.org/abs/2304.04726v1

    - We show that stochastic weight averaging (SWA) and SWA-gaussian provides consistent and clear improvement in NLU tasks (ross-dataset testing on SNLI and MNLI sets). We do not notice a clear advantage of SWAG over SWA.
    - Some datasets include disagreement between annotators. This usually arises from differences in understanding the task, the given information and personal experience. We demonstrate that the prediction uncertainty in SWAG for individual instances very well follows human annotation confusion.
    - This points to the use of SWAG in an active learning scenario, where annotation noise can be identified using a well calibrated prediction model.
    - For future work we consider making use of multiple annotations also during training.

Uppaal, R., Hu, J., & Li, Y. (2023). Is Fine-tuning Needed? Pre-trained Language Models Are Near Perfect for Out-of-Domain Detection. arXiv, 2305.13282. Retrieved from https://arxiv.org/abs/2305.13282v1

    - We are considering the problem of OOD detection for some specific (textual) distribution of interest.
    - We use pre-trained LM that maps text into an embedding. We measure the relative distances of samples in the embedding space. We hypothesize that embeddings of ID samples are closer to each other than the OOD sample embeddings.
    - We demonstrate strong performance of pre-trained LMs on this task, comparing to LMs fine-tuned on ID data. We conclude that fine-tuning on ID data (as was proposed earlier) is not needed for distance-based OOD detection, since our zero-shot OOD detection performs better. We show that the performance of distance-based OOD detection declines over the course of fine-tuning across all objectives we tried, despite the increase in ID classification accuracy.
    - Early stopping can be a promising solution to achieve tradeoff between OOD detection and ID classification performance.
    - To better understand the strong performance, we further show that pre-trained models display strongly separated domain clusters, which leads to the efficacy of distance-based OOD detection.

Pecher, B., Cegin, J., Belanec, R., Simko, J., Srba, I., & Bielikova, M. (2024). Fighting Randomness with Randomness: Mitigating Optimisation Instability of Fine-Tuning using Delayed Ensemble and Noisy Interpolation. arXiv, 2406.12471. Retrieved from https://arxiv.org/abs/2406.12471v1

    - Fine-tuning is sensitive to random initialisation, data shuffling, randomness induced by dropout. This leads to large performance variation (fig. 1, "without mitigation"). To deal with the fine-tuning instability, researchers propose adding noise to the model parameters or ensebling. Ensembling significantly reduce the deviation in results, but also significantly increase the computation costs. Adding noise also perform well, improving generalisability and overall performance, but not necessarily reducing the instability.
    - We propose Delayed Ensemble with Noisy Interpolation (DENI) - a novel strategy for mitigating the randomness sensitivity of finetuning, that comprises two main components: 1) Delayed Ensemble; and 2) Noisy Interpolation.
    - In Delayed Ensemble, we initialise and train a single model and create an ensemble from it by injecting Gaussian noise (eq. 1, fig. 2 E->F). After this, the obtained models are trained for the remainder of the steps, and the prediction is obtained using hard voting.
    - In Noisy Interpolation, we create and interpolates the ensemble multiple times during training (fig. 2, B and C4). We do it in the same way as in Delayed Ensemble, but in later stages of the training, the introduced noise is smaller. These noisy models are then trained (fig. 2, C1->C2) and are aggregated together (fig. 2, C3, D) using uniform interpolation (averaging?). The aggregated model is then trained (fig. 2, C3->D), and the process is repeated (C4 etc.).  In comparison to simple ensembling, the method requires only a fraction of computational resources.
    - DENI provides benefits across 3 representative PEFT methods (fig. 1, "With DENI Mitigation").

## Transformers and RNN

Geva, M., Schuster, R., Berant, J., & Levy, O. (2020). Transformer Feed-Forward Layers Are Key-Value Memories. arXiv, 2012.14913. Retrieved from https://arxiv.org/abs/2012.14913v2

    - Feedforward layers in transformer-based language models operate as key-value memories
    - Each key correlates with a set of human-interpretable input patterns, such as n-grams or semantic topic
    - Each value can induce a distribution over the output vocabulary
    - This distribution correlates with the next-token distribution in the upper layers of the model
    - The learned patterns are human-interpretable
    - Lower layers tend to capture shallow patterns, while upper layers learn more semantic ones
    - The output of a feed-forward layer is a composition of its memories

Peng, H., Pappas, N., Yogatama, D., Schwartz, R., Smith, N. A., & Kong, L. (2021). Random Feature Attention. arXiv, 2103.02143. Retrieved from https://arxiv.org/abs/2103.02143v2

Irie, K., Csordás, R., & Schmidhuber, J. (2022). The Dual Form of Neural Networks Revisited: Connecting Test Time Predictions to Training Patterns via Spotlights of Attention. arXiv, 2202.05798. Retrieved from https://arxiv.org/abs/2202.05798v2

    - It is known that linear layers in NNs trained by GD can be expressed as a key/value/query-attention operation
    - It stores training datapoints and outputs unnormalised dot attention over the entire training experience
    - (none of the mathematical results we’ll discuss is novel)
    - This dual formulation allows for visualising attention weights over all training patterns, given a test input
    - This is not easy: the memory storage requirement forces us to conduct experiments with small datasets
    - Our analysis is not applicable to models which are already trained
    - We experiment on image classification (single-task, multi-task, continual learning) and language modeling
    - We observe many interesting patterns in various scenarios

Peng, B., Alcaide, E., Anthony, Q., Albalak, A., Arcadinho, S., Biderman, S., ...Zhu, R.-J. (2023). RWKV: Reinventing RNNs for the Transformer Era. arXiv, 2305.13048. Retrieved from https://arxiv.org/abs/2305.13048v2

    - We propose a novel model architecture, Receptance Weighted Key Value (RWKV)
    - It leverages a linear attention mechanism
    - This allows us to formulate the model as either a Transformer or an RNN
    - During training it parallelizes computations
    - During inference it maintains constant computational and memory complexity
    - We scale our models as large as 14 billion parameters (the largest dense RNN ever trained)
    - RWKV performs on par with similarly sized Transformers

Sun, Y., Dong, L., Huang, S., Ma, S., Xia, Y., Xue, J., ...Wei, F. (2023). Retentive Network: A Successor to Transformer for Large Language Models. arXiv, 2307.08621. Retrieved from https://arxiv.org/abs/2307.08621v4

    - We propose RetNet to achieve training parallelism, good performance, and low inference cost
    - The inference cost of RetNet is length-invariant
    - Our retention mechanism has three representations: parallel, recurrent, and chunkwise recurrent
    - The parallel representation empowers training parallelism to utilize GPU devices fully
    - The recurrent representation enables efficient O(1) inference in terms of memory and computation
    - The chunkwise recurrent representation can perform efficient long-sequence modeling
    - RetNet is consistently competitive in terms of bothscaling curves and in-context learning

Gu, A., & Dao, T. (2023). Mamba: Linear-Time Sequence Modeling with Selective State Spaces. arXiv, 2312.00752. Retrieved from https://arxiv.org/abs/2312.00752v2

Waleffe, R., Byeon, W., Riach, D., Norick, B., Korthikanti, V., Dao, T., ...Catanzaro, B. (2024). An Empirical Study of Mamba-based Language Models. arXiv, 2406.07887. Retrieved from https://arxiv.org/abs/2406.07887v1

Anthony, Q., Tokpanov, Y., Glorioso, P., & Millidge, B. (2024). BlackMamba: Mixture of Experts for State-Space Models. arXiv, 2402.01771. Retrieved from https://arxiv.org/abs/2402.01771v1

## OOD

#optimization Ben-Tal, A., Ghaoui, L. E., & Nemirovski, A. (2009). Robust Optimization. . doi: 10.1515/9781400831050 https://www.researchgate.net/publication/258222788_Robust_Optimization

    - We present the Robust Optimization (RO) paradigm, primarily for Uncertain Linear, Conic Quadratic, and Semidefinite Programming; Part I, perhaps with chapter 4 skipped, can be used as a stand-alone graduatelevel textbook on Robust Linear Programming, or as a base of a semester-long graduate course on Robust Optimization; in Part II the main concepts are extended to uncertain Convex Programming problems in the conic form; Part III is devoted to Robust Multi-Stage Decision Making (Robust Dynamic Programming, Adjustable Robust Counterparts); Parts II and III can be read independently of each other; a short, single-chapter Part IV presents three realistic examples, worked out in full detail, of application of the RO methodology
    - The data of real-world optimization problems are not known exactly (measurement/estimation errors etc.); even a small uncertainty in the data can make the nominal optimal solution to the problem completely meaningless from a practical viewpoint
    - Robust Optimization is a methodology to generate a robust solution, that is immunized against the effect of data uncertainty; "immunized against uncertainty" means worst-case-oriented: a solution should remain feasible for the constraints, whatever the realization of the data; let us call such a solution robust feasible
    - In Stochastic Optimization (SO), the uncertain numerical data are assumed to be random (their distribution is only partially known); both Robust and Stochastic Optimization are aimed at answering the same question (albeit in different settings), the question of building an uncertainty-immunized solution to an optimization problem with uncertain data; stochastic and Robust Optimization are complementary approaches for handling data uncertainty in Optimization, each having its own advantages and drawbacks
    - In chapter 12, we present some applications of Robust Optimization in the context of Machine Learning; for example, in robust SVM we need to trade-off the training loss and the amount of robustness with respect to spherical perturbations of the data points; one approach is to minimize the worst-case training loss under perturbations of the data points; to our knowledge, the results in this chapter are new

#domain_adaptation Ganin, Ya., & Lempitsky, V. (2014). Unsupervised Domain Adaptation by Backpropagation. arXiv, 1409.7495. Retrieved from https://arxiv.org/abs/1409.7495v2

    - We propose gradient reversal: a new approach to domain adaptation that can be implemented with little effort
    - Our approach promotes learning useful features that are invariant between the domains
    - On top of the feature extractor, we optimize two discriminative classifiers: 1) the class label predictor, 2) the domain classifier that discriminates between the source and the target domains during training
    - We reverse the gradients on the second classifier to maximize the loss on it; this ecourages domain-invariant features
    - The only non-standard component is a gradient reversal layer that leaves the input unchanged during forward propagation and reverses the gradient by multiplying it by a negative scalar during the backpropagation

Ganin, Ya., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette, F., ...Lempitsky, V. (2015). Domain-Adversarial Training of Neural Networks. arXiv, 1505.07818. Retrieved from https://arxiv.org/abs/1505.07818v4

#ood_generalization Li, D., Yang, Y., Song, Y.-Z., & Hospedales, T. M. (2017). Deeper, Broader and Artier Domain Generalization. arXiv, 1710.03077. Retrieved from https://arxiv.org/abs/1710.03077v1

    - A Domain Generalization (DG) problem is to learn domain-agnostic model from multiple training domains, and apply to unseen domain
    - Motivation: target domains may have sparse data for training
    - We develop a low-rank parameterized CNN model based on Tucker decomposition to reduce number of parameters
    - Every weight tensor for a given domain is the sum of a domain specific tensor and a domain agnostic tensor
    - We develop a DG benchmark covering photo, sketch, cartoon and painting domains
    - Our method outperforms existing DG alternatives

Li, D., Yang, Y., Song, Y.-Z., & Hospedales, T. (2018). Learning to Generalize: Meta-Learning for Domain Generalization. AAAI, 32(1). doi: 10.1609/aaai.v32i1.11596

#domain_adaptation Peng, X., Bai, Q., Xia, X., Huang, Z., Saenko, K., & Wang, B. (2018). Moment Matching for Multi-Source Domain Adaptation. arXiv, 1812.01754. Retrieved from https://arxiv.org/abs/1812.01754v4

    - A Domain Adaptation problem is to transfer knowledge learned from multiple labeled source domains to an unlabeled target domain
    - We collect a DomainNet dataset with 6 domains, ~0.6 million images, 345 categories
    - We propose Moment Matching for Multi-Source Domain Adaptation (M3SDA)
    - It consists of three components: feature extractor, moment matching component, and classifiers
    - Moment matching component dynamically aligns moments of domains feature distributions (fig. 3)

#ood_generalization Arjovsky, M., Bottou, L., Gulrajani, I., & Lopez-Paz, D. (2019). Invariant Risk Minimization. arXiv, 1907.02893. Retrieved from https://arxiv.org/abs/1907.02893v3

    - Problem: training data may contain spurious correlations (we do not expect them to hold in the future)
    - We assume that the training data is collected into distinct, separate environments
    - We want to learn correlations that are stable across training environments
    - The idea: to learn invariances across environments, find a data representation such that the optimal classifier on top of that representation matches for all environments
    - We propose Invariant Risk Minimization (IRM): an idealistic objective, whic is bi-leveled and challenging optimization problem
    - We then simplify it into the IRMv1, when gradient norm penalty is used to measure the optimality of the dummy classifier at each environment, and a regularization coefficient is used to balance predictive power (as in ERM) and the invariance
    - However, there are many subsequent works that show that IRM not always work as intended
  
Training data may contain spurious correlations (we do not expect them to hold in the future). he authors assume that the training data is collected into distinct, separate environments, and we want to learn correlations that are stable across training environments. The authors argue that to learn invariances across environments, one need to find a data representation such that the optimal classifier on top of that representation matches for all environments.
  
The authors propose an Invariant Risk Minimization (IRM) objective for models consisting of deep feature extractor and shallow classifier (see eq. IRM and definition 3 on page 5). The IRM is an idealistic objective, which is bi-leveled and challenging optimization problem.
  
The authors then simplify it into the IRMv1: practical version, when we optimize a feature extractor, and a fixed classifier (or regressor) sits on top of it, and we want this classifier to be optimal in all environments. The gradient norm penalty is used to measure the optimality of the dummy classifier at each environment, and a regularization coefficient is used to balance predictive power (as in ERM) and the invariance. See implementation details in sec. 3.2.
  
In the section 3, the authors explain in details how did they come from IRM to IRMv1. For example, if we do not fix the classifier, it will be possible that the classifier's gradient will tend to zero just when we multiply the classifier's weights by a large value, and the deep features by a small value. Thus, IRMv1 objctive will not work in this case (see sec. 3.1.3). If the model is overparametrized, we can choose any W and switch from "classifier matches for all environments" to "classifier is W for all environments", so the optimization only happens over Phi. This suggests that  w = (1, 0, ..., 0) would be a valid choice for our fixed classifier, as noted in 3.1.4. However, in 3.1.5 the authors use w = (1, 1, ..., 1).
  
As an "Example 1", the authors consider the following model: X1 <- N(0, sigma^2); Y <- X1 + N(0, sigma^2); X2 <- Y + N(0, 1). Sigma may vary between environments, so the only robust way to predict is Y_pred = X1. The correlation coefficient between X2 and Y is high when sigma is high, so it also varies between environments and is a spurious correlation, that we don't want to rely on. Also, X2 is not a causal predictor. How can we learn the invariant, causal regression? he authors argue that both ERM and robust learning objective fails here, and robust learning turns out to be equivalent to minimizing a weighted average of environment training errors (proposition 2). However, IRM is successful in this task (see fig. 3 for experiments with synthetic structural models).
  
Instead of "classifier optimality" objective in ERM, we could enforce a stronger condition that the joint distribution of Phi(x) (see eq. for IRM, IRMv1) matches for all environments (as in Domain-Adversarial Training). However, the authors argue that the distribution of the true causal features can change across environments, so such techniques matching feature distributions sometimes attempt to enforce the wrong type of invariance. So it is better to learn correlations invariant across training environments, which is what IRM does.
  
The authors also discuss the connection from invariance to causality and OOD generalization and develop a generalization theory for IRM (sec. 4) based on the assumption that the data from all the environments share the same underlying Structural Equation Model. The authors promote invariance as the main feature of causation, while not being pioneers in doing so.
  
IMO, if deep encoder may detect each environment (if each environment has some distinct properties), then the encoder may incorporate "logic gates" to return different features for each environment. For example, Phi(x)[0] may be already the final prediction, and other elements be constant zero, so that w = (1, 1, ..., 1) is optimal. Also, some elements Phi(x)[i] may contain spurious features for some environments and be constant zero for others, so that w[i] = 1 is optimal. This would be possible if the encoder may differentiate between environments, including the case when some spurious feature is present only in one environment (so this environment may be detected by the presence of this feature). The authors do note that given a flexible Phi, it is possible to write any invariant predictor as 1.0 · Φ, however they say this in the context of the question: how restrictive is linear W? So, the above problem is not discussed by the authors. The authors' experiments also do not cover situations when some spurious feature occurs only in one environment, or, more generally, it is easy for model to differentiate between environments.
  
IMO, the another problem is that the authors do not describe how in principle they did early stopping: on train, test, or a separate val environments.

#ood_generalization Sagawa, S., Koh, P. W., Hashimoto, T. B., & Liang, P. (2019). Distributionally Robust Neural Networks for Group Shifts: On the Importance of Regularization for Worst-Case Generalization. arXiv, 1911.08731. Retrieved from https://arxiv.org/abs/1911.08731v2

    - To avoid learning models that rely on spurious correlations it is a good idea to train models to minimize the worst-case loss over training groups which are selected being aware of spurious correlations. This is called Distributionally robust optimization (DRO), in short this is ERM while increasing the importance of domains with larger errors
    - The problem: if a model achieves zero training loss, then it is optimal on both DRO and ERM objectives; so in the vanishing training loss regime DRO models are not better than ERM
    - We study DRO on NLU, facial attribute recognition, bird recognition
    - We show that strongly-regularized group DRO models that do not attain vanishing training loss can significantly outperform both regularized and unregularized ERM models
    - So, regularization is especially important for good worst-case performance
    - We introduce a new stochastic optimizer for group DRO that is stable and scales to large models and datasets

Zhang, L., Wang, X., Yang, D., Sanford, T., Harmon, S., Turkbey, B., ...Xu, Z. (2019). When Unseen Domain Generalization is Unnecessary? Rethinking Data Augmentation. arXiv, 1906.03347. Retrieved from https://arxiv.org/abs/1906.03347v2

Bellot, A., & van der Schaar, M. (2020). Accounting for Unobserved Confounding in Domain Generalization. arXiv, 2007.10653. Retrieved from https://arxiv.org/abs/2007.10653v6

Choe, Y. J., Ham, J., & Park, K. (2020). An Empirical Study of Invariant Risk Minimization. arXiv, 2004.05007. Retrieved from https://arxiv.org/abs/2004.05007v2

#ood_generalization Gulrajani, I., & Lopez-Paz, D. (2020). In Search of Lost Domain Generalization. arXiv, 2007.01434. Retrieved from https://arxiv.org/abs/2007.01434v1

    - We point at inconsistencies in experimental conditions for testing various Domain Generalization methods
    - We realize that model selection is non-trivial for domain generalization tasks
    - A domain generalization algorithm should be responsible for specifying a model selection method
    - A model selection policy should have no access to the test domain
    - We implement DomainBed, a testbed for domain generalization
    - It includes 7 multi-domain datasets, 9 baseline algorithms, and 3 model selection criteria
    - Carefully designed ERM shows SOTA performance across all datasets

Currently domain generalization methods are evaluated under different datasets and model selection criteria. We aim to compare them in realistic settings. The goal of domain generalization is out-of-distribution generalization: learning a predictor able to perform well at some unseen test domain (we characterize each domain by a dataset containing iid examples). Compared to domain adaptation, in domain generalization even unlabeled data from target domain is not accessible. Domain generalization is the best approximation to real prediction problems.

Because we lack access to a validation set identically distributed to the test data, model/hyperparameters selection in domain generalization is not as straightforward as in supervised learning. It is not a part of experimental design, but a learning problem at least as hard as fitting the model. Therefore, a domain generalization algorithm without a strategy to choose its hyperparameters remains incomplete.

We propose a DomainBed framework for reproducible experimentation in domain generalization. It contains 7 multi-domain datasets, 9 baseline algorithms (with hyperparameter search spaces for all algorithms), and 3 model selection criteria:

1. Training-domain validation set (random train-val split of training domains)
2. Leave-one-domain-out cross-validation (each time holding one of the training domains for validation)
3. Test-domain validation set (oracle).

The latter means that we actually can evaluate **incomplete** algorithms by considering an oracle model selection method, where we select hyperparameters on the test domain, but researchers should disclaim any oracle-selection results as such and specify policies to limit access to the test domain, otherwise we could just train on such test domain data using supervised learning. We propose to allow 20 queries per algorithm, one query per choice of hyperparameters in our random search. This means that we do not allow early stopping based on the validation set. Recall
that we do not consider this a valid benchmarking methodology. Oracle-selection results can be either optimistic, because we access the test distribution, or pessimistic, because the query limit reduces the number of considered hyperparameter combinations. As an alternative to limiting the number of queries, we could borrow tools from differential privacy tools that add Laplace noise to the accuracy statistic of the algorithm.

The datasets in DomainBed differ in many ways. In Rotated MNIST and Colored MNIST, domains are synthetically constructed such that we know what features will generalize a priori, so using too much prior knowledge (e.g. by augmenting with rotations) is off-limits. Also, in datasets other than Colored MNIST, the domain changes the distribution of images, but likely bears no information about the true image-to-label mapping. On the other hand, in Colored MNIST, the domain influences the true image-to-label mapping, biasing algorithms that try to estimate this function directly.

The initial release of DOMAINBED includes implementations of the following algorithms:

1. Empirical Risk Minimization (ERM) minimizes the sum of errors across domains and examples
2. Group Distributionally Robust Optimization (DRO) performs ERM while increasing the importance of domains with larger errors
3. Inter-domain Mixup performs ERM on linear interpolations of examples from random pairs of domains and their labels
4. Meta-Learning for Domain Generalization (MLDG) leverages MAML to meta-learn how to generalize across domains
5. Domain-Adversarial Neural Networks (DANN) employ an adversarial network to match feature distributions
6. Class-conditional DANN (C-DANN) is a variant of DANN matching the conditional distributions p(X|y) across domains, for all labels y
7. CORAL matches the mean and covariance of feature distributions
8. MMD matches the maximum mean discrepancy of feature distributions
9. Invariant Risk Minimization (IRM) learns a feature representation such that the optimal linear classifier on top of that representation matches across domains

Our implementation choices:

1. We opt to finetune large ResNet-50 models for all datasets except Rotated MNIST and Colored MNIST
2. In domain generalization augmentations can approximate some of the variations between domains. So, for MNIST datasets, we use no data augmentation. For other datasets we use a standard set of augmentations (see sec. 4.3)
3. Randomness arising from model selection is often ignored. For instance, does method A outperform method B only because random search for A got lucky? We therefore repeat our entire study three times making every random choice anew: hyperparameters, weight initializations, and dataset splits. Every number we report is a mean over these repetitions.

We conclude that ERM (empirical risk minimization) achieves SOTA performance in domain generalization, when equipped with modern NN architectures and data augmentation techniques. Given any model selection criterion, no method improves upon the average performance of ERM in more than one point. Getting substantial domain generalization improvements over ERM on these datasets proved challenging. We suspect this is because a bigger network architecture (ResNet-50), strong data augmentations, careful hyperparameter tuning (and using the full training data to construct our domains in Rotated MNIST). These results suggest standard techniques to improve in-distribution generalization are very effective at improving OOD generalization. Moreover, it was recently shown that strong data augmentation can improve OOD generalization while not impacting in-distribution generalization. We think that if the practitioner is lucky and performs the data augmentations that cancel the spurious correlations varying from domain to domain, then OOD performance should improve.

We also observe that model selection with a training domain validation set outperforms leave-one-domain-out cross-validation across multiple datasets and algorithms. The stronger performance of oracle-selection (+2%) suggests possible headroom for improvement.

Our concerns:
1. Why do we assume a neural network should be able to classify cartoons, given only photorealistic training data? Is the out-of-distribution performance of modern ERM implementations as good as it gets? Or is it simply as bad as every other alternative? How can we establish upper-bounds on what performance is achievable out-of-distribution via domain generalization techniques?
2. Some of the datasets do not reflect realistic situations. In reality, if one wanted to classify cartoons, the easiest option would be to collect a small labeled dataset of cartoons. Should we consider more realistic, impactful tasks? Attractive alternatives include medical  imaging in different hospitals and self-driving cars in different cities.
3. Each algorithm assumes a different (untestable) type of invariance across domains. Therefore, the performance of a domain generalization
algorithm depends on the problem at hand.

IMO, in this work there is no discussions about pre-training. It is known that pre-trained models have wide range of abilities, and fine-tuning may reduce OOD robustness. So, all results may differ for different amount of pre-training.

Krueger, D., Caballero, E., Jacobsen, J.-H., Zhang, A., Binas, J., Zhang, D., ...Courville, A. (2020). Out-of-Distribution Generalization via Risk Extrapolation (REx). arXiv, 2003.00688. Retrieved from https://arxiv.org/abs/2003.00688v5

    - We point out that shifts at test time may be more extreme in magnitude than shifts between training domains
    - We formulate domain generalization (or OOD generalization) as optimizing the worst-case performance over a perturbation set of possible test domains
    - Our method minimax Risk Extrapolation is an extension of distributionally robust optimization (fig. 1)
    - It can uncover invariant relationships between X and Y (maintained across all domains)
    - We demonstrate that REx solves invariant prediction tasks where IRM fails due to covariate shift

Peyrard, M., Ghotra, S. S., Josifoski, M., Agarwal, V., Patra, B., Carignan, D., ...West, R. (2021). Invariant Language Modeling. arXiv, 2110.08413. Retrieved from https://arxiv.org/abs/2110.08413v2

Rosenfeld, E., Ravikumar, P., & Risteski, A. (2020). The Risks of Invariant Risk Minimization. arXiv, 2010.05761. Retrieved from https://arxiv.org/abs/2010.05761v2

#domain_adaptation Xu, M., Zhang, J., Ni, B., Li, T., Wang, C., Tian, Q., & Zhang, W. (2020). Adversarial Domain Adaptation with Domain Mixup. AAAI, 34(04), 6502–6509. doi: 10.1609/aaai.v34i04.6123

    - To achieve adversarial domain adaptation, earlier it was proposed to use a domain discriminator; in order to fool this domain discriminator, the extracted features should be domain-invariant
    - We present a new method: adversarial domain adaptation with domain mixup (DM-ADA)
    - The authors perform ERM on linear interpolations of examples from random pairs of domains and their labels

Rame, A., Dancette, C., & Cord, M. (2021). Fishr: Invariant Gradient Variances for Out-of-Distribution Generalization. arXiv, 2109.02934. Retrieved from https://arxiv.org/abs/2109.02934v3

    - We propose Fishr regularization for OOD generalization
    - Fishr enforces that domain-level variances of gradients are matched across training domains
    - This is based on the relations between the gradient covariance, the Fisher Information and the Hessian of the loss
    - Fishr achieves SOTA on the DomainBed benchmark

Wortsman, M., Ilharco, G., Kim, J. W., Li, M., Kornblith, S., Roelofs, R., ...Schmidt, L. (2021). Robust fine-tuning of zero-shot models. arXiv, 2109.01903. Retrieved from https://arxiv.org/abs/2109.01903v3

    - Problem: fine-tuning CLIP or ALIGN reduce robustness to distribution shifts
    - We propose to ensemble the weights of the zero-shot and fine-tuned models (WiSE-FT)
    - We test it on a set of image distribution shifts
    - This comes at no additional computational cost during fine-tuning or inference

Arpit, D., Wang, H., Zhou, Y., & Xiong, C. (2021). Ensemble of Averages: Improving Model Selection and Boosting Performance in Domain Generalization. arXiv, 2110.10832. Retrieved from https://arxiv.org/abs/2110.10832v4

Cha, J., Chun, S., Lee, K., Cho, H.-C., Park, S., Lee, Y., & Park, S. (2021). SWAD: Domain Generalization by Seeking Flat Minima. arXiv, 2102.08604. Retrieved from https://arxiv.org/abs/2102.08604v4

Xiao, C., & Madhyastha, P. (2021). A call for better unit testing for invariant risk minimisation. arXiv, 2106.03234. Retrieved from https://arxiv.org/abs/2106.03234v1

Shrestha, R., Kafle, K., & Kanan, C. (2021). Are Bias Mitigation Techniques for Deep Learning Effective? arXiv, 2104.00170. Retrieved from https://arxiv.org/abs/2104.00170v4

Zhang, D., Ahuja, K., Xu, Y., Wang, Y., & Courville, A. (2021). Can Subnetwork Structure be the Key to Out-of-Distribution Generalization? arXiv, 2106.02890. Retrieved from https://arxiv.org/abs/2106.02890v1

Zhang, J., & Bottou, L. (2022). Learning useful representations for shifting tasks and distributions. arXiv, 2212.07346. Retrieved from https://arxiv.org/abs/2212.07346v3

Lin, Y., Dong, H., Wang, H., & Zhang, T. (2022). Bayesian Invariant Risk Minimization. Retrieved from https://openaccess.thecvf.com/content/CVPR2022/html/Lin_Bayesian_Invariant_Risk_Minimization_CVPR_2022_paper.html

Naganuma, H., Hataya, R., & Mitliagkas, I. (2023). An Empirical Study of Pre-trained Model Selection for Out-of-Distribution Generalization and Calibration. arXiv, 2307.08187. Retrieved from https://arxiv.org/abs/2307.08187v3

Alesiani, F., Yu, S., & Niepert, M. (2023). Continual Invariant Risk Minimization. arXiv, 2310.13977. Retrieved from https://arxiv.org/abs/2310.13977v1

Lai, Z.-R., & Wang, W. (2024). Invariant Risk Minimization Is A Total Variation Model. arXiv, 2405.01389. Retrieved from https://arxiv.org/abs/2405.01389v5

Zhang, Y., Sharma, P., Ram, P., Hong, M., Varshney, K., & Liu, S. (2023). What Is Missing in IRM Training and Evaluation? Challenges and Solutions. arXiv, 2303.02343. Retrieved from https://arxiv.org/abs/2303.02343v1

Zhang, J., & Bottou, L. (2024). Fine-tuning with Very Large Dropout. arXiv, 2403.00946. Retrieved from https://arxiv.org/abs/2403.00946v1

    - It is known that ensemble techniques involving multiple data distributions gives richer representations 
    - We investigate the use of very high dropout rates instead of ensembles to obtain such rich representations
    - Training a DNN from scratch using such dropout rates is virtually impossible
    - However, fine-tuning under such conditions is possible
    - It achieves out-of-distribution performances that exceed those of both ensembles and weight averaging
    - We also provide interesting insights on representations and intrinsically linear nature of fine-tuning

Yu, H., Liu, J., Zhang, X., Wu, J., & Cui, P. (2024). A Survey on Evaluation of Out-of-Distribution Generalization. arXiv, 2403.01874. Retrieved from https://arxiv.org/abs/2403.01874v1

## Continual LLM

Zhang, H., Gui, L., Zhai, Y., Wang, H., Lei, Y., & Xu, R. (2023). COPF: Continual Learning Human Preference through Optimal Policy Fitting. arXiv, 2310.15694. Retrieved from https://arxiv.org/abs/2310.15694v4

Sun, Y., Wang, S., Li, Y., Feng, S., Tian, H., Wu, H., & Wang, H. (2019). ERNIE 2.0: A Continual Pre-training Framework for Language Understanding. arXiv, 1907.12412. Retrieved from https://arxiv.org/abs/1907.12412v2

Jang, J., Ye, S., Yang, S., Shin, J., Han, J., Kim, G., ...Seo, M. (2021). Towards Continual Knowledge Learning of Language Models. arXiv, 2110.03215. Retrieved from https://arxiv.org/abs/2110.03215v4

Don-Yehiya, S., Venezian, E., Raffel, C., Slonim, N., Katz, Y., & Choshen, L. (2022). ColD Fusion: Collaborative Descent for Distributed Multitask Finetuning. arXiv, 2212.01378. Retrieved from https://arxiv.org/abs/2212.01378v2

Zhang, Z., Fang, M., Chen, L., Namazi-Rad, M.-R., & Wang, J. (2023). How Do Large Language Models Capture the Ever-changing World Knowledge? A Review of Recent Advances. arXiv, 2310.07343. Retrieved from https://arxiv.org/abs/2310.07343v1

## Other TODO

Jelodar, H., Wang, Y., Yuan, C., Feng, X., Jiang, X., Li, Y., & Zhao, L. (2017). Latent Dirichlet Allocation (LDA) and Topic modeling: models, applications, a survey. arXiv, 1711.04305. Retrieved from https://arxiv.org/abs/1711.04305v2

Babu, G. J., Banks, D., Cho, H., Han, D., Sang, H., & Wang, S. (2021). A Statistician Teaches Deep Learning. arXiv, 2102.01194. Retrieved from https://arxiv.org/abs/2102.01194v2

Musgrave, K., Belongie, S., & Lim, S.-N. (2020). A Metric Learning Reality Check. arXiv, 2003.08505. Retrieved from https://arxiv.org/abs/2003.08505v3

Dudzik, A., & Veličković, P. (2022). Graph Neural Networks are Dynamic Programmers. arXiv, 2203.15544. Retrieved from https://arxiv.org/abs/2203.15544v3

Ibarz, B., Kurin, V., Papamakarios, G., Nikiforou, K., Bennani, M., Csordás, R., ...Veličković, P. (2022). A Generalist Neural Algorithmic Learner. arXiv, 2209.11142. Retrieved from https://arxiv.org/abs/2209.11142v2

Rodionov, G., & Prokhorenkova, L. (2023). Neural Algorithmic Reasoning Without Intermediate Supervision. arXiv, 2306.13411. Retrieved from https://arxiv.org/abs/2306.13411v2

Rodionov, G., & Prokhorenkova, L. (2024). Discrete Neural Algorithmic Reasoning. arXiv, 2402.11628. Retrieved from https://arxiv.org/abs/2402.11628v1

Men, X., Xu, M., Zhang, Q., Wang, B., Lin, H., Lu, Y., ...Chen, W. (2024). ShortGPT: Layers in Large Language Models are More Redundant Than You Expect. arXiv, 2403.03853. Retrieved from https://arxiv.org/abs/2403.03853v2

## ASR

Fiscus, J. G. . A post-processing system to yield reduced word error rates: Recognizer Output Voting Error Reduction (ROVER). 1997 IEEE Workshop on Automatic Speech Recognition and Understanding Proceedings. IEEE. doi: 10.1109/ASRU.1997.659110

Gemello, R., Mana, F., Scanzio, S., Laface, P., & De Mori, R. (2007). Linear hidden transformations for adaptation of hybrid ANN/HMM models. Speech Communication, 49(10), 827–835. doi: 10.1016/j.specom.2006.11.005

Graves, A., Fernández, S., Gomez, F., & Schmidhuber, J. (2006). Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks. ACM Other conferences. Association for Computing Machinery. Retrieved from https://www.cs.toronto.edu/~graves/icml_2006.pdf

    - We propose a method called Connectionist Temporal Classification (CTC) for training RNNs on tasks where real-valued unsegmented input streams are annotated with strings of discrete labels (e.g. handwriting recognition, speech recognition, gesture recognition)
    - Terminology: we refer to the task of labelling unsegmented data sequences as temporal classification, and independent labelling of each time-step, or frame, of the input sequence as framewise classification
    - Let V be a vocabulary of N classes (for ASR, classes may be letters, including a class for "space").
    - Let E be the extended vocabulary of N+1 classes, including "blank" class that means no prediction at the current frame (not the same as space character).
    - CTC network performs per-frame classification to these N+1 classes. So, given K frames, model outputs probabilities of shape (K, N+1). This allows to calculate the probability of any sequence from E^T.
    - We then define a many-to-one mapping B from E^T to to the set of any-length (up to length T) sequences of vocabulary V (denote as V^star).
    - The mapping B: E^T -> V^star works like this: we first remove all repetitions, and then remove all blank tokens (not vice versa).
    - Let the probability of any sequence S from V^star be the sum  probabilities of all X in E^T such that B(X) = S. The probability of X is the product of probabilities of all its tokens.
    - Given predicted probabilities of shape (K, N+1), during inference we want to find the most probable sequence from V^star, and during training we want to calculate the probability of the target sequence from V^star. Both tasks are intractable.
    - We can approximate inference with greedy ("best path") search (taking argmax and then applying B mapping), or, better, by our proposed Prefix search decoding (fig. 2). It relies on the observation that the outputs of a trained CTC network tend to form a series of spikes separated by strongly predicted blanks (fig. 1), and hence we choose boundary points where the probability of observing a blank label is above a certain threshold. We then calculate the most probable labelling for each section individually. In practice, prefix search generally outperforms greedy search.
    - ...TODO
    - TODO https://pytorch.org/audio/stable/tutorials/ctc_forced_alignment_api_tutorial.html
    - On TIMIT dataset, CTC outperformed both a baseline HMM (hidden Markov Model) recogniser and an HMM-RNN hybrid with the same RNN architecture.
    - A key difference between CTC and HMM is that CTC does not explicitly segment its input sequences. Determining the segmentation is a waste of modelling. For tasks where segmentation is required it would seem problematic to use CTC (however, CTC is suitable where approximate segmentation is sufficient).
    - Further we intend to pursue an hierarchy of temporal classifiers, where the labellings at one level (e.g. letters) become inputs for the labellings at the next (e.g. words).

Sak, H., Saraclar, M., & Güngör, T. (2010). On-the-fly lattice rescoring for real-time automatic speech recognition. ResearchGate, 2450–2453. doi: 10.21437/Interspeech.2010-532

    - We propose an algorithmic framework for rescoring lattices (a DAG of transcription hypotheses) on-the-fly.
	- TODO

Graves, A. (2012). Sequence Transduction with Recurrent Neural Networks. arXiv, 1211.3711. Retrieved from https://arxiv.org/abs/1211.3711v1

    - We propose RNN transducer for sequence-to-sequence tasks (such as phoneme recognition).
	- Let we have input vectors X and output vectors Y. In our case, we assume that the output space is discrete, output vectors are one-hot vectors, and the output vocabulary is extended with additional null token (meaning "output nothing"). However the method can be readily extended to continuous output spaces. As in CTC, we assume that the location of the null symbols determines an alignment between the input and output sequences, and we refer to the sequences containing null symbols as "alignments". Given X, the RNN transducer defines a conditional distribution over all possible alighments. This distribution is then collapsed onto the distribution over output tokens by removing null tokens.
	- The prediction network G (sec. 2.1) is a RNN that accepts and returns one-hot vectors from the extended alphabet with null symbol (the latter is represented as zero vector on input space, and as an additional element in output space, so the input vectors have length K, and the output vectors have length K+1). The prediction RNN can be either a one-layer RNN (eq. 2-3) or LSTM (eq. 4-8). The prediction network attempts to model each element of Y given the previous ones; it is therefore similar to a standard next-step-prediction RNN, only with the added option of making null predictions.
	- The transcription network F (sec. 2.2) is a bidirectional RNN. Bidirectionality is preferred because each output vector depends on the whole input sequence; however we have not tested to what extent this impacts performance. For a task with K output labels, the output layer of the transcription network is size K + 1, just like the prediction network. So, the transcription network is similar to a CTC RNN.
	- Let the transcription network outputs T vectors (sec 2.3). Let us consider any single vector from these vectors. Also, consider any single vector from the transcription network outputs. We can add them element-wise and them perform softmax (eq. 12, 13) to yield the output distribution over output alphabet and the null token. The probability of the null token can be interpreted as the need to shift T by 1.
	- In fig. 1, the set of possible paths from the bottom left to the terminal node in the top right corresponds to the complete set of alignments between x and y. Therefore all possible input-output alignments are assigned a probability, the sum of which is the total probability P(y|x) of the output sequence given the input sequence. A similar lattice could be drawn for any finite y. So, we have defined a distribution over all possible output sequences, given a single input sequence. A naive calculation of P(y|x) from the lattice would be intractable. However an efficient forward-backward algorithm is described in sec. 2.4.
	- At test time, we employ a fixed-width beam search through the tree of output sequences.
	- The improved version of RNN Transducer that includes the "output network", is described in "Speech Recognition with Deep Recurrent Neural Networks". An excerpt from this paper is below: CTC defines a distribution over phoneme sequences that depends only on the acoustic input sequence x. It is therefore an acoustic-only model. A recent augmentation, known as an RNN transducer combines a CTC-like network with a separate RNN that predicts each phoneme given the previous ones, thereby yielding a jointly trained acoustic and language model (IMO the authors mean that CTC network does not model the language autoregressively, but i think it models the language in the same way as BERT does). Whereas CTC determines an output distribution at every input timestep, an RNN transducer determines a separate token distribution for every combination of input timestep t and output timestep u, that covers the K phonemes plus null token. Intuitively the network "decides" what to output depending both on where it is in the input sequence and the outputs it has already emitted. For a length U target sequence z, the complete set of TU decisions jointly determines a distribution over all possible alignments between x and z, which can then be integrated out with a forward-backward algorithm to determine log P(z|x).
	- For decoding see https://www.youtube.com/watch?v=dgsDIuJLoJU
	- IMO, the latter variant is similar to RNN encoder-decoder with attention, where the decoder-to-encoder attention is reduced to the following fixed scheme: at step t we look only at the encoder step i, and we can produce either any token without changing i, or a blank token with adding one to i.

Hinton, G., Deng, L., Yu, D., Dahl, G. E., Mohamed, A.-r., Jaitly, N., ...Kingsbury, B. (2012). Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups. IEEE Signal Process. Mag., 29(6), 82–97. doi: 10.1109/MSP.2012.2205597

Veselý, K., Karafiát, M., Grézl, F., Janda, M., & Egorova, E. . The language-independent bottleneck features. 2012 IEEE Spoken Language Technology Workshop (SLT). IEEE. doi: 10.1109/SLT.2012.6424246

Yao, K., Yu, D., Seide, F., Su, H., Deng, L., & Gong, Y. . Adaptation of context-dependent deep neural networks for automatic speech recognition. 2012 IEEE Spoken Language Technology Workshop (SLT). IEEE. doi: 10.1109/SLT.2012.6424251

Graves, A., Mohamed, A.-r., & Hinton, G. (2013). Speech Recognition with Deep Recurrent Neural Networks. arXiv, 1303.5778. Retrieved from https://arxiv.org/abs/1303.5778v1

    - We inversigate deep LSTMs for ASR. We also present an enhancement to a recently introduced RNN transducer: an additional "output network".
	- In the original formulation P(token|audio, prev_tokens) was defined by taking an "acoustic" distribution P(token|audio) from the CTC network, a "linguistic" distribution P(token|prev_tokens) from the prediction network, then multiplying the two together and renormalising.
	- We propose to instead feed the hidden activations of both networks into a separate feedforward output network, whose outputs are then normalised with a softmax function to yield P(token|audio, prev_tokens). This allows a richer set of possibilities for combining linguistic and acoustic information and appears to lead to better generalisation: the number of deletion errors encountered during decoding is reduced.
	- RNN transducers appear to work better when initialised with the weights of a pretrained CTC network and a pretrained next-step prediction network.
	- In this work we pretrain the prediction network on the phonetic transcriptions of the audio training data; however for large-scale applications it would make more sense to pretrain on a separate text corpus.
	- At test time, we exploit the same beam search as the transducer, with the modification that the output label probabilities P(token|audio, prev_tokens) do not depend on the prev_tokens.
	- Two regularisers were used in this paper: early stopping and weight noise (the addition of Gaussian noise to the network weights during training).

Heigold, G., Vanhoucke, V., Senior, A., Nguyen, P., Ranzato, M., Devin, M., & Dean, J. . Multilingual acoustic models using distributed deep neural networks. 2013 IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE. doi: 10.1109/ICASSP.2013.6639348

Huang, J.-T., Li, J., Yu, D., Deng, L., & Gong, Y. . Cross-language knowledge transfer using multilingual deep neural network with shared hidden layers. 2013 IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE. doi: 10.1109/ICASSP.2013.6639081 https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/DNN-MultiLingual-ICASSP2013.pdf

    - We propose a shared-hidden-layer multilingual DNN for speech recognition, in which the hidden layers are made common across many languages while the softmax layers are made language dependent.
    - Our model outperforms models trained using only the language specific data.
    - Our model effectively transfers to new languages, even from different families of the pre-training languages.

Peng, F., Roy, S., Shahshahani, B., & Beaufays, F. . Search results based N-best hypothesis rescoring with maximum entropy classification. 2013 IEEE Workshop on Automatic Speech Recognition and Understanding. IEEE. doi: 10.1109/ASRU.2013.6707767

    - In our ASR application, misrecognizing part of a movie name or an app name results in a poor user experience (such as "I say" instead of "Ice Age").
	- We propose to rerank the N-best speech recognition hypotheses using search results. We design custom features obtailable by search results: number of results, top result title match, score for TV show tc. (sec. 3.2). On inference, for each ASR hypothesis we obtain these search features, and they serve as inputs for our trained classifier, that outputs score. Then we rerank the N-best hypotheses based on their scores.

Thomas, S., Seltzer, M. L., Church, K., & Hermansky, H. . Deep neural network features and semi-supervised training for low resource speech recognition. 2013 IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE. doi: 10.1109/ICASSP.2013.6638959

Yu, D., Seltzer, M. L., Li, J., Huang, J.-T., & Seide, F. (2013). Feature Learning in Deep Neural Networks - Studies on Speech Recognition Tasks. arXiv, 1301.3605. Retrieved from https://arxiv.org/abs/1301.3605v3

Graff, D., Walker, K., Strassel, S., Ma, X., Jones, K. S., & Sawyer, A. (2014). The RATS Collection: Supporting HLT Research with Degraded Audio Data. ACL Anthology, 1970–1977. Retrieved from https://aclanthology.org/L14-1089

    - We introduce the RATS data collection that was designed to cover a diverse range of radio conditions. In the conditions of interest, the signal-to-noise ratio (SNR) often falls below 10dB.
    - The dataset includes four corpora, one for each of the RATS research tasks (Speech Activity Detection, Language Identification, Speaker Identification and Key Word Spotting), comprising clean source audio, the corresponding sets of 8 transceiver channels, and all channel-aligned annotations.

Graves, A., & Jaitly, N. (2014). Towards End-To-End Speech Recognition with Recurrent Neural Networks. International Conference on Machine Learning. PMLR. Retrieved from https://proceedings.mlr.press/v32/graves14.html

    - In ASR, modelling language separately from sound is perhaps the most justifiable departure from end-to-end learning, since it is easier to learn linguistic dependencies from text than speech. Nonetheless, with the advent of speech corpora containing tens of thousands of hours of labelled data, it may be possible to learn the LM directly from the transcripts.
	- We present and end-to-end ASR model that consists of the deep bidirectional LSTM and the CTC output layer. Such a model has been applied to character-level speech recognition before, however, the relatively shallow architecture used in that work did not deliver compelling results.
	- We propose the Expected Transcription Loss to train the network to directly optimise the WER, and, more general, to optimise the expected value of an arbitrary loss function L defined over output transcriptions. To do this, we use Monte-Carlo sampling to approximate both L and its gradient, and prove that the gradient is unbiased. In our sampling, we use the same (sampled) CTC alignment, so that the loss variance largely cancels out (which is crucial when optimising with stochastic gradient estimates).
	- For example, if the sampled alignment yields the character transcription "WTRD ERROR RATE", the gradient would encourage outputs changing the second output label to "O", discourage outputs making changes to the other two words and be close to zero everywhere else.
	- The vast majority of alignments drawn from a randomly initialised network will give completely wrong transcriptions, and there will therefore be little chance of altering the loss by modifying a single output. We therefore recommend that expected loss minimisation is used to retrain a network already trained with CTC, rather than applied from the start.

Narayanan, A., & Wang, D. (2014). Investigation of Speech Separation as a Front-End for Noise Robust Speech Recognition. IEEE/ACM Trans. Audio Speech Lang. Process., 22(4), 826–835. doi: 10.1109/TASLP.2014.2305833

Chan, W., Jaitly, N., Le, Q. V., & Vinyals, O. (2015). Listen, Attend and Spell. arXiv, 1508.01211. Retrieved from https://arxiv.org/abs/1508.01211v2

    - We present Listen, Attend and Spell (LAS): a seq-to-seq model for ASR (fig. 1) that consists of an encoder RNN, which is named the listener, and a decoder RNN, which is named the speller.
	- Key to our approach is the fact that we use a pyramidal RNN model for the listener, which reduces the number of time steps that the attention model has to extract relevant information from.
    - Rare and OOV words are handled automatically, since the model outputs characters (instead of words).
    - The speller produces character sequences without making any independence assumptions between the characters, that is the key improvement of LAS over previous end-to-end CTC models. For example, for the phrase “triple a” the model produces both “triple a” and “aaa” in the top beams. A model like CTC may have trouble producing such diverse transcripts for the same utterance because of conditional independence assumptions between frames.
    - Without the attention mechanism, the model overfits the training data significantly - it memorizes the training transcripts without paying attention to the acoustics.
	- During infarence, the ground truth is missing and the predictions can suffer because the model was not trained to be resilient to feeding in bad predictions at some time steps. To ameliorate this effect, we use a trick that was proposed in "Scheduled sampling for sequence prediction with recurrent neural networks": during training, instead of always feeding in the ground truth transcript for next step prediction, we sometimes sample from our previous character distribution and use that as the inputs in the next step predictions. We do not use a schedule and simply use a constant sampling rate of 10% right from the start of training.
	- We attempted to use the phonemes as a joint objective target, but found no improvements. We also attempted to pretrain the Listen function with context independent or context dependent phonemes generated from a conventional GMM-HMM system, but found no improvements.
	- Decoding is performed with a simple left-to-right beam search. A dictionary can optionally be added to constrain the search space to valid words, however we found that this was not necessary since the model learns to spell real words almost all the time.
	- We have vast quantities of text data, compared to the amount of transcribed speech utterances. We can use language models trained on text corpora alone similar to conventional speech systems. To do so we can rescore our beams with the LM. We find that our model has a small bias for shorter utterances so we normalize our probabilities by the number of characters in the hypothesis and combine it with a LM probability (eq. 16), where LM weight can be determined by a held-out validation set.

Chung, J., Kastner, K., Dinh, L., Goel, K., Courville, A. C., & Bengio, Y. (2015). A Recurrent Latent Variable Model for Sequential Data. Advances in Neural Information Processing Systems, 28. Retrieved from https://proceedings.neurips.cc/paper/2015/hash/b618c3210e934362ac261db280128c22-Abstract.html

Panayotov, V., Chen, G., Povey, D., & Khudanpur, S. (2015). Librispeech: An ASR corpus based on public domain audio books. IEEE International Conference on Acoustics, Speech, and Signal Processing. Retrieved from https://www.danielpovey.com/files/2015_icassp_librispeech.pdf

    - We introduce the LibriSpeech corpus for ASR that is derived from audiobooks and is a part of the LibriVox project
    - It contains 1000 hours of speech sampled at 16 kHz
    - We automatically aligned the audio recordings with the corresponding texts, and split them into short segments
    - We tried to exclude segments of audio that might not correspond exactly with the aligned text
    - Models trained with our corpus do better on the standard Wall Street Journal (WSJ) test sets than models built on WSJ itself

Bahdanau, D., Chorowski, J., Serdyuk, D., Brakel, P., & Bengio, Y. (2015). End-to-End Attention-based Large Vocabulary Speech Recognition. arXiv, 1508.04395. Retrieved from https://arxiv.org/abs/1508.04395v2

    - We propose a new approach to ASR, when alignment between the input features and the desired character sequence is learned automatically by an attention mechanism built into the RNN.
    - Training on long sequences can be made feasible by limiting the area explored by the attention to a range of most promising locations ("windowing"). This reduces the total training complexity from quadratic to linear.
    - We introduce a recurrent architecture that successively reduces source sequence length by pooling frames neighboring in time.
    - Integrating an n-gram language model into the decoding process yields recognition accuracies similar to other HMM-free RNN-based approaches.

Collobert, R., Puhrsch, C., & Synnaeve, G. (2016). Wav2Letter: an End-to-End ConvNet-based Speech Recognition System. arXiv, 1609.03193. Retrieved from https://arxiv.org/abs/1609.03193v2

    - We present Wav2Letter: a model for end-to-end speech recognition that consists of CNN acoustic model 
    - We train with an Auto Segmentation Criterion (ASG), an our alternative to the Connectionist Temporal Classification (CTC) (fig. 2, 3). In contrast to CTC, 1) there are no blank labels, and therefore t produces a much simpler graph, 2) we have un-normalized scores on the nodes, 3) we apply global normalization instead of per-frame normalization. We show that ASG can be faster than CTC, and as accurate.
    - We perform inference with a simple beam search decoder with beam threholding, histogram pruning and language model smearing
    - Our model shows competitive results in WER on the Librispeech corpus with MFCC features, and promising results from raw waveform.

Fraccaro, M., Sønderby, S. K., Paquet, U., & Winther, O. (2016). Sequential Neural Models with Stochastic Layers. Advances in Neural Information Processing Systems, 29. Retrieved from https://proceedings.neurips.cc/paper/2016/hash/208e43f0e45c4c78cafadb83d2888cb6-Abstract.html

Hori, T., Hori, C., Watanabe, S., & Hershey, J. R. . Minimum word error training of long short-term memory recurrent neural network language models for speech recognition. 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE. doi: 10.1109/ICASSP.2016.7472827

    - In RNN language models for ASR error correction, training with cross entropy loss not necessary minimizes WER.
	- We propose a minimum word error (MWE) criterion. It minimizes the expected WER using a set of N-best lists generated by a speech recognizer. In MWE loss (eq. 10), we have K training samples, each sample consists of N best hypotheses from acoustic model. The left part is edit distance between hypothesis and the true transcription, and the right part is the hypothesis probability from RNN language model. So, minimizing MWE means minimizing probability of hypotheses that are far from the true transcription.

Kim, S., Hori, T., & Watanabe, S. (2016). Joint CTC-Attention based End-to-End Speech Recognition using Multi-task Learning. arXiv, 1609.06773. Retrieved from https://arxiv.org/abs/1609.06773v2

    - In ASR, the attention model has often been shown to improve the performance over CTC, mainly because it explicitly uses the history of the target character without any conditional independence assumptions. However, in realenvironment speech recognition tasks, the model shows poor results because the alignment estimated in the attention mechanism is easily corrupted due to the noise. Another issue is that the model is hard to learn from scratch due to the misalignment on longer input sequences, and therefore a windowing technique is commonly used to limit the area explored by the attention mechanism, but several parameters for windowing need to be determined manually depending on the training data.
    - We propose to use a shared-encoder representation trained by both CTC and attention model objectives simultaneously. Along with improving performance, our framework significantly speeds up learning with fast convergence.

Peddinti, V., Manohar, V., Wang, Y., Povey, D., & Khudanpur, S. (2016). Far-Field ASR Without Parallel Data. ResearchGate, 1996–2000. doi: 10.21437/Interspeech.2016-1475 https://www.danielpovey.com/files/2016_interspeech_ami.pdf

    - When parallel audio recordings (close-talk microphone +  distant microphone) are available, the alignments (matchings between audio features and phonemes) used for training the acoustic models can be generated from close-talk microphone audio recordings to obtain WER improvements. However, far-field audio is usually not accompanied with close-talk microphone recordings.
    - We use the lattice-free maximum mutual information (MMI) objective (not proposed by us), which is tolerant to minor mis-alignment errors (such as shown in fig. 1), which is actual when alignments are generated from distant microphone recordings.
    - We also propose a method to select reliable utterances for training from distant microphone recordings.
    - These methods reduce the performance gap between the ASR systems that are trained using alignments generated from distant and close-talk microphone readings from 8% to 1.5%.
    - IMO, not clear why do we need alignment, while we can train with CTC loss.

Ghahremani, P., Manohar, V., Hadian, H., Povey, D., & Khudanpur, S. . Investigation of transfer learning for ASR using LF-MMI trained neural networks. 2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU). IEEE. doi: 10.1109/ASRU.2017.8268947 https://www.danielpovey.com/files/2017_asru_transfer_learning.pdf

    - We investigate two transfer learning approaches – weight transfer and multi-task training - in ASR, while different domains are different acoustic conditions (for English language), and source domain data is larger than target domain data. A typical weight transfer approach is to first train the model on a large dataset, retain only N layers and add new task-specific adaptation layers over those. In multi-task we share the initial layers across all tasks, and each task has a specific final layer.
    - We found that multi-task training performs better than weight transfer. However weight transfer is still effective compared to the unadapted model, and hence it might be preferable over multi-task training as it does not require re-training.
    - In weight transfer, single-stage training (training target-specific layers) of transferred layers with very small LR is better than 2-stage training by freezing the transfered layers at the 1st stage and fine-tuning them at 2nd stage.
    - Even a model trained on source data for half or quarter the number of epochs is as effective for weight transfer as a fully-trained model.

Ebbers, J., Heymann, J., Drude, L., Glarner, T., Haeb-Umbach, R., & Raj, B. (2017). Hidden Markov Model Variational Autoencoder for Acoustic Unit Discovery. doi: 10.21437/Interspeech.2017-1160

Hori, T., Watanabe, S., & Hershey, J. R. (2017). Joint CTC/attention decoding for end-to-end speech recognition. ACL Anthology, 518–529. doi: 10.18653/v1/P17-1048 https://aclanthology.org/P17-1048.pdf

    - We propose a joint decoding algorithm for end-to-end ASR with a hybrid CTC/attention architecture.
    - Our joint CTC/attention approach combines the CTC and attention-based sequence probabilities in the inference step, as well as the training step.
    - The decoding objective is defined using multiplying text probabilities from CTC and attention (eq. 14). The CTC probability enforces a monotonic alignment that does not allow large jumps or looping of the same frames.
    - We perform one-pass/rescoring joint decoding, in which we compute the probability of each partial hypothesis using CTC and an attention model.
    - This greatly reduces irregular alignments without any heuristic search techniques.

Hsu, W.-N., Zhang, Y., & Glass, J. (2017). Learning Latent Representations for Speech Generation and Transformation. arXiv, 1704.04222. Retrieved from https://arxiv.org/abs/1704.04222v2

Hsu, W.-N., Zhang, Y., & Glass, J. (2017). Unsupervised Learning of Disentangled and Interpretable Representations from Sequential Data. Advances in Neural Information Processing Systems, 30. Retrieved from https://proceedings.neurips.cc/paper/2017/hash/0a0a0c8aaa00ade50f74a3f0ca981ed7-Abstract.html

Kannan, A., Wu, Y., Nguyen, P., Sainath, T. N., Chen, Z., & Prabhavalkar, R. (2017). An analysis of incorporating an external language model into a sequence-to-sequence model. arXiv, 1712.01996. Retrieved from https://arxiv.org/abs/1712.01996v1

	- Recently the end-to-end LAS (Listen, Attend, and Spell) model was proposed for ASR. It  jointly learns an encoder, which serves as an acoustic model, a decoder, which serves as a language model, and an attention mechanism, which learns alignments.
	- Our goal is to explore why the performance of LAS still lags behind a SOTA ASR system with separate acoustic, pronunciation and language models. We propose that one reason for the performance degradation could be that the LAS decoder, that is trained only on transcribed audio-text pairs. In comparison, SOTA LMs are typically trained on a billion words or more.
	- We investigate the impact of training a separate LM on auxiliary text-only data, and incorporating this model as an additional cost term when decoding a LAS model (shallow fusion).
	- We find that RNN LMs are more effective at reducing error than n-gram LMs.
    - On Google Voice Search (which has much more training data than WSJ used in previous studies), we demonstrate that the use of shallow fusion with an neural LM with wordpieces yields a large WER reduction, obviating the need for second-pass rescoring, despite being 70 times smallerthan the second pass LM.

Kim, C., Misra, A., Chin, K., Hughes, T., & Bacchiani, M. (2017). Generation of Large-Scale Simulated Utterances in Virtual Rooms to Train Deep-Neural Networks for Far-Field Speech Recognition in Google Home. ResearchGate, 379–383. doi: 10.21437/Interspeech.2017-1510 https://static.googleusercontent.com/media/research.google.com/ru//pubs/archive/46107.pdf

    - We develop an acoustic room simulator to generate large-scale simulated data for far-field speech recognition.
    - The system simulates millions of different room dimensions, a wide distribution of reverberation time and signal-to-noise ratios, and a range of microphone and sound source locations.
    - The simulator-driven approach is quite effective in obtaining large improvements in real / rerecorded conditions.

Ko, T., Peddinti, V., Povey, D., Seltzer, M. L., & Khudanpur, S. . A study on data augmentation of reverberant speech for robust speech recognition. 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE. doi: 10.1109/ICASSP.2017.7953152

Li, J., Ye, G., Zhao, R., Droppo, J., & Gong, Y. (2017). Acoustic-To-Word Model Without OOV. arXiv, 1711.10136. Retrieved from https://arxiv.org/abs/1711.10136v1

    - Problem: the word-based CTC is a very good end-to-end ASR model, but it maps all the unknown words into OOV
    - We propose a hybrid CTC with both word-based CTC and character-based CTC heads that are synchronized
    - Whenever the ASR model emits an OOV token, we rely on character-based CTC

Prabhavalkar, R., Sainath, T. N., Wu, Y., Nguyen, P., Chen, Z., Chiu, C.-C., & Kannan, A. (2017). Minimum Word Error Rate Training for Attention-based Sequence-to-Sequence Models. arXiv, 1712.01818. Retrieved from https://arxiv.org/abs/1712.01818v1

    - We explore training attention-based seq2seq ASR models to directly minimize expected WER, instead of cross-entropy loss.
	- Our loss function is the expected number of word errors over the training set. We can approximate the expectation using an empirical average over samples drawn from the model. Its gradient can be itself be expressed as an expectation, which allows it to be approximated using samples. So, we approximate the WER expectation using N-best hypotheses decoded from the model using beam-search.

Shannon, M. (2017). Optimizing expected word error rate via sampling for speech recognition. arXiv, 1706.02776. Retrieved from https://arxiv.org/abs/1706.02776v1

    - IN ASR task Minimum Bayes risk (MBR) training have been shown effective in terms of WER. MBR minimizes an expected distance between a reference and a hypothesis. In word-level edit-based MBR, the distance between a reference and a hypothesis is measured as WER (given the prevalence of WER as an evaluation metric). However, this is hard to compute.
	- We show that the gradient of the expected loss optimized by word-level edit-based MBR training may itself be written as an expectation, allowing the gradient to be approximated by sampling.
	- The loss computation is shown on fig. 1. Sample_path is a sample from the model, collapse_path is a function that translates sample to output sequence (for CTC head, it removes all duplicates and blank tokens), get_loss computes edit distance (TODO what is gamma)?
	- IMO, looks similar to "Minimum word error training of long short-term memory recurrent neural network language models for speech recognition" and "Minimum Word Error Rate Training for Attention-based Sequence-to-Sequence Models" and.

Watanabe, S., Hori, T., Kim, S., Hershey, J. R., & Hayashi, T. (2017). Hybrid CTC/Attention Architecture for End-to-End Speech Recognition. IEEE J. Sel. Top. Signal Process., 11(8), 1240–1253. doi: 10.1109/JSTSP.2017.2763455 https://www.merl.com/publications/docs/TR2017-190.pdf

    - We propose MOL: a hybrid CTC/attention end-to-end ASR. During training, we propose a multi-objective learning method by attaching a CTC objective to an attention-based encoder network as a regularization. This greatly reduces the number of irregularly aligned utterances. During decoding, we propose a joint decoding approach, which combines both attention-based and CTC scores in a rescoring/one-pass beam search algorithm to eliminate the irregular alignments.
    - Comparing with attention-only ASR, our model learned the desired alignment in an early training stage. This result indicates that the CTC loss guided the alignment to be monotonic.
    - This paper is a combination of two previous papers from the same authors "Joint CTC-Attention based End-to-End Speech Recognition using Multi-task Learning" and "Joint CTC/attention decoding for end-to-end speech recognition" and extends them by providing more details and experimental discussions.

Chung, Y.-A., & Glass, J. (2018). Speech2Vec: A Sequence-to-Sequence Framework for Learning Word Embeddings from Speech. arXiv, 1803.08976. Retrieved from https://arxiv.org/abs/1803.08976v2

Fukuda, T., Fernandez, R., Rosenberg, A., Thomas, S., Ramabhadran, B., Sorin, A., & Kurata, G. (2018). Data augmentation improves recognition of foreign accented speech. IBM Research. doi: 10.21437/Interspeech.2018-1211 https://www.isca-archive.org/interspeech_2018/fukuda18_interspeech.pdf

    - We reproduce two accents, Latin American and Asian accented English speech with voice transformation (modifying glottal source and vocal tract parameters), noise addition, and speed modification.
    - We find that all augmentations provide improvements in accented ASR, with the largest gains coming from speed modification, then voice transformation and noise addition providing the least improvement.

Glarner, T., Hanebrink, P., Ebbers, J., & Haeb-Umbach, R. (2018). Full Bayesian Hidden Markov Model Variational Autoencoder for Acoustic Unit Discovery. doi: 10.21437/Interspeech.2018-2148

Narayanan, A., Misra, A., Sim, K. C., Pundak, G., Tripathi, A., Elfeky, M., ...Bacchiani, M. (2018). Toward domain-invariant speech recognition via large scale training. arXiv, 1808.05312. Retrieved from https://arxiv.org/abs/1808.05312v1

    - A problem: when ASR is used in conditions that do not match the training domain, performance significantly drops.
    - We combine large scale training data from multiple application domains, obtaining 162K hours of speech, and simulate conditions like background noise, codecs and sample rates.
    - We train a model that is robust to multiple application domains, and variations like codecs and noise, and allows for rapid adaptation for unseen conditions (using as little as 10 hours of data from a new domain - and performing on par with domain specific model trained from scratch using 70 times as much data).

Oord, A. v. d., Li, Y., & Vinyals, O. (2018). Representation Learning with Contrastive Predictive Coding. arXiv, 1807.03748. Retrieved from https://arxiv.org/abs/1807.03748v2

    - We propose a Contrastive Predictive Coding for unsupervised learning and demonstrate its effectiveness on four distinct domains: speech, images, text and reinforcement learning in 3D environments.
    - We stand for unsupervised pre-training, since it learns more general features than supervised pre-training. For example, in ASR pre-training features that are useful to transcribe human speech may be less suited for speaker identification, or music genre prediction. So, ASR pre-trained features will lack certain information. Same for image pre-training.
    - One way for unsupervised learning is to predict future observations (predictive coding).
    - We propose to compress raw data into a latent embedding space and train to predict future in this space (fig. 1) with autoregressive models. We rely on Noise-Contrastive Estimation for the loss function.

Ravanelli, M., & Bengio, Y. . Speaker Recognition from Raw Waveform with SincNet. 2018 IEEE Spoken Language Technology Workshop (SLT). IEEE. doi: 10.1109/SLT.2018.8639585

Snyder, D., Garcia-Romero, D., Sell, G., Povey, D., & Khudanpur, S. . X-Vectors: Robust DNN Embeddings for Speaker Recognition. 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE. doi: 10.1109/ICASSP.2018.8461375

Tjandra, A., Sakti, S., & Nakamura, S. (2018). Machine Speech Chain with One-shot Speaker Adaptation. arXiv, 1803.10525. Retrieved from https://arxiv.org/abs/1803.10525v1

Aksan, E., & Hilliges, O. (2019). STCN: Stochastic Temporal Convolutional Networks. arXiv, 1902.06568. Retrieved from https://arxiv.org/abs/1902.06568v1

Ardila, R., Branson, M., Davis, K., Henretty, M., Kohler, M., Meyer, J., ...Weber, G. (2019). Common Voice: A Massively-Multilingual Speech Corpus. arXiv, 1912.06670. Retrieved from https://arxiv.org/abs/1912.06670v2

    - We present the Common Voice: a massively-multilingual collection of transcribed speech
    - Over 50,000 individuals have participated so far, resulting in 2,500 hours of collected audio
    - Using either the Common Voice website or iPhone app, contributors record their voice by reading sentences displayed on the screen. The recordings are later verified by other contributors.
    - For languages with more than 500,000 Wikipedia articles, text sentences are extracted from Wikipedia using community provided rule-sets per language.

Baevski, A., Auli, M., & Mohamed, A. (2019). Effectiveness of self-supervised pre-training for speech recognition. arXiv, 1911.03912. Retrieved from https://arxiv.org/abs/1911.03912v3

    - We compare different approaches of self-supervised pre-training for speech data
    - As one alternative (fig. 1a), we take "Discrete BERT" ASR model, that consists of vq-wav2vec, which quantizes the Librispeech dataset into 13.5k unique codes, follwed by pre-trained BERT model. Instead of vq-wav2vec we also tried k-means clustering MFCC and FBANK features with 13.5k centroids (to match the vq-wav2vec setup). We directly fine-tune the pre-trained BERT model on transcribed speech data using a CTC loss.
    - As another alternative (fig. 1b), we try "Continuous BERT". MLM cannot be performed with continuous inputs and outputs, as there are no targets to predict in place of the masked tokens. Instead, we classify the masked positive example among a set of negatives with InfoNCE loss. In this case, the inputs to BERT are dense wav2vec features, MFCC or FBANK features.
    - We show that the most effective method is to first learn a discrete vocabulary of the data with vq-wav2vec followed by standard BERT training over these discrete units. This performs much better than directly learning from the continuous audio data. Thus, disentangling acoustic unit discovery from learning the sequential relationship between them enables better representations.

Baevski, A., Schneider, S., & Auli, M. (2019). vq-wav2vec: Self-Supervised Learning of Discrete Speech Representations. arXiv, 1910.05453. Retrieved from https://arxiv.org/abs/1910.05453v3

    - We propose vq-wav2vec, that learns discrete representations of fixed length segments of audio signal by utilizing the wav2vec loss and architecture.
    - To choose the discrete variables, we consider a Gumbel-Softmax quantization or K-means vector quantization. These methods perform relatively comparably.
    - Discretization enables the direct application of algorithms from the NLP community which require discrete inputs. We apply BERT to the quantized representations, and then pass BERT's outputs to any acoustic model such as wav2letter. We improve SOTA on the WSJ and TIMIT benchmarks by leveraging BERT pre-training.
    - We plan to explore self-supervised pre-training algorithms which mask part of the continuous audio input, or fine-tune BERT to output ranscriptions instead of feeding the pre-trained features to an acoustic model.

Baumann, T., Köhn, A., & Hennig, F. (2019). The Spoken Wikipedia Corpus collection: Harvesting, alignment and an application to hyperlistening. Lang. Resources &. Evaluation, 53(2), 303–329. doi: 10.1007/s10579-017-9410-y

    - The Spoken Wikipedia project unites volunteer readers of Wikipedia articles (for for persons who are unable or unwilling to read out of alexia, visual impairment, or because their sight is currently occupied, e.g. while driving). We present our open-source software pipeline that downloads, extracts, normalizes and text–speech aligns the Spoken Wikipedia.
    - We present and analyse, for three languages (de, en, nl), the resulting corpora of read encyclopedic content read by a large variety of speakers.

Chung, Y.-A., Hsu, W.-N., Tang, H., & Glass, J. (2019). An Unsupervised Autoregressive Model for Speech Representation Learning. arXiv, 1904.03240. Retrieved from https://arxiv.org/abs/1904.03240v2

    - We propose Autoregressive Predictive Coding (APC) for unsupervised speech representation learning.
    - We introduce a time shifting factor that asks the model to predict further steps. Our results show that the number of steps to the target frame controls what is learned in the representation. How this hyperparameter is set depends on how the representation is going to be used.
    - APC and CPC (Contrastive Predictive Coding) differ significantly in the type of information the model learns. (the difference is not clearly described in the paper; concurrent works?)

Guo, J., Sainath, T. N., & Weiss, R. J. (2019). A spelling correction model for end-to-end speech recognition. arXiv, 1902.07178. Retrieved from https://arxiv.org/abs/1902.07178v1

    - Usually acoustic models are accompanied with language models, but they are only trained on transcribed audio-text pairs, which leads to performance degradation especially on rare words.
    - When, on the contrary, we try to incorporate an external LM, we still observe that numerous rare word and proper noun errors. We hypothesize that this is because the LM was not trained with objective of correcting errors.
    - We propose the following method: given texts, we synthetically generate audio, run the baseline LAS speech recognizer on it, thus creating a set of textto-text pairs representing an error hypothesis and its corresponding ground truth. Then we train a spelling corrector (SC) model on these text-to-text pairs.
    - Our SC model is based on encoder-decoder bi-directional LSTM with attention.
    - During inference, acoustic model (LAS) with beam search produces N hypotheses with corrsponding log probability scores, and for every hypothesis our SC model can similarly be used to generate M new hypotheses with corresponding log probability scores. Rescoring all N×M candidates with an LM gives a set of LM scores. Finally, we can find the most likely hypothesis.

Kahn, J., Rivière, M., Zheng, W., Kharitonov, E., Xu, Q., Mazaré, P.-E., ...Dupoux, E. (2019). Libri-Light: A Benchmark for ASR with Limited or No Supervision. arXiv, 1912.07875. Retrieved from https://arxiv.org/abs/1912.07875v1

Khurana, S., Joty, S. R., Ali, A., & Glass, J. . A Factorial Deep Markov Model for Unsupervised Disentangled Representation Learning from Speech. ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE. doi: 10.1109/ICASSP.2019.8683131

Li, J., Lavrukhin, V., Ginsburg, B., Leary, R., Kuchaiev, O., Cohen, J. M., ...Gadde, R. T. (2019). Jasper: An End-to-End Convolutional Neural Acoustic Model. arXiv, 1904.03288. Retrieved from https://arxiv.org/abs/1904.03288v3

    - We propose Jasper: an end-to-end CNN that achieves SOTA on LibriSpeech among models without any external training data.
    - We propose a new residual connection topology we call Dense Residual (DR).
    - We use Connectionist Temporal Classification (CTC) loss.
    - To improve training, we further introduce a new layer-wise optimizer called NovoGrad, a variant of the Adam with a smaller memory footprint.

Liu, A. T., Yang, S.-w., Chi, P.-H., Hsu, P.-c., & Lee, H.-y. (2019). Mockingjay: Unsupervised Speech Representation Learning with Deep Bidirectional Transformer Encoders. arXiv, 1910.12638. Retrieved from https://arxiv.org/abs/1910.12638v2

Ma, D., Li, G., Xu, H., & Chng, E. S. . Improving code-switching speech recognition with data augmentation and system combination. 2019 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC). IEEE. doi: 10.1109/APSIPAASC47483.2019.9023316 http://www.apsipa.org/proceedings/2019/pdfs/339.pdf

    - Code-switching speech is defined as speech which contains more than one language within an utterance. Code-switching ASR is challenging due to data sparsity issue, compared with monolingual speech recognition case. Data augmentation is used 1) to alleviate data sparsity issue, and 2) to improve the robustness. One of the simpler kind of noise is additive noise, such as music noise, babble noise, and white noise. Another kind of noise is reverberant noise, which is produced by convolving the original data with the room impulse response (RIR) signal.
    - We are solving code-switching ASR task on the South East Asian Mandarin-English (SEAME) data.
    - We are attempting different types of noise (fig. 1) to the training data. One is reverberant noise data source from http://www.openslr.org/28. It is an entire bunch of Room Impulse Response (RIR) data sets, that are to be convolved with the training audio, generating reverberant noise effect. Another noise category is additive noise, that is obtained from MUSAN data from http://www.openslr.org/17. We first perform noise corruption and then the speech speed perturbation.
    - We note that all the above-mentioned data augmentation methods are only conducted on the training data part (not a dev set).
    - We also perform comprehensive Minimum Bayes Risk-based lattice combination methods, which yield improvements.

Narayanan, A., Prabhavalkar, R., Chiu, C.-C., Rybach, D., Sainath, T. N., & Strohman, T. (2019). Recognizing long-form speech using streaming end-to-end models. arXiv, 1910.11455. Retrieved from https://arxiv.org/abs/1910.11455v1

Neekhara, P., Hussain, S., Pandey, P., Dubnov, S., McAuley, J., & Koushanfar, F. (2019). Universal Adversarial Perturbations for Speech Recognition Systems. arXiv, 1905.03828. Retrieved from https://arxiv.org/abs/1905.03828v2

    - We propose an algorithm to find a single quasi-imperceptible perturbation, which when added to any arbitrary speech signal, will most likely fool  a victim ASR model.
    - The algorithm requires access to the victim’s model architecture and parameters.
    - Attack Success Rate depends on the magnitude of the perturbation (fig. 3).
    - Perturbations generalize to a significant extent across models that are not available during training.

Park, D. S., Chan, W., Zhang, Y., Chiu, C.-C., Zoph, B., Cubuk, E. D., & Le, Q. V. (2019). SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition. arXiv, 1904.08779. Retrieved from https://arxiv.org/abs/1904.08779v3

    - We propose SpecAugment: an augmentation that is applied to the feature inputs of a NN (log mel spectrogram).
    - SpecAugment consists of warping the features, masking blocks of frequency channels, and masking blocks of time steps.
    - SpecAugment greatly improves the performance of ASR networks.

Park, K., & Mulc, T. (2019). CSS10: A Collection of Single Speaker Speech Datasets for 10 Languages. arXiv, 1903.11269. Retrieved from https://arxiv.org/abs/1903.11269v3

    - The existing multi-lingual speech-text datasets have several problems: 1) the Tundra dataset uses only one audiobook per language, 2) The M-AILABS uses multiple speakers in each language, which make it unideal for the single speaker TTS task (namely, generating speech from text in the voice of a single speaker) where more data from a single speaker tends to help model performance.
    - We propose CSS10, a collection of single speaker (one speaker per language) speech datasets for ten languages, composed of short audio clips from LibriVox audiobooks.

Pascual, S., Ravanelli, M., Serrà, J., Bonafonte, A., & Bengio, Y. (2019). Learning Problem-agnostic Speech Representations from Multiple Self-supervised Tasks. arXiv, 1904.03416. Retrieved from https://arxiv.org/abs/1904.03416v1

    - We propose problem-agnostic speech encoder (PASE) for self-supervised speech models, where a single feature encoder is followed by multiple workers that solve different self-supervised tasks, defined as regression or binary classification (fig. 1). The intuition is that each self-supervised task may bring a different view or soft constraint on the learned representation.
    - We employ simple worker structure to encourage the encoder, and not the workers, to discover high-level features.

Schneider, S., Baevski, A., Collobert, R., & Auli, M. (2019). wav2vec: Unsupervised Pre-training for Speech Recognition. arXiv, 1904.05862. Retrieved from https://arxiv.org/abs/1904.05862v4

    - We propose wav2vec model that was trained on large amounts of unlabeled audio
    - Wav2vec takes raw audio as input and computes a general representation at a lower temporal frequency (fig. 1). The encoder is a 5-layer CNN, and the output stride is 10 ms, and the receptive field is about 30 ms of 16 kHz. Then, the 9-later CNN "context network" mixes multiple representations with a total receptive field about 210 ms (810 ms for large model).
    - The objective is to predict future samples from a given signal context, with contrastive loss that requires distinguishing a true future audio sample from negatives (as in "Representation Learning with Contrastive Predictive Coding").
    - To fine-tune wav2vec for the TIMIT task (predicting phonemes), we pass output representations (instead of MFCC features as a baseline) into acoustic CNN model, which outputs phoneme probabilities. The model is trained using the Auto Segmentation Criterion.
    - To fine-tune wav2vec on WSJ benchmark (transribing text), acoustic CNN model predicts probabilities for 31 graphemes, including the standard English alphabet, the apostrophe and period, two repetition characters (e.g. the word ann is transcribed as an1), and a silence token used as word boundary. For decoding the emissions from the acoustic model we use a lexicon as well as a separate language model trained on the WSJ language modeling data only.

Stephenson, C., Feather, J., Padhy, S., Elibol, O., Tang, H., McDermott, J., & Chung, S. (2019). Untangling in Invariant Speech Recognition. Advances in Neural Information Processing Systems, 32. Retrieved from https://proceedings.neurips.cc/paper/2019/hash/e2db7186375992e729165726762cb4c1-Abstract.html

Andrusenko, A., Laptev, A., & Medennikov, I. (2020). Towards a Competitive End-to-End Speech Recognition for CHiME-6 Dinner Party Transcription. arXiv, 2004.10799. Retrieved from https://arxiv.org/abs/2004.10799v3

    - End-to-end ASR models are prone to accuracy degradation in noisy and low-resource conditions.
    - We show that on the CHiME-6 Challenge data (real dinner parties recorded in reverberant and noisy conditions), our best end-to-end model (RNN-Transducer with improved beam search and the Guided Source Separation augmentation) outperforms the hybrid baseline (TDNN-F, factorized time-delayed neural network) system only by 2.7% WER.

Baevski, A., Zhou, H., Mohamed, A., & Auli, M. (2020). wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations. arXiv, 2006.11477. Retrieved from https://arxiv.org/abs/2006.11477v3

    - We present wav2vec 2.0, a framework for self-supervised learning from raw audio data
    - The raw waveform is first procesed by CNN feature encoder F, and then is fed to a context network G which follows the Transformer architecture. We use a convolutional layer which acts as relative positional embedding.
    - To pre-train the model we mask a certain proportion of time steps in the latent space of feature encoder F. The training objective requires identifying for each masked time step the correct quantized latent audio representation G(F(waveform)) in a set of distractors, that are uniformly sampled from other masked time steps of the same utterance (noise-contrastive loss). The outputs of the feature encoder are quantized in a differentiable way with a Gumbel softmax quantization module Q to produce targets Q(F(waveform)). The additional Diversity Loss is used to increase the use of the quantized codebook representations. Note that the model do nor employ MLM (BERT) loss.
    - Compared to vq-wav2vec, wav2vec 2.0, the transformer accepts continuous inputs from CNN.
    - Pre-trained models are fine-tuned for speech recognition by adding a randomly initialized linear projection into C classes representing the vocabulary. Models are optimized by minimizing a CTC loss, and we apply a modified version of SpecAugment.
    - We achieve SOTA on the full Librispeech benchmark for noisy speech.
    - We expect further performance gains by switching to a seq2seq architecture and a word piece vocabulary.
    - A question: if CNN ontput stride becomes too low, the pre-training objective becomes easier?
    - A concurrent work with Conformer

Gulati, A., Qin, J., Chiu, C.-C., Parmar, N., Zhang, Y., Yu, J., ...Pang, R. (2020). Conformer: Convolution-augmented Transformer for Speech Recognition. arXiv, 2005.08100. Retrieved from https://arxiv.org/abs/2005.08100v1

    - We propose Conformer, convolution-augmented transformer for speech recognition
    - Each conformer block contains convolution and MHSA
    - We use a single-LSTM-layer decoder in all our models
    - We perform various ablation studies
    - A concurrent work with wav2vec 2.0

Kahn, J., Lee, A., & Hannun, A. . Self-Training for End-to-End Speech Recognition. ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE. doi: 10.1109/ICASSP40776.2020.9054295

Kawakami, K., Wang, L., Dyer, C., Blunsom, P., & Oord, A. v. d. (2020). Learning Robust and Multilingual Speech Representations. arXiv, 2001.11128. Retrieved from https://arxiv.org/abs/2001.11128v1

    - We learn audio representations using contrastive predictive coding, then we train an ASR model using our representations, and evaluate it under domain and language shifts.
    - We find that large pretraining dataset lead to more robustness to domain shifts, compared to both log filterbank features as well as to pretraining just on LibriSpeech.
    - We also train ASR models on 25 languages, and show that our representations outperform those pretrained only on clean English data in the language transfer setup.
    - We confirm that usupervised representations consistently improve robustness on downstream tasks, and representations learned from multilingual data can transfer across many languages.

Keung, P., Niu, W., Lu, Y., Salazar, J., & Bhardwaj, V. (2020). Attentional Speech Recognition Models Misbehave on Out-of-domain Utterances. arXiv, 2002.05150. Retrieved from https://arxiv.org/abs/2002.05150v1

    - We discovere the problem of echographic transcription: when autoregressive seq2seq models are used to decode out-of-domain audio, the output transcript contains the same words or phrases repeated over and over again. There are many 5-second recordings that produce more than 500 characters of decoding output.
    - When decoding audio from the British National Corpus with an attentional encoder-decoder model trained solely on the LibriSpeech corpus.
    - This behavior occurs even when the model performs well on the in-domain task.
    - A frame-synchronous hybrid (DNN-HMM) model trained on the same data does not produce these unusually long transcripts (fig. 1), but these decoding issues are reproducible in a speech transformer model from ESPnet, and to a lesser extent in a self-attention CTC model. This suggests that these issues are intrinsic to the use of the attention mechanism, when the decoder can attend over the entire length of the encoded input to generate each output token.
    - When the outputs are very repetitive, the attention mechanism of the AR-S2S model attends to the same section of audio without proceeding forwards in time (fig. 2).
    - We create a separate length prediction model to predict the correct number of wordpieces in the output, which allows us to identify and truncate problematic decoding results.

Khurana, S., Laurent, A., Hsu, W.-N., Chorowski, J., Lancucki, A., Marxer, R., & Glass, J. (2020). A Convolutional Deep Markov Model for Unsupervised Speech Representation Learning. arXiv, 2006.02547. Retrieved from https://arxiv.org/abs/2006.02547v2

Koenecke, A., Nam, A., Lake, E., Nudell, J., Quartey, M., Mengesha, Z., ...Goel, S. (2020). Racial disparities in automated speech recognition. Proc. Natl. Acad. Sci. U.S.A. Retrieved from https://www.pnas.org/doi/pdf/10.1073/pnas.1915768117

    - ASR systems exhibit WER of 0.35 for black speakers compared with 0.19 for white speakers.
    - We suggest that ASR systems are confused by the phonological, phonetic, or prosodic characteristics of African American Vernacular English. 
    - The likely cause of this shortcoming is insufficient audio data from black speakers when training the models.

Kürzinger, L., Winkelbauer, D., Li, L., Watzel, T., & Rigoll, G. (2020). CTC-Segmentation of Large Corpora for German End-to-End Speech Recognition. Speech and Computer. Springer. doi: 10.1007/978-3-030-60276-5_27

Likhomanenko, T., Xu, Q., Pratap, V., Tomasello, P., Kahn, J., Avidov, G., ...Synnaeve, G. (2020). Rethinking Evaluation in ASR: Are Our Models Robust Enough? arXiv, 2010.11745. Retrieved from https://arxiv.org/abs/2010.11745v3

    - We study acoustic model transfer across five public datasets, as well as transfer to out-of-domain, real-world audio data.
    - No single validation or test set from public datasets is sufficient to measure transfer to other public datasets or to real-world audio data. This s uggests that ASR researchers interested in producing transferable acoustic models should report results on several public datasets, at very least including TED-LIUM (v3).
    - Reverberative and additive noise augmentation improves generalization performance across domains.
    - We provided a recipe for a community-reproducible robust ASR model, which can be trained with a couple of public audio datasets, and language models trained on the Common Crawl dataset

Ling, S., & Liu, Y. (2020). DeCoAR 2.0: Deep Contextualized Acoustic Representations with Vector Quantization. arXiv, 2012.06659. Retrieved from https://arxiv.org/abs/2012.06659v1

Liu, A. T., Li, S.-W., & Lee, H.-y. (2020). TERA: Self-Supervised Learning of Transformer Encoder Representation for Speech. arXiv, 2007.06028. Retrieved from https://arxiv.org/abs/2007.06028v3

Mani, A., Palaskar, S., Meripo, N. V., Konam, S., & Metze, F. (2020). ASR Error Correction and Domain Adaptation Using Machine Translation. arXiv, 2003.07692. Retrieved from https://arxiv.org/abs/2003.07692v1

    - We propose to carry out ASR error correction via domain adaptation. We learn an adaptation module that goes from hypothesis of pre-trained ASR towards reference text (as in automatic machine translation), thus learning to fix systematic errors the pre-trained ASR makes due to domain mismatch.

Nguyen, T. A., de Seyssel, M., Rozé, P., Rivière, M., Kharitonov, E., Baevski, A., ...Dupoux, E. (2020). The Zero Resource Speech Benchmark 2021: Metrics and baselines for unsupervised spoken language modeling. arXiv, 2011.11588. Retrieved from https://arxiv.org/abs/2011.11588v2

    - We introduce a new unsupervised task, spoken language modeling: the learning of linguistic representations from raw audio signals without any labels. We suggest that self-supervised acoustic models may actually go beyond acoustic modeling, learning their own LM from raw audio.
    - We introduced the new Zero Resource Speech Benchmark 2021 for spoken language models. It is composed of 4 zero-shot tests probing 4 linguistic levels: acoustic, lexical, syntactic and semantic. They are zero-shot in that they do not require training a classifier.
    - A self-supervised pipeline of Contrastive Predictive Coding + k-means clustering + LM (LSTM or BERT), trained on LibriSpeech, can perform above chance on all of these tests, while being worse than text-based models trained on the same data.
    - Seems like there is another close paper (another version?): https://arxiv.org/abs/2102.01192v2

Pratap, V., Xu, Q., Sriram, A., Synnaeve, G., & Collobert, R. (2020). MLS: A Large-Scale Multilingual Dataset for Speech Research. arXiv, 2012.03411. Retrieved from https://arxiv.org/abs/2012.03411v2

    - We intoduce the Multilingual LibriSpeech (MLS) dataset derived from read audiobooks from LibriVox and consists of 8 languages, including about 44.5K hours of English and a total of about 6K hours for other languages

Ravanelli, M., Zhong, J., Pascual, S., Swietojanski, P., Monteiro, J., Trmal, J., & Bengio, Y. (2020). Multi-task self-supervised learning for Robust Speech Recognition. arXiv, 2001.09239. Retrieved from https://arxiv.org/abs/2001.09239v2

    - We propose PASE+, an improved version of PASE for self-supervised speech models, to perform better in noisy and reverberant environments. We employ an online speech distortion module, that contaminates the input signals with various disturbances (fig. 1). We also combine our CNN encoder with a quasi-recurrent neural network (QRNN). Finally, we refine the set of self-supervised workers.

Rivière, M., Joulin, A., Mazaré, P.-E., & Dupoux, E. (2020). Unsupervised pretraining transfers well across languages. arXiv, 2002.02848. Retrieved from https://arxiv.org/abs/2002.02848v1

    - We show that a slight modification of the CPC (contrastive predictive coding) pretraining extracts features that transfer well from  English (Librispeech) to several low-resource languages from the Common Voice database.
    - Out modifications: 1) we replace batch normalization with a channel-wise normalization, 2) we replace each linear classifier with a 1-layer Transformer network, 3) we use an LSTM instead of a GRU
    - To evaluate on a target language, we freeze the model after the pre-training and simply learn a linear classifier for the targeted language, using CTC loss. This procedure explicitly measures the linear separability of the phoneme representation, once transferred to a target language.

Sainath, T. N., He, Y., Li, B., Narayanan, A., Pang, R., Bruguier, A., ...Zhao, D. (2020). A Streaming On-Device End-to-End Model Surpassing Server-Side Conventional Model Quality and Latency. arXiv, 2003.12710. Retrieved from https://arxiv.org/abs/2003.12710v2

    - For ASR on-device streaming models, we develop a first-pass RNN-T model and a second-pass LAS rescorer fo achieve both high quality and low latency (the delay between when a user stops speaking and the hypothesis is finalized).
    - We combine these models because RNN-T models have been shown to be competitive in quality, and non-streaming models, such as LAS, have been shown to perform well nder low-latency constrains.
    - We train our model on multi-domain audio-text utterance pairs, including search traffic, telephony data and YouTube data to increase acoustic diversity and the vocabulary seen by the model. We also train with accented English speech to make the model more robust to different pronunciations.
    - One of the issues with using multi-domain data is that each domain has different transcription conventions ("$100" versus "one hundred dollars"). We explore feeding a domain ID to the RNN-T encoder as a one-hot vector, with the UD being one of the 4 domains. Thus we are able to improve upon a model trained on voice search data only (single domain).
    - We also explore various ideas to improve latency of our model.

Shor, J., Jansen, A., Maor, R., Lang, O., Tuval, O., Quitry, F. d. C., ...Haviv, Y. (2020). Towards Learning a Universal Non-Semantic Representation of Speech. arXiv, 2002.12764. Retrieved from https://arxiv.org/abs/2002.12764v6

Xu, Q., Likhomanenko, T., Kahn, J., Hannun, A., Synnaeve, G., & Collobert, R. (2020). Iterative Pseudo-Labeling for Speech Recognition. arXiv, 2005.09267. Retrieved from https://arxiv.org/abs/2005.09267v2

Zhang, Q., Lu, H., Sak, H., Tripathi, A., McDermott, E., Koo, S., & Kumar, S. (2020). Transformer Transducer: A Streamable Speech Recognition Model with Transformer Encoders and RNN-T Loss. arXiv, 2002.02562. Retrieved from https://arxiv.org/abs/2002.02562v2

    - We propose Transformer Transducer by replacing ENN-based audio and label encoders in the RNN-T architecture with Transformer encoders. As in the original RNN-T model, the joint network (fig. 1) at each step combines the audio encoder output and the label encoder output given the previous non-blank output token sequence. The joint network returns the distribution over the next token. Our model has 18 audio and 2 label encoder layers.
    - The model uses the RNN-T loss (see the original paper "Sequence Transduction with Recurrent Neural Networks")
    - Our model achieves a new SOTA on the LibriSpeech benchmark

Zheng, X., Liu, Y., Gunceler, D., & Willett, D. (2020). Using Synthetic Audio to Improve The Recognition of Out-Of-Vocabulary Words in End-To-End ASR Systems. arXiv, 2011.11564. Retrieved from https://arxiv.org/abs/2011.11564v2

    - We aim to boost the recognition accuracy of RNN-T model on out-of-vocabulary (OOV) words.
    - We use a text-to-speech (TTS) engine to provide synthetic audio. We use 2.3K hours of anonymised far-field in-house data. OOV words are all the words that have not appeared in train set but have appeared at least three times in dev set. The utterances in Dev containing any OOV words are extracted as a subset DevOOV (this only accounts for 0.7% of the Dev set, or 6.5K utterances). Similarly, the utterances in Eval containing any (the same) OOV words are extracted as a subset EvalOOV, containing 4.3K utterances.
    - The best performance is achieved by fine-tuning the RNN-T on both original training data and extra synthetic data with elastic weight consolidation (EWC) applied on the encoder. This yields a 57% relative word error rate (WER) reduction on utterances containing OOV words without any degradation on the whole test set.

Abdullah, B. M., Mosbach, M., Zaitova, I., Möbius, B., & Klakow, D. (2021). Do Acoustic Word Embeddings Capture Phonological Similarity? An Empirical Study. arXiv, 2106.08686. Retrieved from https://arxiv.org/abs/2106.08686v1

Aksënova, A., van Esch, D., Flynn, J., & Golik, P. (2021). How Might We Create Better Benchmarks for Speech Recognition? ACL Anthology, 22–34. doi: 10.18653/v1/2021.bppf-1.4

    - We outline a taxonomy of speech recognition use cases, proposed for the next generation of ASR benchmarks. The ultimate goal of benchmarking should be the ability to predict how well an ASR system is going to generalize to new and unseen data. An ideal benchmark set would include what we will call "horizontal" and "vertical" variation.
    - As for "horizontals" (scenarios), ASR application domains can be roughly subdivided based on the number of speakers, the mode of speech (spontaneous vs. prepared speech) and the intended recipient (human or device). An ideal benchmark set would cover as many of these horizontals as possible: dictation, voice search and control, voicemail, oration, audiobooks, conversations and meetings, podcasts, movies and TV.
    - As for "verticals" (technical challenges), the ideal benchmark should cover as many of the following factors: terminology and phrases, speed, acoustic environment, encoding formats.
    - Also, defining and applying a comprehensive set of transcription conventions is critical in building high-quality data sets. For example, should hesitations like "uh" be transcribed? And if a speaker says "wanna", should the transcription reflect that as such, or should the transcriber transcribe that as "want to"? Perhaps the most important choice in such transcription conventions is whether to adopt "spoken-domain" transcriptions, where numbers are spelled out in words (e.g. "three thirty"), or "written-domain" transcriptions, where they are rendered in the typical written form ("3:30").
    - Test sets that are intended to measure how well an ASR system deals with speech with background noise should have a realistic amount of background noise: not too little, but also not too much. Adding noise artificially does not take into account the Lombard effect (speakers tend to increase their vocal effort when speaking in loud noise).
    - The average WER alone, weighted by the number of words, is not sufficient to describe the shape of the distribution over the individual local measurements. Given two ASR systems with identical WERs, we almost always prefer the one with the lower standard deviation, as it reduces the uncertainty w.r.t. the worst case. A more accurate metric that samples the shape of the distribution consists of percentiles (e.g. 90, 95 or 99) that are more suitable to provide an upper bound. Additionally, reporting the standard deviation allows researchers to judge whether an improvement in WER is significant or just a statistical fluctuation. Finally, WER can also be calculated on not just the top machine hypothesis, but also on the full n-best list.
    - Downstream use cases may require more than just a word-by-word textual transcription. For example, having per-word confidence scores can be helpful in dialog systems. Having accurate timestamps at the word level is essential in many application of the long form domain. Having phonemic transcriptions for every word enables downstream disambiguation.
    - Speaker diarization is yet another type of of metadata that can be emitted at a per-word or per-phrase level, for which independent benchmarks already exist.
    - Optimizing only for WER, as most current benchmarks imply, does not reflect considerations that are ubiquitous in real-world deployments, such as latency and compute resources. The process of finding the most likely hypothesis in ASR (often referred to as “decoding” for historical reasons) requires an efficient exploration of the search space: a subset of all possible hypotheses. A small search space allows for quick decoding, but often comes at the cost of higher WER. It is common to report an RTF vs WER curve which shows all possible operating points, allowing for mutual trade off. It is common to normalize the RTF by the number of CPU cores and hardware accelerators.
    - For ASR systems that stream output to the user while recognition is ongoing, additional metrics will be useful. A streaming system that emits highly inaccurate intermediate hypotheses can yield a jarring user experience, even if the final hypothesis achieves an acceptable WER. Yet another factor is streaming latency, e.g. how quickly partials are emitted.
    - In some cases, ASR models can hallucinate transcriptions: e.g. providing transcriptions for audio even where no speech is present, or simply misbehaving on out-of-domain utterances. Measuring whether an ASR system is prone to such hallucinations can be done by running it on test sets from domains that were unseen at training time. In addition, it is possible to employ reject sets which contain various kinds of audio that should not result in a transcription: for example, such reject sets may cover various noise, silence, speech in other languages, and so on.
    - If a new word such as ‘COVID-19’ comes up which is not yet recognized by the system, it would be preferable if adding such a new word could be done without necessitating a full retrain of the system. So, the degree to which it is easy to debug and fix any ASR system is worth mentioning.
    - The ideal benchmark set should enable developers to understand how their system behaves when processing various accents or dialects; whether factors like gender and age influence recognition performance in their system. We describe a simple, metric-independent population-weighted visualization framework designed to evaluate ASR systems based on demographic metadata.

Ao, J., Wang, R., Zhou, L., Wang, C., Ren, S., Wu, Y., ...Wei, F. (2021). SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing. arXiv, 2110.07205. Retrieved from https://arxiv.org/abs/2110.07205v3

Babu, A., Wang, C., Tjandra, A., Lakhotia, K., Xu, Q., Goyal, N., ...Auli, M. (2021). XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale. arXiv, 2111.09296. Retrieved from https://arxiv.org/abs/2111.09296v3

    - 

Bakhturina, E., Lavrukhin, V., & Ginsburg, B. (2021). A Toolbox for Construction and Analysis of Speech Datasets. arXiv, 2104.04896. Retrieved from https://arxiv.org/abs/2104.04896v3

    - The creation of new speech datasets remains an ongoing problem, because 1) Domain lexicon and language changes over time, 2) datasets need to contain a variety of samples with noisy room conditions, multi/single speakers recordings, speakers with different geographic origin, accent, gender, age, etc.
    - We introduce an open-sourced NeMo toolbox for analysis of existing speech datasets and construction of new speech corpora (fig. 2). The CTC-Segmentation tool can be used to splitting long audio files, and Speech Data Explorer (SDE) tool is for interactive audio data analysis.
    - We use our toolbox explaining how to construct the Russian LibriSpeech corpus, which improved WER on the MCV Russian dev subset.

Bapna, A., Chung, Y.-a., Wu, N., Gulati, A., Jia, Y., Clark, J. H., ...Zhang, Y. (2021). SLAM: A Unified Encoder for Speech and Language Modeling via Speech-Text Joint Pre-Training. arXiv, 2110.10329. Retrieved from https://arxiv.org/abs/2110.10329v1

Cámbara, G., Peiró-Lilja, A., Farrús, M., & Luque, J. (2021). English Accent Accuracy Analysis in a State-of-the-Art Automatic Speech Recognition System. arXiv, 2105.05041. Retrieved from https://arxiv.org/abs/2105.05041v1

    - We train an ASR model on the MLS dataset, and test the model on the CommonVoice dataset, which has labels indicating accent.
    - We see that WER can degrade up to an absolute 10% for accents with phonetic and prosodic characteristics further from American and UK English, like Asian accents.

Chan, W., Park, D., Lee, C., Zhang, Y., Le, Q., & Norouzi, M. (2021). SpeechStew: Simply Mix All Available Speech Recognition Data to Train One Large Neural Network. arXiv, 2104.02133. Retrieved from https://arxiv.org/abs/2104.02133v3

    - We present SpeechStew, an ASR model that is trained on a combination of various publicly available ASR datasets.
    - SpeechStew uses the Conformer RNN-T architecture.
    - SpeechStew simply mixes all the datasets without any balancing.
    - SpeechStew achieves SoTA or near SoTA results across a variety of tasks, without the use of an external LM, and learns powerful transfer learning representations.
    - On noisy, low resource ASR datasets, such as CHiME-6, end-to-end methods struggle relative to HMM-based baselines. We show that simple fine-tuning SpeechStew on CHiME-6 without a LM give WER compareble to a strong HMM baseline with LM. So, one can simply finetune a pre-trained model for only a few thousand gradient steps and achieve strong results.

Chen, S., Wang, C., Chen, Z., Wu, Y., Liu, S., Chen, Z., ...Wei, F. (2021). WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing. arXiv, 2110.13900. Retrieved from https://arxiv.org/abs/2110.13900v5

    - We propose WavLM, a self-supervised model that jointly learns masked speech prediction and denoising.
    - Speech denoising allows to apply the model to non-ASR tasks, such as diariazation, separation, speech enhancement.
    - WavLM employs gated relative position bias to better capture the sequence ordering of input speech.
    - We use more training data to eliminate the audiobook data bias (60k hours of Libri-Light, 10k hours of GigaSpeech, and 24k hours of VoxPopuli)

Chung, Y.-A., Zhang, Y., Han, W., Chiu, C.-C., Qin, J., Pang, R., & Wu, Y. (2021). W2v-BERT: Combining Contrastive Learning and Masked Language Modeling for Self-Supervised Speech Pre-Training. arXiv, 2108.06209. Retrieved from https://arxiv.org/abs/2108.06209v2

    - We propose w2v-BERT that explores MLM for self-supervised speech representation learning.
    - w2v-BERT combines the core methodologies from wav2vec 2.0 and BERT (fig. 1). It discretizes input continuous speech signals into a finite set of discriminative speech tokens with contrastive learning, and solves a masked prediction task on the discretized tokens. In contrast to HuBERT which relies on an iterative re-clustering and re-training process, or vq-wav2vec, which concatenates two separately trained modules, w2v-BERT directly optimizes a contrastive loss and a masked prediction loss simultaneously. In turn, wav2vec 2.0 only employs contrastive learning, whose resulting ASR performance lags behind that of combining contrastive learning and masked prediction.
    - We pre-train w2v-BERT on 60k hours of unlabeled speech data from the Libri-Light corpus
    - w2v-BERT yields SOTA performance on the well-benchmarked LibriSpeech task, after fine-tuning on LibriSpeech by adding an LSTM decoder, so that the ASR network is a sequence transducer.

Feng, S., Kudina, O., Halpern, B. M., & Scharenborg, O. (2021). Quantifying Bias in Automatic Speech Recognition. arXiv, 2103.15122. Retrieved from https://arxiv.org/abs/2103.15122v2

    - We systematically quantify the bias of a Dutch SotA ASR system against gender, age, regional accents and non-native accents.
    - Female speech is better recognised than male speech, for all native and non-native groups and for both speech styles.
    - Among the native speakers, teenagers achieve the best WER performances in read and HMI speech, followed by the older adults;(age 65+) (their speech was not always well articulated) while children were the worst recognised. Speech of native speakers is recognised much better than that of non-native speakers of Dutch. Among the non-native speakers, the performance differences between children and adults do not differ much.
    - For each group, the WER performance of human-machine interaction speech is consistently worse than that of read speech, probably because the former is less well prepared than the latter. This confirms that the size of the bias is influenced by the speaking style of the person.
    - As for accents, speech spoken by people from Flanders achieved the worst WER performance in all age groups except for the older adults. Also, the results suggest that older speakers in the Netherlands typically have stronger regional accents than children and teenagers.

Galvez, D., Diamos, G., Ciro, J., Cerón, J. F., Achorn, K., Gopi, A., ...Reddi, V. J. (2021). The People's Speech: A Large-Scale Diverse English Speech Recognition Dataset for Commercial Usage. arXiv, 2111.09344. Retrieved from https://arxiv.org/abs/2111.09344v1

    - We introduce the People’s Speech, a supervised ASR dataset. In total, there are 52,500 hours of audio in our dataset, of which 51,890 hours are English. The dataset also includes 23 other languages (fig. 2). The dataset contains different background noise (fig. 6)
    - The data is collected via searching vimeo.com and archive.org for appropriately licensed audio data with existing transcriptions and aligning the audio to the transcripts.
    - Audio content with transcripts on the Internet Archive is 1) abundant, 2) searchable by license type, and 3) diverse, facilitating the creation of a our large-scale open speech dataset, so large-scale datasets can be created more efficiently than previously thought.
    - We do not create train, test, and dev splits for our dataset for two reasons. One, we don’t have a way to ensure there is no speaker overlap between the splits. Two, we don’t have a way to ensure duplicated audio data.

Gao, C., Cheng, G., & Zhang, P. (2021). Multi-Variant Consistency based Self-supervised Learning for Robust Automatic Speech Recognition. arXiv, 2112.12522. Retrieved from https://arxiv.org/abs/2112.12522v2

    - A problem in self-supervised audio models is that the clean signal and the background noise play the same role in the self-supervised objective function. If the signal-to-noise ratio (SNR) is low, the SSL model may pay more attention to the inessential background noise than the text-related speech signal.
    - We propose a multivariant consistency based self-supervised learning (MVC-SSL) as a robust pre-training method that adapts to different environments. This method is designed for noisy and distant-talking speech in real-world applications.
    - MVC-SSL can calculate the contrastive loss among audios from different channels or acoustic conditions. As a result, the MVC-SSL model can be more robust with the background noise and reverberation.
    - We evaluate on the CHiME-4 and AMI. The proposed method can reduce 30% relative WER over the baseline wav2vec2.0.

Gris, L. R. S., Casanova, E., de Oliveira, F. S., Soares, A. d. S., & Junior, A. C. (2021). Brazilian Portuguese Speech Recognition Using Wav2vec 2.0. arXiv, 2107.11414. Retrieved from https://arxiv.org/abs/2107.11414v3

    - We fine-tune the Wav2vec 2.0 model for Brazilian Portuguese.
    - Our work suggests that the use of datasets with a large variety of vocabulary and speakers are still important to the development of robust models, as well as in-domain training.

Hsu, W.-N., Bolte, B., Tsai, Y.-H. H., Lakhotia, K., Salakhutdinov, R., & Mohamed, A. (2021). HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units. arXiv, 2106.07447. Retrieved from https://arxiv.org/abs/2106.07447v1

    - Self-supervised pre-training for speech is complex, because the boundaries between sound units are not known, there is no prior lexicon of discrete sound units available, and multiple sounds may be present in each input utterance.
    - We propose the Hidden-Unit BERT (HuBERT) approach for self-supervised speech representation learning.
    - HuBERT relies on predicting K-means cluster assignments of masked segments of continuous input. HuBERT utilizes an offline clustering step to provide aligned target labels for a BERT-like prediction loss (fig. 1).
    - The learned representation quality improves dramatically with iteratively refining K-means cluster assignments using learned latent representations for a previous iteration. Since we expect a pre-trained model to provide better representations than the raw acoustic feature such as MFCCs, we can create a new generation of clusters by training a discrete latent model (such as K-Means or GMM) over the learned latent representations. This idea is similar to iterative pseudo labeling for semi-supervised ASR.
    - HuBERT either matches or improves upon the state-of-theart wav2vec 2.0 performance on all fine-tuning subsets of 10mins, 1h, 10h, 100h, and 960h.
	- IMO, the refinment stage is not explained clearly. Probably, they run K-Means over the feature vectors obtained on the last layer.
	- IMO this looks like a denoising and discretizing autoencoder, that will capture not only phonemes, but many irrelevant (for many tasks) details, such as voice identity, pace, white and structured noise and background speech.

Hsu, W.-N., Sriram, A., Baevski, A., Likhomanenko, T., Xu, Q., Pratap, V., ...Auli, M. (2021). Robust wav2vec 2.0: Analyzing Domain Shift in Self-Supervised Pre-Training. arXiv, 2104.01027. Retrieved from https://arxiv.org/abs/2104.01027v2

    - We focus on a case where the pre-training domain, the fine-tuning domain and the test data all differ from each other.
    - We show that adding unlabeled data whose domain matches the test data always improves performance, even if the labeled data for fine-tuning is out-of-domain. This has practical applications since it is much easier to obtain unlabeled data for a particular target domain than labeled data.
    - Pre-training on three domains achieves better performance than on two domains, which in turn is better than on one domain, regardless of what labeled data they are fine-tuned on.

Koizumi, Y., Karita, S., Wisdom, S., Erdogan, H., Hershey, J. R., Jones, L., & Bacchiani, M. (2021). DF-Conformer: Integrated architecture of Conv-TasNet and Conformer using linear complexity self-attention for speech enhancement. arXiv, 2106.15813. Retrieved from https://arxiv.org/abs/2106.15813v2

    - We focus on Single-channel speech enhancement (SE), the task of recovering target speech from a noisy signal (which does not include interference speech signals). Conv-TasNet is a powerful model for SE. One of the main research topics in SE is improving the mask prediction architecture ("M" in eq. 1). For example, the improved time-dilated convolution network (TDCN++) extended Conv-TasNet to improve SE performance. A promising candidate for improving mask prediction networks is the Conformer architecture.
    - We propose DF-Conformer (dilated FAVOR Conformer). We combine Conformer layers with the dilated convolution layers of the TDCN++ architecture. To make the model computationally feasible, we use a linear-complexity variant of self-attention in the Conformer, known as fast attention via positive orthogonal random features (FAVOR+), as used in Performer.
    - To train the model, we use speech from LibriVox and non-speech sounds from freesound.org.
    - Examples of spectrograms and attention matrices in DF-Conformer-8 are shown in fig. 2.

Lai, C.-I. J., Zhang, Y., Liu, A. H., Chang, S., Liao, Y.-L., Chuang, Y.-S., ...Glass, J. (2021). PARP: Prune, Adjust and Re-Prune for Self-Supervised Speech Recognition. arXiv, 2106.05933. Retrieved from https://arxiv.org/abs/2106.05933v2

    - We first show that applying widely-adopted pruning methods, such as One-Shot Magnitude Pruning (OMP) and Iterative Magnitude Pruning (IMP) for SOTA speech SSL models is extremely time-consuming, and gives no performance boost.
    - We present Prune-Adjust-Re-Prune (PARP), a method for discovering sparse subnetworks within pre-trained speech SSL. It consists of the following two cyclic steps: 1) Prune the model, 2) Fine-tune the network on target downstream task/language while zeroing out pruned weights, but allowing them to be be updated by gradient descent. After a few number of model updates, re-prune again and so on. So, different from other pruning methods, PARP allows pruned-out weights to be revived during finetuning.
    - We fine-tune we pre-trained wav2vec2-base and wav2vec2-large on the 10h/1h/10min splits from Librispeech and Libri-light and show that PARP yields over 10% WER reduction over the full model under ultra-low data regime (fig. 3), and the method adds minimal computation overhead to existing SSL downstream fine-tuning.
    - We also show the transferability of pruning mask discovered from a source language by finetuning its subnetwork on another target language, and the possibility of obtaining a single shared subnetwork for all downstream languages.
    - We also demonstrate PARP’s effectiveness on pre-trained BERT/XLNet, mitigating the cross-task performance degradation reported in BERT-Ticket.
    - IMO, fine-tuning SSL wav2vec2 model on LibreSpeech is very different from fine-tuning a pre-trained ASR model on some specific dataset. In the latter case we also need to compare the fine-tuned versions against the original model.

Majumdar, S., Balam, J., Hrinchuk, O., Lavrukhin, V., Noroozi, V., & Ginsburg, B. (2021). Citrinet: Closing the Gap between Non-Autoregressive and Autoregressive End-to-End Models for Automatic Speech Recognition. arXiv, 2104.01721. Retrieved from https://arxiv.org/abs/2104.01721v1

    - End-to-end neural ASR models can be roughly classified into 1) non-autoregressive CTC models, 2) autoregressive Seq2Seq models with attention, 3) autoregressive RNN-Transducers. CTC models are more stable to train than autoregressive models, but the latest Seq2Seq models and RNN-Transducers significantly outperform them.
    - It is believed that "Because of the strong conditional independence assumption, CTC does not implicitly learn a LM over the data (unlike the attention-based encoder-decoder architectures). It is therefore essential when using CTC to interpolate a language model".
    - We propose Citrinet: a deep convolutional CTC model for ASR. It significantly closes the gap between CTC and the best Seq2Seq and Transducers models without any external LM, contrary to the common belief that "CTC requires an external LM to output meaningful results".

Meng, Y., Chou, Y.-H., Liu, A. T., & Lee, H.-y. (2021). Don't speak too fast: The impact of data bias on self-supervised speech models. arXiv, 2110.07957. Retrieved from https://arxiv.org/abs/2110.07957v3

    - We study how pre-training data affects self-supervised speech models by pre-training models on biased datasets targeting different factors of speech, including gender, content, and prosody.
    - Our experiments show that self-supervised speech models (TERA and APC) have tolerance toward gender bias.
    - We find that the content of speech has little impact on the performance of S3Ms across downstream tasks
    - self-supervised speech models do show a preference toward a slower speech rate.

Pasad, A., Chou, J.-C., & Livescu, K. (2021). Layer-wise Analysis of a Self-supervised Speech Representation Model. arXiv, 2107.04734. Retrieved from https://arxiv.org/abs/2107.04734v3

    - We analyze wav2vec 2.0 representations.
    - We use projection-weighted Canonical Correlation Analysis to measure similarity between the wav2vec 2.0 layer representations and quantities of interest such as 1) representations from from adifferent layer of the same model, 2) representations from a fine-tuned version of the model, 3) mel filter bank features (acoustic information), 4) acoustically grounded word embeddings (word-level acoustic-phonetic information), 5) GloVe word embeddings (word meaning information).
    - We use mutual information to measure dependence between the the wav2vec 2.0 layer representations and the corresponding (discrete) phone or word labels.
    - As we go deeper into the model, the representation starts deviating from the input speech features followed by a reverse trend where even deeper layers become more similar to the input, as if reconstructing the input (fig. 1, black line).
    - The shallowest layers encode acoustic features, followed by phonetic, word identity, and word meaning information, and then followed by a reverse (fig. 1, other lines).
    - The final convolutional layers and initial transformer layers are highly correlated with mel spectrogram features, suggesting that the model learns to extract features similar to human-engineered ones.
    - Fine-tuning the model for ASR breaks the autoencoder-style behavior in the final few layers. Higher layers change the most in fine-tuning, suggesting that the pre-trained model may not serve as a good initialization of these top layers for ASR. This finding suggests re-initializing the final 1-3 layers before fine-tuning, as has been recently discovered for BERT. This outperforms the standard approach of initializing all layers from the pre-trained model, with large improvements when fine-tuning on the 10-minute training set and minor improvements for larger training sets.

Riviere, M., Copet, J., & Synnaeve, G. (2021). ASR4REAL: An extended benchmark for speech models. arXiv, 2110.08583. Retrieved from https://arxiv.org/abs/2110.08583v1

    - We introduce a set of ASR benchmarks matching real-life conditions.
    - We have noticed a small accuracy gap based on gender.
    - ASR models show big accuracy variations by accents.
    - ASR models show very strong performance gap based on the socio-economic status of the speaker.
    - ASR models show important performance drop onspontaneous speech.
    - LMs in their current form are not be adapted for spontaneous speech: even a LM trained on a dataset as big as Common Crawl does not seem to have significant positive effect.
    - This reiterates the importance of developing conversational LMs.

Sadhu, S., He, D., Huang, C.-W., Mallidi, S. H., Wu, M., Rastrow, A., ...Maas, R. (2021). Wav2vec-C: A Self-supervised Model for Speech Representation Learning. arXiv, 2103.08393. Retrieved from https://arxiv.org/abs/2103.08393v2

    - The wav2vec 2.0 problem formulation can result in several locally optimal codebooks. Some examples from our experiments: 1) only two codes are assigned: one for speech and the other for nonspeech; 2) the model assigns specific codes to fixed temporal locations, irrespective of the speech, to enable a good contrastive loss. The latter is especially probable if voice usually occurs at fixed temporal locations, as in our dataset. Hence, the wav2vec 2.0 codebook learning methodology might not generalize well to our pre-training dataset.
    - We propose Wav2vec-C - a modification of wav2vec 2.0, regularized by an additional consistency network that learns to reconstruct the input features from the quantized representations in a way similar to a VQ-VAE model. This enforces the latent space to preserve meaningful information that enable a low reconstruction error.
    - Our encoder network f (fig. 1) is a 3-layer LSTM as encoder with log-STFT input features. We quantize LSTM outputs with a product quantization module ("q" on fig. 1): we split each 768-demensional output vector into a pair of 384-dimensional sub-vectors, and store two codebooks (one for each sub-vector). Each codebook contains 320 384-dimensional code vectors, and quantization is performed by nearest neighbor search. Then the two code vectors are concatenated. To learn codebooks, we use either Gumbel-softmax, or K-means with a specific backprop method (sec 2.4.1, 2.4.2). We use a SpecAugment module to mask out portions of the continuous encodings, and feed them into a transformer context network g (fig. 1) with sinusoidal positional embedding, outputting the context representations. We apply a contrastive loss between the quantized encodings and the context representations, by selecting 50 negative samples. Our consistency network r (fig. 1) is a 3-layer LSTM that accepts the quantized embeddings and outputs "consistency vectors", denoted as S. We minimize the consistency loss: L2 distance between S and the log-STFT input features X. If we ignore the consistency loss, the rest of the model is similar to wav2vec 2.0 with one difference: wav2vec 2.0 uses CNN encoder that accepts raw audio. We also tune a magnitude of a diversity loss in Gumbel-softmax to avoid the codebook collapse that is commonly observed in VQ problems.
    - For SSL and fine-tuning, we use real far-field speech with varied degrees of SNR ranging between -40 to 50 dB. The proposed SSL model after RNN-T fine-tuning achieved a 1.4% relative WER reduction over baseline (without SSL) compared to a 0.7% reduction from wav2vec 2.0.
    - We also observed that ASR robustness is correlated with codebook diversity. Our model with Gumbel-softmax codebook fully utilizes codebook, while wav2vec 2.0 on our data under-utilizes codebook. The k-means codebook uses only a small fraction of the codes. The k-means codebook is is better than Gumbel-softmax for noisy test sets, and the Gumbel-softmax codebook is better on clean test sets. We hypothesize that a small codebook diversity may be good for noise robustness.

Salesky, E., Wiesner, M., Bremerman, J., Cattoni, R., Negri, M., Turchi, M., ...Post, M. (2021). The Multilingual TEDx Corpus for Speech Recognition and Translation. arXiv, 2102.01757. Retrieved from https://arxiv.org/abs/2102.01757v2

    - We present the Multilingual TEDx corpus, a collection of audio recordings from TEDx talks in 8 source languages (es, fr, pt, it, ru, el, ar, de).

Serai, P., Sunder, V., & Fosler-Lussier, E. (2021). Hallucination of speech recognition errors with sequence to sequence learning. arXiv, 2103.12258. Retrieved from https://arxiv.org/abs/2103.12258v3

    - We present a dual encoder model for ASR error prediction that can look at both word and phoneme sequence representations of input text to further improve the fidelity of hallucinated errors. In our model, the word sequence decoder is conditioned on a word sequence encoder and a phoneme sequence encoder from the ground truth transcription (fig. 1, 2).
    - This may be helpful when cascading ASR systems with NLU systems trained on text data. We can augment input text using our model, so that the downstream NLU model observes during training an input that contains the kind of errors expected from ASR at test time, and thus can learn to be robust to them.

Shi, X., Yu, F., Lu, Y., Liang, Y., Feng, Q., Wang, D., ...Xie, L. (2021). The Accented English Speech Recognition Challenge 2020: Open Datasets, Tracks, Baselines, Results and Methods. arXiv, 2102.10233. Retrieved from https://arxiv.org/abs/2102.10233v1

    - Accent poses a great challenge to the robustness of ASR. The difference between accents is mainly reflected in three aspects of pronunciation: stress, tone and duration.
    - We release a set of 160 hours of accented English speech from 8 countries with labels as the training set, and another 20 hours of speech without labels as the test set, including two unseen accents from another two countries.
    - Track 1 aims to study the English accent recognition problem. The final ranking is based on the recognition accuracy on the whole test set. The winner used TDNN based classification network with phonetic posteriorgram (PPG) feature as input, and they use text-to-speech (TTS) to expand the training data. Most teams have done a lot of work in data augmentation.
    - Track 2 studies the robustness of ASR system on accented English speech where the word error rate on the whole test set is used as the evaluation metric. Test sets include accents beyond training data in order to evaluate the generalization performance of the model. The winner used a CTC model with LAS rescoring while the CTC model was initialized by a Wav2Vec encoder trained in an unsupervised manner. The superior performance indicates that unsupervised training is promising in improving performance when labeled data is limited. Similarly to track 1, various data augmentation tricks were widely adopted, including volume augmentation and speed perturbation. Noise and reverberation did not give an obvious gain, probably because the acoustic and channel conditions between the test set and the training set is similar.

Shon, S., Pasad, A., Wu, F., Brusco, P., Artzi, Y., Livescu, K., & Han, K. J. (2021). SLUE: New Benchmark Tasks for Spoken Language Understanding Evaluation on Natural Speech. arXiv, 2111.10367. Retrieved from https://arxiv.org/abs/2111.10367v3

Tomanek, K., Zayats, V., Padfield, D., Vaillancourt, K., & Biadsy, F. (2021). Residual Adapters for Parameter-Efficient ASR Adaptation to Atypical and Accented Speech. arXiv, 2109.06952. Retrieved from https://arxiv.org/abs/2109.06952v1

    - We show that residual adapters work extremely well for acoustic adaptation of different speech models.
    - We study personalized models for atypical speech, and group models for accented speech.
    - Adapter layers work well for two different SOTA models: RNN-T, and Transformer Transducers (T-T).
    - This provides an easy way to deploy and store adapted models: a (generic) base model is deployed to all clients, and each individual or group can receive a personalized set of trained adapter layers that is small in size.

Wang, H., Qian, Y., Wang, X., Wang, Y., Wang, C., Liu, S., ...Wang, D. (2021). Improving Noise Robustness of Contrastive Speech Representation Learning with Speech Reconstruction. arXiv, 2110.15430. Retrieved from https://arxiv.org/abs/2110.15430v1

    - Usually noise robustness in ASR is achieved by speech enhancement modules. However, the ASR performance of the noise-suppressed speech may be degraded, caused by the nonlinear distortion brought by DNN. One solution is to train both networks jointly to allow train-time adaptation of the ASR network to such distortions. This complicates architecture and training.
    - We propose a novel self-supervised approach to address the robust ASR problem. We combine a reconstruction module with the contrastive learning framework of wav2vec 2.0.
    - Our network (fig. 1) takes in a noisy waveform as the input. We optimize the network 1) to reconstruct the clean speech, and 2) to solve the contrastive task by distinguishing the positive samples from the negative samples. Our reconstruction module uses two-layer bidirectional LSTM network with layer normalization, followed by a CNN decoder. Reconstruction is only performed in the pre-training stage, and it is not required in fine-tuning and inference.
    - In comparison with Wav2vec-C, we explicitly incorporate the denoising process, while they mainly focused on improving the codebook utilization ratio of wav2vec 2.0.
    - On Librispeech with synthetic noise, by adding the reconstruction module, our proposed model maintained the performance for clean speech, meanwhile, performed significantly better for noisy speech. For the real-world noisy speech from the CHiME-4 challenge, we have obtained the state of the art ASR performance without any denoising front-end, but the performance boost for the LibriSpeech based model was more obvious.
    - IMO, in CV there were shown that the robustness to synthetic distribution shifts, including noise does not make the model to be robust to the real distribution shifts. However, maybe this does not hold in ASR.

Wang, C., Rivière, M., Lee, A., Wu, A., Talnikar, C., Haziza, D., ...Dupoux, E. (2021). VoxPopuli: A Large-Scale Multilingual Speech Corpus for Representation Learning, Semi-Supervised Learning and Interpretation. arXiv, 2101.00390. Retrieved from https://arxiv.org/abs/2101.00390v2

    - We introduce VoxPopuli, a large-scale multilingual corpus providing 400K hours of unlabeled speech data in 23 languages from 2009-2020 European Parliament (EP) event recordings.
    - VoxPopuli also contains 1.8K hours of transcribed speeches in 15 languages and their aligned oral interpretations into 15 target languages totaling 17.3K hours.
    - We examine the out-of-domain pre-training on VoxPopuli with further few-shot phoneme recognition on CommonVoice. The results suggest the high generality of the speech representations learned from VoxPopuli.
    - Pre-training with in-domain VoxPopuli  unlabeled data substantially improves performance on VoxPopuli ASR data, especially for low-resource languages.

Yang, S.-w., Chi, P.-H., Chuang, Y.-S., Lai, C.-I. J., Lakhotia, K., Lin, Y. Y., ...Lee, H.-y. (2021). SUPERB: Speech processing Universal PERformance Benchmark. arXiv, 2105.01051. Retrieved from https://arxiv.org/abs/2105.01051v4

    - We introduce Speech processing Universal PERformance Benchmark (SUPERB) as a challenge with a leaderboard and a benchmark.
    - SUPERB targets at the direct usability of pretrained models on various popular tasks through any usage. We focus on investigating a simple framework solving all SUPERB tasks with a frozen, shared pretrained model, and lightweight prediction heads finetuned for each task. The framework puts an explicit constraint on downstream models to be as lightweight as possible for all tasks.
    - SSL models explored in this paper are summarized in Table 1. They include wav2vec 2.0, HuBERT and many other models.
    - Tasks from ASR: Phoneme Recognition, Automatic Speech Recognition, Keyword Spotting (detects preregistered keywords), Query by Example Spoken Term Detection (detects a spoken term (query) in an audio database (documents) by binary discriminating a given pair of query and document into a match or not).
    - Tasks to analyze speaker modeling: Speaker Identification (classifies each utterance for its speaker identity, where speakers are in the same predefined set for both training and testing), Automatic Speaker Verification (verifies whether the speakers of a pair of utterances match as a binary classification) and Speaker Diarization (predicts who is speaking when for each timestamp, and multiple speakers can speak simultaneously).
    - Tasks from Spoken Language Understanding: Intent Classification (classifies utterances into predefined classes to determine the intent of speakers) and Slot Filling (predicts a sequence of semantic slot-types from an utterance, like a slot-type FromLocation for a spoken word Taipei, which is known as a slot-value).
    - Task from paralinguistics: Emotion Recognition (predicts an emotion class for each utterance).
    - Since the last-layer representation is not always the best, the framework collects multiple hidden states from the pretrained model and weighted-sum them as the final representation. PR, KS, SID, IC, ER are simple tasks that are solvable with linear downstream models. For ASR, a vanilla 2-layer 1024-unit BLSTM is adopted and optimized by CTC loss on characters. The trained model is decoded with LibriSpeech official 4-gram LM powered by KenLM. As for ASV, we adopt the well-known x-vector as the downstream model.
    - wav2vec 2.0 and HuBERT outperforms others by a large margin on Phoneme Recognition and Intent Classification tasks. Their results on Speaker Identification and Emotion Recognition are also highly competitive.
    - FBANK cannot work on any task using linear models, but achieves competitive performance when allowing non-linear downstream models (in Automatic Speech Recognition, Slot Filling, Automatic Speaker Verification, and Speaker Diarization).
    - We observe that the ranking on Phoneme Recognition aligns with Automatic Speech Recognition weakly.
    - IMO, while Emotion Recognition refers to paralinguistics, spoken words may also highly correlate with emotions, so this task has two ways to solve with different transferability to another conditions.

Zhang, Y., Park, D. S., Han, W., Qin, J., Gulati, A., Shor, J., ...Wu, Y. (2021). BigSSL: Exploring the Frontier of Large-Scale Semi-Supervised Learning for Automatic Speech Recognition. arXiv, 2109.13226. Retrieved from https://arxiv.org/abs/2109.13226v3

Algayres, R., Ricoul, T., Karadayi, J., Laurençon, H., Zaiem, S., Mohamed, A., ...Dupoux, E. (2022). DP-Parse: Finding Word Boundaries from Raw Speech with an Instance Lexicon. arXiv, 2206.11332. Retrieved from https://arxiv.org/abs/2206.11332v1

Berrebbi, D., Shi, J., Yan, B., Lopez-Francisco, O., Amith, J. D., & Watanabe, S. (2022). Combining Spectral and Self-Supervised Features for Low Resource Speech Recognition and Translation. arXiv, 2204.02470. Retrieved from https://arxiv.org/abs/2204.02470v2

    - Spectral features (SF) such as log Mel-filterbanks could be more robust to domain shifts than self-supervised (SSL) features, since the majority of SSL models are trained using English speech only.
    - We combine SF and SSL representations through learnable fusions (linear, convolutional and co-attention based combinations).
    - We obtained strong improvements over ASR and ST datasets compared with the SSL baseline. Our models performances are strong for both in-domain and out-of-domain scenarios.
    - We also propose a MoE-based technique that enables quantifying the domain shift between the SSL training data and the target language resources. For example, Arabic model uses HuBERT and FBANK with similar weights, but Totonac (a rare language) model seems to rely at more than 80% on FBANK features.

Borgholt, L., Havtorn, J. D., Edin, J., Maaløe, L., & Igel, C. (2022). A Brief Overview of Unsupervised Neural Speech Representation Learning. arXiv, 2203.01829. Retrieved from https://arxiv.org/abs/2203.01829v1

    - We review the development of unsupervised representation learning for speech over the last decade.
    - Speech data offers unique challenges for unsupervised representation learning. As a result, methods from other domains rarely translate directly.
    - We group previous work into self-supervised models and probabilistic latent variable models (fig. 1). These categories are neither exhaustive nor mutually exclusive.
    - In general, work on global representations within self-supervised learning precedes work on local representations.
    - We also briefly touch upon evaluation procedures. A common approach is to evaluate the representations in terms of their usefulness for downstream tasks.

Borsos, Z., Marinier, R., Vincent, D., Kharitonov, E., Pietquin, O., Sharifi, M., ...Zeghidour, N. (2022). AudioLM: a Language Modeling Approach to Audio Generation. arXiv, 2209.03143. Retrieved from https://arxiv.org/abs/2209.03143v2

    - A current problem is that when not provided with strong conditioning (e.g., linguistic features), current audio synthesis models generate unstructured audio, such as babbling speech.
    - We introduce AudioLM (fig. 1, 2), a framework for audio generation that enables high-quality audio generation with long-term coherent structure. Starting from raw audio waveforms, we first construct coarse semantic tokens. Autoregressive modeling of these tokens captures both local dependencies (e.g., phonetics in speech, local melody in piano music) and global long-term structure  (e.g., language syntax and semantic content in speech; harmony and rhythm in piano music). However, these tokens lead to poor reconstruction. So, in addition to semantic tokens, we rely on fine-level acoustic tokens produced by a SoundStream neural codec, which capture the details of the audio waveform. We train a LM to generate both semantic and acoustic tokens.
    - When conditioned on a prefix (or prompt) of only 3 seconds of speech from a speaker not seen during training, AudioLM produces consistent continuations while maintaining the original speaker voice, prosody and recording conditions.
    - AudioLM is also suited for music generation.
    - We show that the semantic tokens and he acoustic tokens complement each other in terms of phonetic discriminability and reconstruction quality.

Casanova, E., Shulby, C., Korolev, A., Junior, A. C., Soares, A. d. S., Aluísio, S., & Ponti, M. A. (2022). ASR data augmentation in low-resource settings using cross-lingual multi-speaker TTS and cross-lingual voice conversion. arXiv, 2204.00618. Retrieved from https://arxiv.org/abs/2204.00618v5

    - We propose an ASR data augmentaiton method suitable using only a single real speaker in a target language.
    - We perform augmentations with cross-lingual multi-speaker speech synthesis and cross-lingual voice conversion (fig. 1).
    - We used the YourTTS model, a multilingual zero-shot multi-speaker text-to-speech model. Although the focus of the model is on TTS it can also do zero-shot voice conversion. For example, it was able to produce female voices even without being trained on female voices. Here, we fine-tuned the YourTTS model in English, pt-BR and ru-RU.

Chen, C., Hou, N., Hu, Y., Shirol, S., & Chng, E. S. (2022). Noise-robust Speech Recognition with 10 Minutes Unparalleled In-domain Data. arXiv, 2203.15321. Retrieved from https://arxiv.org/abs/2203.15321v1

    - We propose SimuGAN to simulate noisy spectrum from the clean spectrum.
    - In Simu-GAN, the generator maps the clean spectrum to the noisy spectrum and the discriminator distinguishes the simulated noisy spectrum from the real noisy spectrum. We also apply the multi-layer patch-wise contrastive loss to the generator (fig. 1). After SimuGAN training, only the generator is required to generate the simulated noisy data.
    - To train Simu-GAN, we utilize small amounts of clean/noisy data from the channel A from the RATS dataset.
    - We also propose a dual-path ASR system to improve the robustness of the ASR systems under noisy conditions. Specifically, we input the noisy speech generated by the Simu-GAN model and the corresponding clean speech as dual-path inputs into the conformer-based ASR system (fig. 2). We then reduce a KL divergence-based consistency loss between the two decoder outputs, as well as ASR losses for both outputs.

Chen, Z., Zhang, Y., Rosenberg, A., Ramabhadran, B., Moreno, P., Bapna, A., & Zen, H. (2022). MAESTRO: Matched Speech Text Representations through Modality Matching. arXiv, 2204.03409. Retrieved from https://arxiv.org/abs/2204.03409v2

    - We present a novel self-supervised modality matching algorithm Maestro. It can effectively use small amounts of transcribed speech data to unify representations learnt from massive amounts of untranscribed speech and unspoken text. The model can transfer to diverse downstream tasks such as ASR and Speech Translation.
    - When learning from unspoken text, speech-text alignment information is unavailable. Therefore, Maestro uses durations predicted from a duration prediction model in a fashion similar to speech synthesis (fig. 1).
    - We establish a new SOTA on VoxPopuli multilingual ASR.

Chen, Z., Bapna, A., Rosenberg, A., Zhang, Y., Ramabhadran, B., Moreno, P., & Chen, N. (2022). Maestro-U: Leveraging joint speech-text representation learning for zero supervised speech ASR. arXiv, 2210.10027. Retrieved from https://arxiv.org/abs/2210.10027v2

    - Zero-supervised speech ASR means learning ASR without the availability of any (in-language) transcribed resources. Most prior research either learns models for phoneme recognition (implicitly assuming an additional model for phoneme to grapheme conversion), or assumes the availability of grapheme to phoneme (G2P) models for text augmentation, but the construction of such models require as much human efforts as speech transcription.
    - We assume the availability of unlabeled speech and text in 102 languages, and the availability of supervised speech in 52 of these languages. Given these resources, we attempt to improve end-to-end ASR quality on the remaining 50 zero-supervised-speech languages. For unseen writing systems, we convert graphemes (text) into a common representation that is shared across all languages.
    - Out baseline Maestro model fails to perform well on this task (CER of 54.2% averaged over 50 languages).
    - We propose several improvements to the Maestro model, namely, the use of language embeddings and adapters, and use of byte level text representations. It results in a final zero supervised speech average CER of 30.8% (fig. 2). The training data, architecture and evaluation is shown in fig. 1.

Cheng, Y., Zhang, Y., Johnson, M., Macherey, W., & Bapna, A. (2022). Mu^2SLAM: Multitask, Multilingual Speech and Language Models. arXiv, 2212.09553. Retrieved from https://arxiv.org/abs/2212.09553v2

Chiu, C.-C., Qin, J., Zhang, Y., Yu, J., & Wu, Y. (2022). Self-supervised learning with random-projection quantizer for speech recognition. International Conference on Machine Learning. PMLR. Retrieved from https://proceedings.mlr.press/v162/chiu22a

    - We propose BERT-based Speech pre-Training with Random-projection Quantizer (BEST-RQ), a simple and effective self-supervised learning algorithm for speech recognition (fig. 1).
    - The algorithm masks speech signals and feeds them to the encoder, and the encoder learns to predict the masked region based on the unmasked speech signals where the learning targets are labels provided by a random-projection quantizer, which projects speech signals to a randomly initialized matrix, and finds a nearest vector in a randomly initialized codebook (neither the projection matrix nor the codebook is updated throughout the learning process).
    - On LibriSpeech the algorithm achieves similar results as previous work with non-streaming models, and provides better improvement with streaming models.
    - We study the relation between representation learning quality and the self-supervised learning quality, and demonstrate that the two objectives are not inherently aligned. Such an observation is central to our design. Our self-supervised learning algorithm eliminates the requirement of representation learning through applying a random-projection quantizer.
    - IMO, it is not clear what is meant under representation learning here, and why is it eliminated.

Choi, K., & Yeo, E. J. (2022). Opening the Black Box of wav2vec Feature Encoder. arXiv, 2210.15386. Retrieved from https://arxiv.org/abs/2210.15386v1

Conneau, A., Ma, M., Khanuja, S., Zhang, Y., Axelrod, V., Dalmia, S., ...Bapna, A. (2022). FLEURS: Few-shot Learning Evaluation of Universal Representations of Speech. arXiv, 2205.12446. Retrieved from https://arxiv.org/abs/2205.12446v1

    - We introduce FLEURS, the Few-shot Learning Evaluation of Universal Representations of Speech benchmark, to catalyze research in low-resource speech understanding.
    - FLEURS is suited for a variety of speech tasks including ASR, Speech-to-Text and Speech-to-Speech Translation, Speech LangID, and Multilingual Speech-to-Speech and Speech-to-Text Retrieval.
    - FLEURS contains n-way parallel speech and text in 102 languages with transcripts and strong quality control.
    - To collect data, we start with the dev and devtest sets from FLoRes-101 dataset for machine translation. It contains 2009 sentences extracted from English Wikipedia and these sentences have been translated in 101 languages by human translators.
    - For each sentence in the 102 languages we collected three recordings by three different native speakerss, imposing a balance in terms of sex ratio when possible. All recordings are kept as they-are, either from quiet or noisy environment.
    - Each recording is evaluated by additional workers, rejecting some recordings and leaving us between zero and three recordings per sentence in the final dataset. In the first version of the dataset, about 21.5% of the sentences are missing because none of the three recordings were validated.
    - Most other datasets are aligned at a document level with automatic segmentation and alignment for segments, but FLEURS initially contains short utterances.
    - For ASR baseline, we add two LSTM layer to fine-tune wav2vec-BERT, using a CTC loss. We do not include meta information of language identification labels in modeling, and there is no language model used for hypothesis scoring. Experiments show that fine-tuning from multimodal speech+text pre-training (mSLAM) is slightly worse than fine-tuning from speech-only pre-training (wav2vec-BERT). Most gains of multimodal pre-training are observed in groups with large amounts of unlabeled speech.

Conneau, A., Bapna, A., Zhang, Y., Ma, M., von Platen, P., Lozhkov, A., ...Johnson, M. (2022). XTREME-S: Evaluating Cross-lingual Speech Representations. arXiv, 2203.10752. Retrieved from https://arxiv.org/abs/2203.10752v3

    - We introduce XTREME-S, a new benchmark to evaluate universal cross-lingual speech representations. It covers 102 languages from 10+ language families, 3 different domains and includes 7 downstram tasks divided into 4 different task families: recognition, translation, classification and retrieval (fig. 1). Test sets are available in open-source and are not hidden to the public.
    - XTREME-S also includes FLEURS, a recently introduced general-purpose multilingual evaluation dataset.
    - The training sets of XTREME-S range from a few hours to a few hundred hours of labeled data per language. This is a few-shot setting suited for low-resource understanding.
    - For ASR, we use three datasets: Fleurs, MLS and VoxPopuli, which cover more than 100 languages. In Fleurs ASR, multilingual fine-tuning is used and "unit error rate" (characters, signs) of all languages is averaged. In MLS ASR, multilingual fine-tuning on all languages is also used. The first baseline is a 600M wav2vec-BERT model trained on 429k unlabeled data in 51 languages from VoxPopuli, MLS, CommonVoice and BABEL, similar to XLS-R. The second is the 600m parameter mSLAM model.

Dunbar, E., Hamilakis, N., & Dupoux, E. (2022). Self-supervised language learning from raw audio: Lessons from the Zero Resource Speech Challenge. arXiv, 2210.15759. Retrieved from https://arxiv.org/abs/2210.15759v1

Fan, R., & Alwan, A. (2022). DRAFT: A Novel Framework to Reduce Domain Shifting in Self-supervised Learning and Its Application to Children's ASR. arXiv, 2206.07931. Retrieved from https://arxiv.org/abs/2206.07931v1

    - Including target domain data might not be feasible at the pretraining stage.
    - We propose domain responsible adaptation and finetuning (DRAFT): a three-stage (pretraining, adaptation, and finetuning) training paradigm to reduce domain shifting in pretrained speech models through an additional adaptation stage (fig. 1).
    - In DRAFT, residual adapters (RAs) are placed between blocks in the transformer and are responsible for learning domain specific information during an additional adaptation stage. The additional adaptation stage trains the model with finetuning data and with the same SSL loss that was used in the pretraining stage. To prevent catastrophic forgetting of the learned knowledge from source domain data, only RA parameters are updated during the adaptation stage.
    - DRAFT is agnostic to the type of SSL method used and is evaluated with APC, Wav2vec2.0, and HuBERT.
    - When performing DRAFT on SSL-pretrained speech models (trained with adult speech data) for child ASR tasks, we obtain significant improvements over baselines without adaptation for both causal (pretrained with APC) and noncausal transformers (pretrained with Wav2vec2.0 or HuBERT).
    - There is another similar paper (another version?) called "Towards Better Domain Adaptation for Self-Supervised Models: A Case Study of Child ASR".

Ferreira, A. I. S., & Oliveira, G. d. R. (2022). Domain Specific Wav2vec 2.0 Fine-tuning For The SE&R 2022 Challenge. arXiv, 2207.14418. Retrieved from https://arxiv.org/abs/2207.14418v1

    - This paper presents our efforts to build a robust ASR model for SE&R 2022 challenge, considering prepared and spontaneous speech in different Portuguese dialects.
    - While most of the available public datasets are composed of prepared speech, the domain of real ASRs are far more complex, mainly because it is formed by spontaneous speech and different speech dialects. Also, most of ASR use cases involve high noise environments or low recording equipment, which is not adressed in most of the public datasets available.
    - We fine-tune Wav2vec 2.0 XLSR-53 (that is considered the state-of-the-art in Brazilian Portuguese) using only public available Portuguese datasets. We used selective noise insertion and audio normalization during training.
    - Overall, our models did not show a huge improvement in performance when compared to the baseline model.

Fu, L., Li, S., Li, Q., Deng, L., Li, F., Fan, L., ...He, X. (2022). UFO2: A unified pre-training framework for online and offline speech recognition. arXiv, 2210.14515. Retrieved from https://arxiv.org/abs/2210.14515v2

    - ASR systems are typically categorized in: 1) online mode (a.k.a. streaming), which is developed to emit each hypothesized word as quickly and accurately as possible when it is spoken, and 2) offline mode, which aims to accurately emit the complete hypotheses after processing a full utterance. Self-supervised pre-training is usually offline, i.e. each represented feature is conditioned on the full-context inputs. When online ASR model is build upon SSL pre-training, the performance might be hindered due to the mode inconsistency between the pre-training and fine-tuning.
    - We propose a Unified pre-training Framework for Online and Offline (UFO2) Automatic Speech Recognition.
    - UFO2 unifies the online and offline modes into a single model. We apply 4 strategies on the feature extraction and training objectives. The full-context MHSA extracts offline-mode features (conditioned on the complete utterance). Simultaneously, the dynamic-chunked MHSA mimics different latency ranges for online-mode learning. The online and offline representation models share all the encoder and quantizer weights. Stop gradient is operated to decouple the impact of the online-mode objectives to the quantizer. The online and offline objectives are aggregated.
    - The proposed UFO2 significantly enhances the performance compared to the baseline methods on the LibriSpeech dataset. However, the performance in online mode still underperforms the offline mode.

Gat, I., Kreuk, F., Nguyen, T. A., Lee, A., Copet, J., Synnaeve, G., ...Adi, Y. (2022). Augmentation Invariant Discrete Representation for Generative Spoken Language Modeling. arXiv, 2209.15483. Retrieved from https://arxiv.org/abs/2209.15483v2

    - We consider a generative spoken language modeling task (GSLM, language modeling from audios recordings only). Such speech LMs usually operate over discrete units obtained from quantizing internal representations of self-supervised models.
    - We focus on measuring and improving the robustness of discrete input representations for GSLM, with respect to variations such as time-stretch, pitch-shift, additive-noise, and reverberation.
    - We propose Unit Edit Distance for evaluating the robustness.
    - We show the lack of robustness of GSLM models (such as HuBERT).
    - We propose a method for learning robust discrete representation on top of any speech SSL model. We forward a clean signal through an encoder followed by a pre-trained quantizer (k-means). Next, we forward an augmented signal through the same encoder, followed by a new quantizer. The CTC loss between the deduplicated output of the clean signal and the output of the augmented signal is used to learn the parameters of the new quantizer (fig. 3)
    - We evaluate the proposed method using the standard GSLM setup, i.e., ABX, sWUGGY, sBLIMP.

Gris, L. R. S., Junior, A. C., Santos, V. G. d., Dias, B. A. P., Leite, M. Q., Svartman, F. R. F., & Aluísio, S. (2022). Bringing NURC/SP to Digital Life: the Role of Open-source Automatic Speech Recognition Models. arXiv, 2210.07852. Retrieved from https://arxiv.org/abs/2210.07852v1

Inaguma, H., Popuri, S., Kulikov, I., Chen, P.-J., Wang, C., Chung, Y.-A., ...Pino, J. (2022). UnitY: Two-pass Direct Speech-to-speech Translation with Discrete Units. arXiv, 2212.08055. Retrieved from https://arxiv.org/abs/2212.08055v2

Millet, J., Caucheteux, C., Orhan, P., Boubenec, Y., Gramfort, A., Dunbar, E., ...King, J.-R. (2022). Toward a realistic model of speech processing in the brain with self-supervised learning. arXiv, 2206.01685. Retrieved from https://arxiv.org/abs/2206.01685v2

    - We compare wav2vec 2.0 to the brain activity of 412 English, French, and Mandarin individuals, while they listened to approximately one hour of audio books.
    - To quantify the similarity between the network’s activations X and the brain recordings Y, linear mapping is fitted to predict the brain response Y given X.
    - We show that wav2vec 2.0 learns brain-like representations with as little as 600 hours of unlabelled speech – a quantity comparable to what infants can be exposed to during language acquisition. Low-level brain areas (A1, A2) are best predicted by the first transformer layers, higher level areas (IFG, STS) are best predicted by deeper layers. Remarkably, this hierarchy extends to supplementary motor and motor areas in both hemispheres.
    - The auditory-, speech-, and language-specific representations learned by the model converge to those of the human brain.
    - While our analyses suggest that learning allows wav2vec 2.0 to capture some lexical features in its deep layers, it remains unclear whether these layers also capture complex syntactic structures, such as recursive syntactic trees.

Nguyen, T. A., Sagot, B., & Dupoux, E. (2022). Are discrete units necessary for Spoken Language Modeling? arXiv, 2203.05936. Retrieved from https://arxiv.org/abs/2203.05936v2

    - Spoken language modeling usually relies on transforming the audio into a sequence of discrete units and then training a language model directly on such pseudo-text. Is such a discrete bottleneck necessary (fig. 1)?
    - We show experimentally that discretization is beneficial for spoken language modeling. We show that discretization disentangles linguistic information from non-linguistic signals, forcing the transformer to focus on linguistic ones. But we can get rid of discrete bottlenecks by using low-level continuous inputs so long as we still use discrete targets. When there are no discrete units at all, the LM performances are still limited, even if using a NCE loss could help a bit with syntactic and semantic metrics.
    - On the basis of this study, we train a language model on the discrete units of the HuBERT features, reaching new SOTA results in the lexical, syntactic and semantic metrics of the Zero Resource Speech Challenge 2021.

Pasad, A., Shi, B., & Livescu, K. (2022). Comparative layer-wise analysis of self-supervised speech models. arXiv, 2211.03929. Retrieved from https://arxiv.org/abs/2211.03929v3

    - The authors extends the previous work "Layer-wise Analysis of a Self-supervised Speech Representation Model" that focused on wav2vec 2.0 to 11 pre-trained models. All the models in this work use a masking-based pretext task, thus using both left and right context to recover the masked segment (target).
    - As in our previous work, we use projection-weighted canonical correlation analysis (PWCCA) that returns a correlation-based measure given N pairs of vectors. Using PWCCA, we measure the similarity between layer representations and various variables of interest: mel filter bank features (acoustic information), one-hot encoded phone labels, and one-hot encoded word labels.
    - CCA similarity with local features (CNN outputs) are shown in fig. 1. Some models have a clear autoencoderstyle pattern, i.e., high similarity with the input for the initial and final layers and a drop in similarity for the middle layers. This behavior is most prominent in models trained to recover (in some sense) the local features.
    - Fig. 2, 3, 4 show the layerwise spectrogram, phonetic and word similarity respectively. Models that have a strong autoencoder-style dynamic (W2V2, XLSR-53, and FaST-VGS+) tend to have a peak in both phonetic and word content in one or more of the intermediate layers. hese models have the same masking-based contrastive loss that recovers the local features. For the models which are trained to predict discrete units learned in an intermediate layer (HuBERT, WavLM, AV-HuBERT) the phonetic and word information appears to be concentrated toward higher layers.
    - We also measure the performance of individual layers on downstream tasks, using a prediction model on top of the frozen representations, trained on labeled data for the task. Do layers with high property content also perform better on downstream tasks that are expected to benefit from that property? PR and ASR performance are well-correlated with both CCA-phone and CCA-word scores. Semantic task (SLURP-action, SLURP-scenario, and IC) performance is much more correlated with CCA-word than with CCA-phone. The best-performing layer isalways lower than at least the top two layers and is close to the layers observed to have the most phonetic and word-level content as measured by CCA.
    - Evaluating each individual layer to find the best-performing layer is much more computationally demanding than evaluating CCA. Our findings suggest using CCA-word/CCAphone to narrow down the choice of layers.
    - We compare the best layer performance to the task performance using a learnable weighted sum of all layers . In general, they perform on par (fig. 7). It is natural to ask whether the layer weights learned in all-layers experiments are themselves a good indicator of usefulness for downstream tasks. We find that the mean rank correlation between layer weights and task performance is much lower than the mean rank correlation between CCA-word and task performance. So, layer weights are relatively unreliable.

Pham, N.-Q., Waibel, A., & Niehues, J. (2022). Adaptive multilingual speech recognition with pretrained models. arXiv, 2205.12304. Retrieved from https://arxiv.org/abs/2205.12304v1

    - We show that using multilingual pretrained acoustic (wav2vec 2.0) and language (MBART50) models for the encoder and decoder respective brings a large improvement for seq2seq models for ASR. Encoder pretraining is more impactful for languages with higher resources while the decoder counterpart is more effective for languages with medium-low resources. Surprisingly, many languages with extremely low resource (less than 5 hours) do not benefit much from this combination.
    - The language specific modulation techniques such as language adapters and factorized adaptive weights have a strong impact on especially the low-resource languages mentioned above.
    - We enhance the encoder with either content bias and position bias to self-attention, or stacking MBART50 encoder on top of the wav2vec encoder (without any length conversion) and achieve more improvements in WER. We hypothesize that the latter helps the cross-attention layers because they are familiar with the output of the text encoder during training.

Radford, A., Kim, J. W., Xu, T., Brockman, G., McLeavey, C., & Sutskever, I. (2022). Robust Speech Recognition via Large-Scale Weak Supervision. arXiv, 2212.04356. Retrieved from https://arxiv.org/abs/2212.04356v1

    - Current SSL audio encoders such as Wav2Vec 2.0 lack an equivalently performant decoder mapping those representations to usable outputs, necessitating a finetuning stage in order to actually perform a task such as speech recognition. An additional risk with fine-tuning is learning brittle and spurious patterns that don’t generalize to other datasets.
    - As demonstrated earlier, multi-domain pre-training yields higher robustness and generalize much more effectively to held-out datasets than models trained on a single source. However, there is still only a moderate amount of this data easily available.
    - We release Whisper, an encoder-decoder Transformer for speech recognition, with a small stem of two convolution layers, and byte-level BPE text tokenizer (fig. 1), trained on many different tasks, including multilingual speech recognition, speech translation, spoken language identification, and voice activity detection.
    - Whisper was trained on 680,000 hours of weakly supervised audio data (including 117,000 hours or recordings on 96 languages other than English), splitted into 30-second segments. We construct the dataset from audio that is paired with transcripts on the Internet. This results in a very diverse dataset covering a broad distribution of audio from many different environments, recording setups, speakers, and languages. When collecting dataset, we developed several automated filtering methods to improve transcript quality.
    - Many transcripts on the internet are not actually human-generated but the output of existing ASR systems. It was shown that training on mixed human and machine-generated data can impair the performance of translation systems. We developed many heuristics to detect and remove machine-generated transcripts from the training dataset. For example, an all-uppercase or all-lowercase transcript is very unlikely to be human generated.
    - For an additional filtering pass, after training an initial model we manually inspect data sources with high WER. This showed a large amount of only partially transcribed or poorly aligned/misaligned transcripts as well as remaining low-quality machine-generated captions that filtering heuristics did not detect.
    - We train on all audio, including segments where there is no speech.
    - We use multi-tasking with a simple format to specify a task and conditioning information as a sequence of input tokens to the decoder. First, we predict the language being spoken which is represented by a unique token for each language in our training set (99 total), or <|nospeech|> token. The next token specifies the task with an <|transcribe|> or <|translate|> token. After this, we specify whether to predict timestamps or not by including a <|notimestamps|> token. At this point, the task and desired format is fully specified, and the output begins.
    - With some probability we also condition on the history of text of the transcript (in the hope that it will learn to use longer-range text context to resolve ambiguous audio). We add the transcript text preceding the current audio segment to the decoder’s context. We indicate the beginning of prediction with a <|startoftranscript|> token. We only mask out the training loss over the previous context text, and train the model to predict all other tokens.
    - During early development we observed that Whisper transcribes plausible but almost always incorrect guesses for the names of speakers. This happens because many transcripts in the pre-training dataset include the name of the person who is speaking. To avoid this, we fine-tune Whisper models briefly on the subset of transcripts that do not include speaker annotations which removes this behavior.
    - The models are trained for 2^20 updates which is between two and three passes over the dataset. Due to low number of epochs, we do not use any data augmentation or regularization. After the original release of Whisper, we trained an additional Large model (denoted V2) for 2.5X more epochs while adding SpecAugment, Stochastic Depth and BPE Dropout.
    - Systems that output transcripts that would be judged as correct by humans can still have a large WER due to innocuous differences in transcript style. This is particulary important for zero-shot evaluation, when the model do not observe any examples of specific datasets transcript formats. For several datasets, we observe WER drops of up to 50 percent usually due to a quirk such as a dataset’s reference transcripts seperating contractions from words with whitespace. We opt to address this problem with extensive standardization of text before the WER calculation. Our text normalizer was developed through iterative manual inspection. We caution this development procedure comes at a risk of overfitting to the transcription style of Whisper models. We are releasing the code for our text normalizer.
    - Following the paper "Measuring Robustness to Natural Distribution Shifts in Image Classification", we examine 1) overall robustness (average performance across many datasets; here we use a suite of 12 other academic speech recognition datasets), and 2) effective robustness (the difference in performance between the reference dataset and out-of-distribution dataset; we use LibriSpeech as reference dataset due to its central role in modern ASR).
    - We evaluate Whisper in a zero-shot setting without using any of the training data from test distributions.
    - We compare Whisper models with both human performance and standard fine-tuned machine learning models.
    - Even the smallest zero-shot Whisper model, which has only 39 million parameters and a 6.7 WER on LibriSpeech test-clean, is roughly competitive with the best supervised LibriSpeech model when evaluated on other datasets (fig. 2, table 2). The best zero-shot Whisper models roughly match human accuracy and robustness.
    - So, our key finding is that multi-domain training increases robustness. This finding has been replicated across many fields in addition to speech recognition including NLP and CV.
    - As for multi-lingual speech recognition, Whisper performs well on Multilingual LibriSpeech (MLS) in a zero-shot setting. We do use a simple text standardizer for this result which prevents direct comparison or claims of SOTA performance. On VoxPopuli, however, Whisper significantly underperforms prior work (while in prior work models were trained on VoxPopuli data, which is 10 times larger than MLS). However, these two benchmarks are somewhat narrow since they only include 15 unique languages, so we also report performance on the Fleurs dataset. We find a strong squared correlation coefficient of 0.83 between the log of the WER and the log of the amount of training data per language (fig. 3): WER halves for every 16× increase in training data. WER also can be poor when BPE tokenizer is a poor match for some languages.
    - We study the translation capabilities of Whisper models by measuring their performance on the X→en subset of CoVoST2. We achieve a new state of the art of 29.1 BLEU zero-shot without using any of the CoVoST2 training data.
    - We also evaluate Whisper on language identification.
    - We also measure robustness to Additive Noise on LibriSpeech dataset. When either white noise or pub noise from the Audio Degradation Toolbox (Mauch & Ewert, 2013) was added to the audio. All other (LibriSpeech-trained) models quickly degrade as the noise becomes more intensive, performing worse than the Whisper model under additive pub noise of signal-to-noise ratio (SNR) below 10 dB. his showcases Whisper’s robustness to noise.
    - Whisper models are trained on 30-second audio chunks and cannot consume longer audio inputs at once, which presents challenges in real-world applications. We developed Long-form Transcription: a strategy to perform buffered transcription of long audio by consecutively transcribing 30-second segments of audio and shifting the window according to the timestamps predicted by the model. It is crucial to have beam search and specific temperature scheduling for this to perform good. We use beam search with 5 beams using the log probability as the score function, to reduce repetition looping which happens more frequently in greedy decoding. We start with temperature 0, i.e. always selecting the tokens with the highest probability, and increase the temperature by 0.2 up to 1.0 when either the average log probability over the generated tokens is lower than −1 or the generated text has a gzip compression rate higher than 2.4.
    - We also specifically compare with human performance. Whisper’s English ASR performance is not perfect but very close to human-level accuracy.
    - We also trained a series of medium-sized models on subsampled versions of the dataset which are 0.5%, 1%, 2%, 4%, and 8% of the full dataset size, with early stopping based on the validation loss, and taking EMA of the parameters. We see significant variability in improvement rates across tasks and sizes. For English ASR, fter 50K hours the diminishing returns observed, that could be explained by saturation effects when approaching human-level performance. For X -> en translation, performance is practically zero when training on 7,000 hours of audio or less and then follows a roughly log-linear improvement trend till 54,000 hours before also showing diminishing returns. Overall results could suggest that the current best Whisper models are under-trained relative to dataset size and performance could be further improved by a combination of longer training and larger models. Also, we are nearing the end of performance improvements from dataset size scaling for speech recognition.
    - We also compared the performance of models trained on just English ASR with our standard multitask and multilingual training setup. We adjust for the amount of FLOPs spent training on the task of English ASR as only 65% of compute is spent on this task in a joint training setup. For small models trained with moderate amounts of compute, there is indeed negative transfer between tasks and languages (that is, multitasking harms). However, for our largest experiments  multitask and multilingual models outperform their English-only counterparts, demonstrating positive transfer from other tasks.
    - Many remaining errors, particularly in long-form transcription, includes problems such as getting stuck in repeat loops, not transcribing the first or last few words of an audio segment, or complete hallucination where the model will output a transcript entirely unrelated to the actual audio. We suspect fine-tuning Whisper models on a high-quality supervised dataset and/or using reinforcement learning to more directly optimize for decoding performance could help further reduce these errors.
    - Another clear route for improvement is increasing the amount of data for rarer languages.
    - While we studied only zero-shot transfer performance, it is likely that results can be improved further by fine-tuning. It also allows for direct comparisons with prior work since it is a much more common evaluation setting.
    - It’s currently unclear to what degree the benefits of Whisper stem from training its encoder, decoder, or both. This could be studied by either ablating various design components of Whisper, such as training a decoder-less CTC model, or by studying how the performance of existing speech recognition encoders such as wav2vec 2.0 change when used together with a language model. It is also possible that the results could be further improved by incorporating unsupervised pre-training.

Rakib, M., Hossain, Md. I., Mohammed, N., & Rahman, F. (2022). Bangla-Wave: Improving Bangla Automatic Speech Recognition Utilizing N-gram Language Models. arXiv, 2209.12650. Retrieved from https://arxiv.org/abs/2209.12650v1

Shahgir, H. A. Z. S., Sayeed, K. S., & Zaman, T. A. (2022). Applying wav2vec2 for Speech Recognition on Bengali Common Voices Dataset. arXiv, 2209.06581. Retrieved from https://arxiv.org/abs/2209.06581v1

Shim, K., & Sung, W. (2022). A Comparison of Transformer, Convolutional, and Recurrent Neural Networks on Phoneme Recognition. arXiv, 2210.00367. Retrieved from https://arxiv.org/abs/2210.00367v1

Tang, Y., Gong, H., Dong, N., Wang, C., Hsu, W.-N., Gu, J., ...Pino, J. (2022). Unified Speech-Text Pre-training for Speech Translation and Recognition. arXiv, 2204.05409. Retrieved from https://arxiv.org/abs/2204.05409v1

Tjandra, A., Singhal, N., Zhang, D., Kalinli, O., Mohamed, A., Le, D., & Seltzer, M. L. (2022). Massively Multilingual ASR on 70 Languages: Tokenization, Architecture, and Generalization Capabilities. arXiv, 2211.05756. Retrieved from https://arxiv.org/abs/2211.05756v1

    - A problem in multilingual ASR: when we scale up the number of languages, vocabulary size grows.
    - We explore large-scale multilingual on 70 languages with 150,000 hours dataset (fig. 2).
    - We use an end-to-end Transducer model that is composed by encoder (CNN + Transformer), predictor, and joiner modules. We investigate two types of Transducer models: (1) shared input embedding and output architecture, (2) language-specific multiple input embedding, and output linear architecture (fig. 1). We also try subword tokenization.
    - We evaluate our model with test data on two different domains for every language: vid-clean and vid-noisy (more acoustically challenging).
    - Shared character strategy shows inferior results compared to language-specific inputs and outputs.
    - One advantage of this language-specific architecture is that we could represent the same token between different languages with different embedding and weight matrices. Thus, we could disambiguate characters and subwords that look the same in the written space but sound different.
    - Shared character tokenization lead to subpar performance, maybe due to the high variation of decoding timestep between different languages. We show that by minimizing the variance of decoding steps between languages with clever combinations between subwords and characters, we could significantly improve our multilingual result.
    - Our multilingual model could generalized well on the new dataset (MLS). We achieve 9.5% WER on zero-shot and 7.5% WER after finetuning, that is competitive with SOTA performance.
    - We plan to scale up the amount of training data by adding pseudo-labeling pipeline in every language.

van der Merwe, W., Kamper, H., & Preez, J. d. (2022). A Temporal Extension of Latent Dirichlet Allocation for Unsupervised Acoustic Unit Discovery. arXiv, 2206.11706. Retrieved from https://arxiv.org/abs/2206.11706v2

Wirth, J., & Peinl, R. (2022). ASR in German: A Detailed Error Analysis. arXiv, 2204.05617. Retrieved from https://arxiv.org/abs/2204.05617v1

    - In ASR, WER or CER metrics do not provide any insight into the nature or impact of the errors.
    - We evaluate ASR models pretrained on the German language on diverse test datasets.
    - Conformer Transducer outperforms all other models (including Wav2Vec 2.0, Conformer CTC etc.) regarding WER.
    - The lowest difference between average and median (over datasets) we count as an indicator for robustness.
    - Wav2Vec 2.0 does exceptionally bad for German TED and ALC. We hypothesize that this stems from filling words like “äh” and “hm” occurring in their output transcript predictions, which, in contrast, are omitted in the predictions of all other models. These fillers are also missing in all ground truth transcripts.
    - 1. Negligible Errors (9%). These are different forms of otherwise correct transcript predictions, like nonnormalized abbreviations such as “et cetera” or “etc.”
    - 2. Minor Errors (noncontext-breaking) (12%). Models which were trained on less German data often produce transcript predictions with redundant letters, omit single letters or predict hard instead of soft vowels and vice versa (e.g., confusion between d and t) without distorting the meaning. These spelling errors can usually be corrected when utilizing a language model. While increasing CER only slightly, WER quickly rises to unrealistic values if these minor errors are included.
    - 3. Major Errors (context-breaking) (19%). These are fully incorrectly transcribed or omitted words and omitted or inserted letters that change the meaning of a transcript or exclude necessary information. These errors were further divided into subgroups and their causes traced back to systematic errors within the training data.
    - 3.1. Naive Normalization, such as years and large integers: "One Thousand Nine Hundred Sixty-Three" instead of "Nineteen Sixty-Three", and pronounced punctuation marks such as "comma". Here models trained on one type of dataset will generate errors when evaluated with the other type of dataset.
    - 3.2. Various problems within datasets, such as pronounced "weil" (because) is transcribed in the ground truth as "denn" (since), or certain terms such as "paragraph" (article) were transcribed as "ziffer" (subparagraph) within the ground truth transcripts. As a result, models trained on this wrong data consistently predicted these spoken words. Additionally, for poorly edited audio inputs (starting in the middle of a sentence or word), models predicted sentence beginnings that were not present in the audio, and in most cases turned out to be incorrect. This type of error fits into the category "hallucinations".
    - 4. Names, Loan Words and Anglicisms (20%) are a commonly occurring types of errors, since names can often be spelled in several ways and only a small fraction of common names is usually found in training datasets. Foreign words are strongly domain dependent, and some anglicisms have homophonic pronunciations to German phonemes. Anglicisms are closely related to code-switching ASR.
    - 5. Homophones (3%) occur when contextual information is lost, but the underlying phonemes were in general correctly interpreted by the system, e.g., “Graph” and “Graf” (a noble title).
    - 6. Flawed Ground Truth Transcripts (18%). These cases may be caused by incorrect normalization of numbers and symbols, and strong deviations in the alignment process during the automated creation of data sets.
    - 7. Ambiguous Audio Input (11%), when certain words are be pronounced or perceived indistinctly. Here even human listeners were unable to produce clear and correct transcripts from audio recordings, because the pronunciation was too unclear. This could be traced back to speakers having learned German as a second language, multiple German dialects, simple pronunciation errors and slips of the tongue. Increasing the robustness of ASR systems for non-native speakers and dialects is an ongoing research topic.
    - 8. Flawed Audio Input (8%) with cutoffs of spoken words at the beginning or end of audio snippets.
    - We propose sevaral solutions: verification of (correct) normalization, extension of vocabulary through text to speech, training on phoneme vocabulary, audio preprocessing.

Zhang, X., Tan, H., Huang, X., Zhang, D., Tang, K., & Gu, Z. (2022). Adversarial Attacks on ASR Systems: An Overview. arXiv, 2208.02250. Retrieved from https://arxiv.org/abs/2208.02250v1

Zhang, Z., Chen, S., Zhou, L., Wu, Y., Ren, S., Liu, S., ...Wei, F. (2022). SpeechLM: Enhanced Speech Pre-Training with Unpaired Textual Data. arXiv, 2209.15329. Retrieved from https://arxiv.org/abs/2209.15329v3

    - We propose SpeechLM, a cross-modal Speech and Language Model to explicitly malign speech and text pre-training with a predefined unified discrete representation. We introduce two alternative discrete tokenizers to bridge the speech and text modalities, including phoneme-unit and hidden-unit tokenizers, which can be trained using a small amount of paired speech-text data.
    - We propose two pre-training tasks. One is Unit-based Masked Language Modeling (UMLM) trying to predict the unit tokens from the masked speech. The other one is Unit-based Connectionist Temporal Classification (UCTC) task, aiming at reconstructing the whole text sequences from the masked unit sequences. To better align the representations of speech and text, we also adopt a Random Swapping Mechanism for the UMLM task.
    - SpeechLM enhanced by textual data significantly outperforms its speech-only counterparts on various spoken language tasks, e.g., ASR, speech translation (ST), and universal representation evaluation framework SUPERB.

Zhang, Z., Zhou, L., Ao, J., Liu, S., Dai, L., Li, J., & Wei, F. (2022). SpeechUT: Bridging Speech and Text with Hidden-Unit for Encoder-Decoder Based Speech-Text Pre-training. arXiv, 2210.03730. Retrieved from https://arxiv.org/abs/2210.03730v1

    - For the cross-modal speech-to-text models, a key problem is how to naturally connect the speech encoder and the text decoder. An intermediate hidden-unit representation (such as one in HuBERT) can be the bridge between modalities.
    - We propose a unified speech-unit-text pre-training method (SpeechUT) that decouples the speech-to-text model into speech-to-unit and unit-to-text models (fig. 2), to take advantage of a large amount of unpaired speech and text data for pre-training.
    - SpeechUT connects the speech encoder and the text decoder by the unit encoder.
    - In the pre-training stage, SpeechUT performs multi-task pre-training with the following tasks. The first is the speech-to-unit objective similar to HuBERT, where the model needs to predict the unit of the masked positions based on the non-mask regions in a speech sequence. The second is the unit-to-text task is performed as a regular encoder-decoder based sequence-to-sequence task, conditioned on the output of the unit encoder; we also formulate a joint CTC objective which directly predicts the target text sequence from the unit encoder. Note that in S2U and U2T tasks, the unit serves as the target and the input, respectively. Finally, to enhance the unit-in, unit-out property, SpeechUT performs an additional masked unit modeling (MUM) task, with the training data combining all the units in S2U and U2T tasks. To better align the speech and unit representations in the unit encoder, we adopt a simple embedding mixing mechanism for S2U task, which is to mix the embeddings of two modalities in one sequence.
    - To obtain training data, we need to construct the speech-unit paired data, and the unit-text paired data. We introduce the speech-to-unit (S2U) and text-to-unit (T2U) offline generators. SpeechUT employs a small amount of paired ASR data to train the T2U generator.
    - After pre-training, we drop the unit pre-net and stack the speech encoder, the unit encoder and the text decoder into a complete sequence-to-sequence model, which can be fine-tuned for any speech-to-text task, such as ASR and ST.
    - According to our ablation studies, the embedding mixing mechanism has the biggest impact, and the CTC loss, as a part of the U2T task, has a minor influence on the fine-tuning performance. The MUM loss has the minimum effect, so we speculate that the U2T task has already modeled the unit well.
    - We acheieve SOTA on speech recognition and speech translation tasks.

Zhao, J., & Zhang, W.-Q. (2022). Improving Automatic Speech Recognition Performance for Low-Resource Languages With Self-Supervised Models. IEEE J. Sel. Top. Signal Process., 16(6), 1227–1241. doi: 10.1109/JSTSP.2022.3184480

Zhu, Q.-S., Zhang, J., Zhang, Z.-Q., Wu, M.-H., Fang, X., & Dai, L.-R. (2022). A Noise-Robust Self-supervised Pre-training Model Based Speech Representation Learning for Automatic Speech Recognition. arXiv, 2201.08930. Retrieved from https://arxiv.org/abs/2201.08930v1

    - We observe that wav2vec2.0 pre-trained on noisy data brings a performance degradation on the clean test set.
    - We propose a modified pre-training scheme: the noisy speech and clean speech are sent into a shared feature encoder, and the noisy feature is input to the transformer encoder, while the clean feature is fed to the vector-quantization (VQ) module, which provides clean training targets for the transformer encoder.
    - To synthetize noisy data, we randomly select noise samples and mix with the clean speech at different SNRs.
    - The resulting model achieves a much better performance on noisy data at the cost of a tiny performance sacrifice on the clean test set.

Zuluaga-Gomez, J., Prasad, A., Nigmatulina, I., Sarfjoo, S., Motlicek, P., Kleinert, M., ...Zhan, Q. (2022). How Does Pre-trained Wav2Vec 2.0 Perform on Domain Shifted ASR? An Extensive Benchmark on Air Traffic Control Communications. arXiv, 2203.16822. Retrieved from https://arxiv.org/abs/2203.16822v2

    - We analyze the robustness of Wav2Vec 2.0 and XLS-R models on downstream ASR for a completely unseen domain: air traffic control communications (voice communications between air traffic controllers and pilots) with SNR ratio between 5 to 20 dB.
    - Pre-trained Wav2Vec 2.0 models allow rapid fine-tuning with small quantities of adaptation data. Our experiments show large recognition improvements of Wav2Vec 2.0 and XLS-R compared to hybrid-based ASR baselines.
    - Large-scale speech models perform systematically better on female recordings, especially after fine-tuning.

Attia, A. A., Liu, J., Ai, W., Demszky, D., & Espy-Wilson, C. (2023). Kid-Whisper: Towards Bridging the Performance Gap in Automatic Speech Recognition for Children VS. Adults. arXiv, 2309.07927. Retrieved from https://arxiv.org/abs/2309.07927v3

Bain, M., Huh, J., Han, T., & Zisserman, A. (2023). WhisperX: Time-Accurate Speech Transcription of Long-Form Audio. arXiv, 2303.00747. Retrieved from https://arxiv.org/abs/2303.00747v2

    - We propose WhisperX, a system for efficient speech transcription of long-form audio with accurate word-level timestamps. It consists of 3 additional stages to Whisper transcription: (i) pre-segmenting the input audio with an external Voice Activity Detection (VAD) model; (ii) cut and merging the resulting VAD segments into approximately 30 seconds input chunks with boundaries lying on minimally active speech regions enabling batched whisper transcription; and finally (iii) forced alignment with an external phoneme model to provide accurate word-level timestamps.
    - We demonstrate SOTA performance on long-form transcription and word segmentation benchmarks.
    - Pre-segmenting audio enables a 12x transcription speedup via batched inference.

Do, A., Brown, O., Wang, Z., Mathew, N., Liu, Z., Ahmed, J., & Yu, C. (2023). Using fine-tuning and min lookahead beam search to improve Whisper. arXiv, 2309.10299. Retrieved from https://arxiv.org/abs/2309.10299v1

    - We compare the following fine-tuning strategies for Whisper-Tiny on Vietnamese low-resource language: 1) full-paremeter, 2) full-paremeter with decoupling input an output embedding layers, 3) fine-tuning decoder only, 4) fine-tuning decoder embedding layer only, 5) fine-tuning decoder embedding layer only with decoupling input an output embedding layers, 6) LoRA with decoupling input an output embedding layers and different hyperparameters.
    - We fine-tune on our collected dataset and test on FLEURS and CommonVoice 9 (for Vietnamese?)
    - Fine-tuning both the audio encoder and text decoder maximises performance. Applying high-rank LoRA leads to the greatest model improvement, with less than half the number of trainable parameters compared to full-parameter fine-tuning with decoupling embedding. Decoupling input an output embedding layers also improves performance.
    - We suggest Filter-Ends and Min Lookahead as improvements to Whisper’s decoding algorithm. We prove that Min Lookahead is expected to outperform standard beam search.

Fathullah, Y., Wu, C., Lakomkin, E., Jia, J., Shangguan, Y., Li, K., ...Seltzer, M. (2023). Prompting Large Language Models with Speech Recognition Abilities. arXiv, 2307.11795. Retrieved from https://arxiv.org/abs/2307.11795v1

Gandhi, S., von Platen, P., & Rush, A. M. (2023). Distil-Whisper: Robust Knowledge Distillation via Large-Scale Pseudo Labelling. arXiv, 2311.00430. Retrieved from https://arxiv.org/abs/2311.00430v1

    - We leverage pseudo-labelling to distill the Whisper model into a smaller variant, called Distil-Whisper. The encoder is entirely copied from the teacher to the student and frozen during training. The student’s decoder consists of only two decoder layers, which are initialised from the first and last decoder layer of the teacher, so the distilled model is 5.8 times faster with 51% fewer parameters. The model is trained on a weighted sum of 1) KL divergence with teacher output distribution, and 2) next token prediction of pseudo labels.
    - We found there to be little difference in the downstream performance of the distilled model after pseudo-labelling using either greedy or beam-search, and so we opted to pseudo-label the training data with greedy decoding for its faster inference speed.
    - By sharing the same encoder weights as Whisper, Distil-Whisper is designed to be paired with Whisper for speculative decoding, yielding a 2 times speed-up while ensuring the same outputs as the original model.
    - Distil-Whisper maintains the robustness of the Whisper model, performing to within 1% WER on OOD test data in a zero-shot transfer setting.
    - On long-form evaluation, the distilled model outperforms Whisper by 0.1% WER. We show that this performance gain is due to a lower propensity to hallucinate than the original Whisper model. Hallucinations are characterised by either the repetitive generation of identical sequences, or predicting passages of text not spoken in the audio input and are most prevalent in long-form audio transcription, particularly when the audio contains large amounts of silence between spoken utterances. To quantify the amount of repetition and hallucination in the predicted transcriptions, we measure the number of repeated 5-gram word duplicates (5-Dup.) and the insertion error rate (IER) over the four OOD long-form datasets.
    - IMO, the question: does KL loss already includes Pseudo Labeling loss, if the latter is obtained with greedy decoding?

Jain, R., Barcovschi, A., Yiwere, M., Corcoran, P., & Cucu, H. (2023). Adaptation of Whisper models to child speech recognition. arXiv, 2307.13008. Retrieved from https://arxiv.org/abs/2307.13008v1

Koizumi, Y., Zen, H., Karita, S., Ding, Y., Yatabe, K., Morioka, N., ...Bacchiani, M. (2023). Miipher: A Robust Speech Restoration Model Integrating Self-Supervised Speech and Text Representations. arXiv, 2303.01664. Retrieved from https://arxiv.org/abs/2303.01664v2

    - We propose Miipher (multiple features integrated speech restoration), a robust speech restoration (SR) model, that convers degraded speech signals into high-quality ones. The enhanced speech can be further used to train text-to-speech (TTS) models, because the quality of speech generation is directly affected by that of the training samples.
    - We especially focus on the following two difficult degradations where SR frequently fails: phoneme masking by noise and/or reverberation, and phoneme deletion due to codecs and/or down-sampling. If a noisy sample lacks a phoneme, it introduces an unrecoverable error. To solve these issues, we 1) use a speech representation extracted from w2v-BERT, instead of a log-mel spectrogram, 2) we consider the deleted phoneme reconstruction problem as a text-conditioned speech inpainting, and use a text representation extracted from PnG-BERT (fig. 1).
    - Since SSL features often lose speaker information, we also use a speaker embedding extracted from audio with a streaming Conformer-based speaker encoding model. The model was trained on the dataset described in "Parameter-free attentive scoring for speaker verification" while minimizing the generalized end-to-end extended-set softmax (GE2E-XS) loss. Speaker embedding is combined to PnG-BERT features using a CNN-based simple feature-wise linear modulation (FiLM) layer.
    - Then a DF-Conformer-based feature cleaner predicts clean w2v-BERT features. We also apply the 5-layer CNN Post-Net proposed in Tacotron2. We iterate twice the entire feature cleaning process consisting of the feature cleaner and the Post-Net, where the parameters of the layers are shared. We used a combined loss function of MAE, MSE, and a spectral convergence loss. This loss is calculated before and after the Post-Net, and calculated for both iterations.
    - Then, a WaveFit neural vocoder synthesizes a restored waveform from cleaned w2v-BERT features. In addition to the original adversarial loss function proposed in WaveFit, we used the multi-period discriminator (MPD).
    - We trained the proposed model with a proprietary dataset that contains 2,680 hours of noisy and studio-quality speech pairs. To apply the noise, we used the TAU Urban Audio-Visual Scenes 2021 dataset, internally collected noise snippets that simulate conditions like cafe, kitchen, and cars, and noise and music sources. The noisy utterances were generated by mixing randomly selected speech and noise samples from these datasets with signal-to-noise ratio (SNR) from 5 dB to 30 dB. In addition, we augmented the noisy dataset with 4 patterns depending on the presence or absence of reverberation and codec artifacts.

Liu, O., Tang, H., & Goldwater, S. (2023). Self-supervised Predictive Coding Models Encode Speaker and Phonetic Information in Orthogonal Subspaces. arXiv, 2305.12464. Retrieved from https://arxiv.org/abs/2305.12464v3

Maison, L., & Estève, Y. (2023). Some voices are too common: Building fair speech recognition systems using the Common Voice dataset. arXiv, 2306.03773. Retrieved from https://arxiv.org/abs/2306.03773v1

    - We use the French Common Voice dataset to quantify the biases of a pre-trained wav2vec 2.0 model toward several demographic groups.
    - We fine-tune the pre-trained model on a variety of fixed-size, carefully crafted datasets. Results highlights the importance of prioritizing speaker diversity over dataset size and demographic diversity when collecting audio data.

Peng, Y., Tian, J., Yan, B., Berrebbi, D., Chang, X., Li, X., ...Watanabe, S. (2023). Reproducing Whisper-Style Training Using an Open-Source Toolkit and Publicly Available Data. arXiv, 2309.13876. Retrieved from https://arxiv.org/abs/2309.13876v3

    - We present an Open Whisper-style Speech Model (OWSM), which reproduces Whisperstyle training using an open-source toolkit and publicly available data. We will provide reproducible recipes encompassing the entire pipeline, including data preparation, training, inference, and scoring. Furthermore, we will release pre-trained models and training logs.
    - Our multitask data format mostly follows OpenAI Whisper (fig. 1). Our model is designed to support any-to-any speech-to-text translation, whereas Whisper can only perform any-to-English translation.
    - OWSM additionally employs a joint CTC loss for ASR targets. In our preliminary experiments, we observed suboptimal convergence of the attention-based encoder-decoder, and incorporating a joint ASR CTC loss to the encoder output can stabilize training and expedite convergence.
    - We combine training sets from various publicly available ASR and ST corpora. Our largest dataset comprises 180k hours of labeled audio data (approximately one quarter of the Whisper's total data). We have developed new data preparation scripts tailored specifically for Whisper-style training (long-form audio data).
    - Our training data is gathered from many public corpora with inconsistent case and punctuation. During inference, we find that OWSM models are able to recognize the corpus and generate outputs that are consistent with the training data format. In the future, we will normalize the text to address this issue. Note that this analysis demonstrates the benefit of using public data and open-source code, without which we cannot discover such issues.
    - For inference, OpenAI Whisper implements both greedy decoding and beam search with temperature fallback. The latter is a complicated procedure relying on many heuristics. Our OWSM utilizes the ESPnet framework, ensuring compatibility with various decoding algorithms including greedy search, beam search, and joint CTC/attention decoding (for ASR only).
    - The current OWSM still falls behind Whisper in many benchmarks.
    - IMO, the authors evaluate on the datasets which are in-domain for OWSM and out-of-domain for Whisper.

Pratap, V., Tjandra, A., Shi, B., Tomasello, P., Babu, A., Kundu, S., ...Auli, M. (2023). Scaling Speech Technology to 1,000+ Languages. arXiv, 2305.13516. Retrieved from https://arxiv.org/abs/2305.13516v1

Qu, L., Weber, C., & Wermter, S. (2023). Emphasizing Unseen Words: New Vocabulary Acquisition for End-to-End Speech Recognition. arXiv, 2302.09723. Retrieved from https://arxiv.org/abs/2302.09723v2

    - ASR systems need to continuously acquire new vocabulary.
    - We generate out-of-vocabulary (OOV) words using text-to-speech systems. We choose 100 OOV words appearing in LRS3-TED dataset but not existing in LibriSpeech dataset. Then, we crawl texts including the new words from the Internet and synthesize audio with TTS systems. Note that the same OOV words (in another itterances) are used to test the model.
    - We enlarge the classification loss on utterances containing OOV words (sentence-level), or rescale the gradient used for back-propagation for OOV words (word-level), when fine-tuning a previously trained model on synthetic audio. To overcome catastrophic forgetting, we employ L2 regularization and elastic weight consolidation (EWC). This can support continual learning of an ASR system.
    - We use two-pass hybrid CTC/attention ASR architecture (fig. 5).
    - Compared with previous methods that just fine-tune synthetic audio with EWC, the experimental results on the LibriSpeech benchmark reveal that our proposed loss rescaling approach can achieve significant improvement on the recall rate with only a slight decrease on WER.
    - Word-level rescaling is more stable than utterance-level rescaling and leads to higher recall rates and precision on OOV word recognition.

Robertson, S., & Dunbar, E. (2023). Bigger is not Always Better: The Effect of Context Size on Speech Pre-Training. arXiv, 2312.01515. Retrieved from https://arxiv.org/abs/2312.01515v1

    - We experiment on ABX phone discriminability task: ic scores the number of times model representations of tokens of a given phoneme are more similar to other representations of the same versus another phoneme.
    - We find that phone discriminability in the pre-trained CPC model representations peaks at around 40 ms of preceding context, and that having too much context (beyond around 320 ms) substantially degrades the quality of the representations.
    - This pattern also transfers to supervised ASR on top of frozen pre-trained representations.
    - So, we saw strong support for the hypothesis that too much context is detrimental. Because the downstream ASR model can perform significant pattern recognition on its own, it may be more important for the upstream model to retain a high-fidelity representation of the input than to perform extensive pre-processing.

Rouditchenko, A., Khurana, S., Thomas, S., Feris, R., Karlinsky, L., Kuehne, H., ...Glass, J. (2023). Comparison of Multilingual Self-Supervised and Weakly-Supervised Speech Pre-Training for Adaptation to Unseen Languages. arXiv, 2305.12606. Retrieved from https://arxiv.org/abs/2305.12606v2

Rubenstein, P. K., Asawaroengchai, C., Nguyen, D. D., Bapna, A., Borsos, Z., Quitry, F. d. C., ...Frank, C. (2023). AudioPaLM: A Large Language Model That Can Speak and Listen. arXiv, 2306.12925. Retrieved from https://arxiv.org/abs/2306.12925v1

    - We introduce AudioPaLM, a unified speech-text LLM, capable of consuming and producing both speech and text.
    - AudioPaLM fuses text-based (PaLM-2) and speech-based (AudioLM) language models.
    - AudioPaLM uses joint vocabulary that can represent speech and text with a limited number of discrete tokens. This allows training a decoder-only model on a mixture of tasks that involve arbitrarily interleaved speech and text (speech recognition, text-to-speech synthesis, speech-to-speech translation). We initialize decoder weights with those of a text-only LLM.
    - We achieve SOTA on AST and S2ST benchmarks, and competitive performance on ASR benchmarks.
    - AudioPaLM performs zero-shot AST with speech input/target language combinations that were not seen in training.
    - AudioPaLM is able to transfer a voice across languages based on a short spoken prompt.

Veliche, I.-E., & Fung, P. (2023). Improving Fairness and Robustness in End-to-End Speech Recognition through unsupervised clustering. arXiv, 2306.06083. Retrieved from https://arxiv.org/abs/2306.06083v1

    - We focus on solving fairness and robustness issue in ASR while meeting privacy requirement (regulators have passed down strict laws that prohibit the use of demographic and other Personal & Private Information in building AI systems).
    - We propose clustering the training data using utterance level embeddings extracted with a speaker ID model and usethe resulting cluster ID as an additional feature in training instead. At inference time, we give each utterance an “unknown” cluster ID as an additional feature.

Wagner, L., Zusag, M., & Bloder, T. (2023). Careful Whisper -- leveraging advances in automatic speech recognition for robust and interpretable aphasia subtype classification. arXiv, 2308.01327. Retrieved from https://arxiv.org/abs/2308.01327v1

Xu, H., Jia, F., Majumdar, S., Huang, H., Watanabe, S., & Ginsburg, B. (2023). Efficient Sequence Transduction by Jointly Predicting Tokens and Durations. arXiv, 2304.06795. Retrieved from https://arxiv.org/abs/2304.06795v2

Yang, C.-H. H., Li, B., Zhang, Y., Chen, N., Prabhavalkar, R., Sainath, T. N., & Strohman, T. (2023). From English to More Languages: Parameter-Efficient Model Reprogramming for Cross-Lingual Speech Recognition. arXiv, 2301.07851. Retrieved from https://arxiv.org/abs/2301.07851v1

    - The success of current neural ASR models is still related to the scale of training data.
    - We propose Conformer-based ASR Reprogramming (CAR) for cross-lingual adaptation, that makes the model frozen and only inserts few trainable modules.
    - CAR is based on The Neural reprogram method (see "Adversarial Reprogramming of Neural Networks") which mainly adds trainable parameters at its input level of a pre-trained model.
    - Our neural reprogramming has three major components (fig. 1): (1) input reprogramming, associated with standard model reprogramming or input-prompting, (2) latent space reprogramming, related to the residual adapters, and (3) multilingual graphemes pre-training which aims to resolve the existing challenges of cross-lingual learning on graphemes mismatching.
    - We only require 11M (4.8% of its full pretrained model) trainable parameters to achieve 11.9% WER cross seven languages in MLS benchmark for ASR task.

Zaiem, S., Parcollet, T., & Essid, S. (2023). Automatic Data Augmentation for Domain Adapted Fine-Tuning of Self-Supervised Speech Representations. arXiv, 2306.00481. Retrieved from https://arxiv.org/abs/2306.00481v1

    - We propose a novel supervised domain adaptation method for self-supervised speech representations. We apply properly calibrated data augmentations on a large clean dataset, bringing it closer to the target domain, and using it as part of an initial fine-tuning stage. Augmentations are automatically selected through the minimization of a conditional-dependence estimator, based on the target dataset (fig. 1). After this, fine-tuning on the small target domain dataset is performed.
    - The method is validated with an oracle simulated experiment and experiments with naturally noisy datasets.

Zhang, Y., Han, W., Qin, J., Wang, Y., Bapna, A., Chen, Z., ...Wu, Y. (2023). Google USM: Scaling Automatic Speech Recognition Beyond 100 Languages. arXiv, 2303.01037. Retrieved from https://arxiv.org/abs/2303.01037v3

    - We introduce the Universal Speech Model (USM) that performs ASR across 100+ languages.
    - Our training pipeline utilizes three types of datasets: Unpaired Audio (including 12M hours of YouTube-based audio covering over 300 languages), Unpaired Text and Paired ASR Data (including 90k hours of labeled multilingual data covering 73 language and 100k hours of en-US pseudo-labeled data generated by noisy student training).
    - Our model is 2B-parameter Conformer.
    - The first step (fig. 1) is Unsupervised Pre-training: BEST-RQ (BERT-based Speech pre-Training with Random-projection Quantizer). We find that BEST-RQ pre-training can effectively scale to the very large data regime with a 2B Conformer, comparing favorably against Wav2Vec 2.0 and W2v-BERT in this setting.
    - The second step is MOST (Multi-Objective Supervised pre-Training). Here, we optimize a weighted sum of the BEST-RQ masked language model loss, along with the text-injection losses (including the supervised ASR loss and modality matching losses).
    - The third step is Supervised ASR Training of ASR models trained with CTC or RNN-T decoder (these models have been observed to hallucinate compared to attention-based seq-to-seq decoders). We also introduce chunk-wise attention (fig. 4): the USM-CTC/LAS models trained with it is able to produce high-quality transcripts for very long utterances.
    - We compare the performance of our models against public baselines, including Whisper large-v2. For the massively multilingual speech recognition test dataset from YouTube, we observe that Whisper hallucinates in many languages, resulting in a WER exceeding 100%. We exclude languages for which Whisper produces > 40% WER, and also use segmented decoding for Whisper with 30-second segments to further reduce the effect of hallucinations. Still, our USM-LAS and USM-CTC models outperform Whisper by a wide margin on YouTube en-US, despite training on significantly less supervised data.
    - We investigate whether MOST representations are useful for adapting the model to new domains by freezing the entire learned encoder produced by MOST and adjusting a small amount of parameters added to the network by residual adapters.By adding only 2% to the total number of parameters, the MOST only performs slightly worse than the fine-tuning baselines. The small number of parameters being trained in this approach makes it feasible to extend our system to a large number of new domains and new tasks, even with a limited amount of training data, such as in FLEURS.

Zuluaga-Gomez, J., Ahmed, S., Visockas, D., & Subakan, C. (2023). CommonAccent: Exploring Large Acoustic Pretrained Models for Accent Classification Based on Common Voice. arXiv, 2305.18283. Retrieved from https://arxiv.org/abs/2305.18283v1

Attia, A. A., Demszky, D., Ogunremi, T., Liu, J., & Espy-Wilson, C. (2024). Continued Pretraining for Domain Adaptation of Wav2vec2.0 in Automatic Speech Recognition for Elementary Math Classroom Settings. arXiv, 2405.13018. Retrieved from https://arxiv.org/abs/2405.13018v1

    - We show that continued pretraining (CPT) in effective in adapting Wav2vec 2.0 to the classroom domain (which is uniquely characterized by the abundance of children’s speech and unique classroom noise like children’s babble noise).
    - We perform CPT for Wav2vec2.0 (starting from 3 different ASR checkpoints) on unlabeled classroom data and then finetune on labeled classroom data for ASR. To compare, we perform finetuning of the off-the-shelf Wav2vec2.0 and Whisper models without CPT. Our results show that CPT is the most effective way for adapting Wav2vec2.0 for noisy classroom speech.
    - IMO, the WER std is too much to make such conclusions (see table 3). Also, since the in-domain 5-gram LM is not used for Whisper, the comparison is not fair.
    - In cases with children’s babble noise, Whisper hallucinates by repeating a single word or phrase, for example, “how do you know that what what is the denominator what is the denominator the the the the the...” with the word ”the” repeating 206 times. Wav2vec does not suffer from the same hallucination problem.

Bevilacqua, A., Saviano, P., Amirante, A., & Romano, S. P. (2024). Whispy: Adapting STT Whisper Models to Real-Time Environments. arXiv, 2405.03484. Retrieved from https://arxiv.org/abs/2405.03484v1

    - We introduce Whispy, a production-ready system that can be used as a self-contained transcription service (fig. 1). Whispy is a wrapper around the Whisper pretrained models. The overall system lives within an HTTP server.
    - Whispy processes short audio chunks that accumulate within a shifting buffer. An agreement algorithm based on the Levenshtein distance extracts the most accurate transcript suggestions when overlapping portions of the buffer are transcribed.
    - The produced transcription is filtered to detect potential hallucinations, that is required because of the intrinsic tendency of large text-generation models to produce, from time to time, unreliable or unpredictable outputs. Other than content-oriented hallucinations, Whisper can produce hallucinatory artifacts resulting in the repetition of a single token or sequence of tokens. We designed a naive hallucination filter capable of reliably detecting repeating tokens and skim them. However, during our test campaigns we did not experience a sufficiently large number of hallucinatory events to determine the goodness of the hallucination filter.
    - Whispy maintains transcription performance comparable to its offline counterparts, while exhibiting minimal latency.

Frieske, R., & Shi, B. E. (2024). Hallucinations in Neural Automatic Speech Recognition: Identifying Errors and Hallucinatory Models. arXiv, 2401.01572. Retrieved from https://arxiv.org/abs/2401.01572v1

    - The main difference between phonetic ASR errors and hallucinations lies in the severity of the latter. Phonetic ASR errors appear as badly transcribed words or phrases, especially when utterances are phonetically similar. They are being evaluated as a number of phonetic substitutions, insertions and deletions. On the contrary, the hallucinatory output does not have phonetic or semantic connection with the source utterance, even though it is often fluent and coherent. This third aspect is crucial to differentiate hallucinations from random repetitions and word salad.
    - We propose a framework to differentiate hallucinations from other ASR errors. Since hallucinations should resemble probable model outputs, the fluency measure should be high and semantic connection to reference should be low.
    - We use cosine similarity to estimate if the reference and the output are semantically related. We define hallucinations as semantically disconnected outputs. Therefore, they are the errors with the lowest cosine similarity and simultaneously high WER.
    - To evaluate sentence fluency, we use perplexity, which gives intuition of how viable is the sentence according to LLM. We find that results returned by Flan T5 Small contain less extreme values of perplexity values than GPT2, hence the decision to choose the former model for the fluency evaluation.
    - The model used for all the experiments is an encoder-decoder transformer provided by fairseq.
    - We present the method to induce hallucinations with random noise injection to the source utterance.
    - IMO, 1) phonetic mistakes may significantly change the semantics and hence give low cosine similarity, so this is not unique to hallucinaions, 2) non-phonetic mistakes (hallucinaions) are not necessarily related to the large change in semantics, and 3) fluency cannot be measured by perplexity, because gramatically correct and meaningful text is not always associated with high probability in any corpus.

Gupta, A., Saon, G., & Kingsbury, B. (2024). Exploring the limits of decoder-only models trained on public speech recognition corpora. arXiv, 2402.00235. Retrieved from https://arxiv.org/abs/2402.00235v1

    - We train Decoder-Only Transformer for ASR (DOTA) solely using cross-entropy loss (inspired by their success as language models) on public ASR data alone.
    - The audio input to the model is a sequence of audio frames followed by text token embeddings. We find that bidirectionality over audio frames is critical to high performance across model scales. For simplicity, we used sinusoidal positional embeddings.
    - We vary over a wide range of hyperparameters, including augmentations and the datasets included in the training set. For example, augmentations are not very important.
    - Our training data consists of MultilingualLibriSpeech (English), PeoplesSpeech, GigaSpeech, SPGISpeech, CommonVoice 11.0, LibriSpeech, Fisher, TedLium 3, AMI, FLEURS (English), VoxPopuli (English), LJ Speech, VoiceMail, VCTK. This resulted in 93K hours of speech-text pairs. We normalized the transcripts using Whisper’s EnglishTextNormalizer module which converts text to lower case, removes punctuation and applies several other case-by-case transformations. We further remove newline character and insert space between consecutive digits. We then tokenize the text using bert-base-uncased tokenizer.
    - DOTA outperforms Whisper large-v3 on 7 out of 15 test sets. As our models are trained on these sets, the errors rates are low.
    - IMO, the most important thing is that the test sets are in-domain for DOTA and out-of-domain for Whisper, so the comparison is by no means fair.

He, H., Shang, Z., Wang, C., Li, X., Gu, Y., Hua, H., ...Wu, Z. (2024). Emilia: An Extensive, Multilingual, and Diverse Speech Dataset for Large-Scale Speech Generation. arXiv, 2407.05361. Retrieved from https://arxiv.org/abs/2407.05361v2

Ji, S., Jiang, Z., Cheng, X., Chen, Y., Fang, M., Zuo, J., ...Zhao, Z. (2024). WavTokenizer: an Efficient Acoustic Discrete Codec Tokenizer for Audio Language Modeling. arXiv, 2408.16532. Retrieved from https://arxiv.org/abs/2408.16532v1

	- Current mainstream discrete speech representations are divided into semantic and acoustic tokens (IMO, probably they mean speech-related tokens as "semantic"; however, HuBERT is their example of semantic tokens, but its design is not specific to speech, it's all about the training data). Semantic tokens often lack acoustic information, necessitating multi-stage cascades in downstream models to generate raw waveforms. Acoustic tokens can uniformly model speech, music, and audio. A robust acoustic tokenizer should at least maintain the encoder-VQ-decoder structure - this indicates that the Codec model should primarily function as a Tokenizer and De-Tokenizer. Additionally, the temporal dimension of codecs matter.
	- We argue that a single quantizer layer fundamentally differs from multiple quantizers. When the number of quantizers exceeds one, downstream models (that use this quantization to perform various tasks) require additional design efforts, while with a single quantizer, speech modalities can be directly autoregressively embedded into large multimodal models, such as LLama.
	- We introduce WavTokenizer, a discrete acoustic codec model. Our model is built on the framework of VQ-GANs. WavTokenizer passes the raw audio X through three modules. 1) a CNN+LSTM encoder network that outputs a latent feature representation Z, 2) A single quantizer discretizes Z to generate a discrete representation Z_q, 3) an improved decoder that reconstructs the audio signal ̃X from the compressed latent representation Z_q.
	- The model is trained end-to-end with 4 losses (eq. 3-6). The quantizer loss penalizes the distance between Z and Z_q. The mel-spectrum reconstruction loss penalizes the distance between Mel(X) and Mel(̃X). We also apply a perceptual loss in the form of discriminators operating at different resolutions, with the adversarial loss (a hinge loss over the logits of these discriminators) and the feature matching loss, penalizes the distance between feature maps in distriminators, obtained from X and from ̃X.
	- The goal of WavTokenizer is to compress speech representations into the codebook space of a single quantizer. Expanding the codebook space can reduce information loss caused by compressing the hierarchical RVQ structure into a single quantizer. We expanded the codebook space from 2^10 to 2^14. We adjusted the number of cluster centers to 200 to align with the larger codebook space. During training, each input’s selected code is updated using an EMA with a decay of 0.99, and codes unassigned for several batches are replaced with input vectors randomly sampled from the current batch. This forced activation strategy helps ensure effective utilization of the large codebook space.
	- In decoder, we achieve waveform upsampling through inverse Fourier transform. We also introduced an attention module in the decoder.

Jin, W., Cao, Y., Su, J., Shen, Q., Ye, K., Wang, D., ...Liu, Z. (2024). Towards Evaluating the Robustness of Automatic Speech Recognition Systems via Audio Style Transfer. arXiv, 2405.09470. Retrieved from https://arxiv.org/abs/2405.09470v1

Junczyk, M. (2024). Framework for Curating Speech Datasets and Evaluating ASR Systems: A Case Study for Polish. arXiv, 2408.00005. Retrieved from https://arxiv.org/abs/2408.00005v1

    - We collect an open benchmark of 24 openly available datasets for Polish ASR.
    - We perform the most extensive comparison to date of ASR systems for the Polish language. Significant variations across different systems, datasets, and speaker demographics were discovered.

Kang, I. E., Van Gysel, C., & Siu, M.-H. (2024). Transformer-based Model for ASR N-Best Rescoring and Rewriting. arXiv, 2406.08207. Retrieved from https://arxiv.org/abs/2406.08207v1

    - Previos work in ASR error correction  focused exclusively on either reranking the N-best hypotheses, i.e., "rescoring", or overriding the 1-best with its predicted corrections, i.e., "rewriting".
	- We propose Transformer Rescore Attention (TRA) model capable of both rescoring and rewriting ASR hypotheses. TRA does not require acoustic representations as input and can operate as a standalone model outside of on-device ASR, the acoustic representations never leaves the device hence preserving privacy.
	- We also propose a new Matching Query Similarity Distribution (MQSD) objective, that can work well with cross-entropy based training to perform both rescore and rewrite tasks.
	- TODO

Kim, H., Myung, J., Kim, S., Lee, S., Kang, D., & Kim, J. (2024). LearnerVoice: A Dataset of Non-Native English Learners' Spontaneous Speech. arXiv, 2407.04280. Retrieved from https://arxiv.org/abs/2407.04280v1

Koenecke, A., Choi, A. S. G., Mei, K. X., Schellmann, H., & Sloane, M. (2024). Careless Whisper: Speech-to-Text Hallucination Harms. arXiv, 2402.08021. Retrieved from https://arxiv.org/abs/2402.08021v2

    - We evaluate Whisper’s transcription performance on the axis of "hallucinations", defined as undesirable generated text "that is nonsensical, or unfaithful to the provided source input".
    - We set the sampling temperature parameter to 0.
    - Roughly 1% of Whisper transcriptions from AphasiaBank dataset contain entire hallucinated phrases. Whisper hallucinates entire made-up sentences when no one is speaking in the input audio files.
    - 38% of hallucinations are harmful or concerning in some way (as opposed to innocuous and random), such as explicit portrayals of physical violence or death, or false authority like thanking viewers or specific groups, and linking to websites that misrepresent the speaker source (table 1).
    - Longer pauses in spoken speech (thereby, with longer periods of background noise in the audio file) could result in more hallucinations due to Whisper being seeded by noise rather than speech (fig. 3).
    - Notably, we found no evidence of hallucinations in competing speech recognition systems such as Google Speech-to-Text or the latest Google Chirp model. As such, we believe hallucinations to currently be an OpenAI-specific concern.
    - Our work compares a relatively small set of aphasia speakers to control group speakers in a setting where they are being asked a standard slate of interview questions. If a broader set of topics were to be discussed, it is possible that the scope of hallucinations would be widened.
    - Another one category of hallucinations is the appearance of other languages. For example, Whisper is prone to generating non-English transcriptions even when provided an argument indicating that the target language is English.
    - We hypothesize on two underlying mechanisms that likely result in these hallucinations.

Li, X., Takamichi, S., Saeki, T., Chen, W., Shiota, S., & Watanabe, S. (2024). YODAS: Youtube-Oriented Dataset for Audio and Speech. arXiv, 2406.00899. Retrieved from https://arxiv.org/abs/2406.00899v1

    - We introduce YODAS (YouTube-Oriented Dataset for Audio and Speech), a large-scale, multilingual dataset comprising currently over 500k hours of speech data in more than 100 languages
    - YODAS includes: 1) the manual subset of 86,400 hours of audio data paired with manual transcriptions, 2) the automatic subset of 335,845 hours of audio data with automatic transcriptions from YouTube, 3) The unlabeled subset of 144,174 hours of raw audio data, devoid of any transcription.
    - English emerges as the most prevalently used language, with Spanish and Russian occupying the second and third positions, respectively, when assessed based on duration. The automatic subset has only a very limited number of languages (14 languages) compared with the manual subset (140 languages).
    - In the automatic subset, most utterances are short and have little variance. This is because the automatic subtitle frequently divides long utterances into small chunks to help viewers to follow subtitles easier.
    - We focus on monolingual speech recognition, and build simple baseline models for the top-25 languages in the manual subset. Our baseline is a linear layer randomly initialized on top of the pre-trained XLSR representations, which is then optimized with the CTC loss. The subword vocabulary is prepared with BPE using SentencePiece, where we use 300 as the vocabulary size for most languages except 5000 for Mandarin and 3000 for Japanese. For simplicity, we do not perform speech augmentation. The decoding is done greedily without any language models. Table 5 displays our results for monolingual speech recognition. We observe that languages possessing a larger BPE vocabulary size tend to correspond with higher CER (is it a random split validation?). Models trained on the manual subset yield significantly superior performance compared to those trained on the automatic subset.

Ma, M., Koizumi, Y., Karita, S., Zen, H., Riesa, J., Ishikawa, H., & Bacchiani, M. (2024). FLEURS-R: A Restored Multilingual Speech Corpus for Generation Tasks. arXiv, 2408.06227. Retrieved from https://arxiv.org/abs/2408.06227v1

    - We introduce FLEURS-R speech-text dataset. It maintains an N-way parallel speech corpus in 102 languages as FLEURS, with improved audio quality and fidelity by applying the speech restoration model Miipher.
    - The goal of the dataset is to catalyze speech generation research in low-resource languages, since in the original FLEURS dataset all recordings are kept as they-are, either from quiet or noisy environment, while speech generation models are requested to produce high quality speech.
    - Since Miipher supports only English, we replaced the acoustic feature extractor from w2v-BERT to the 2-billion parameter non-fine-tuned Universal Speech Model (USM). It is known that deeper layers tend to lose detailed and local acoustic information; therefore, we used the intermediate feature from the 13th of 32th layers based on preliminary experiments. Also, neither text nor speaker conditioning improved the reconstruction accuracy. Consequently, both speaker encoder and PnG-BERT text encoder were removed from the new Miipher network architecture.
    - To identify successfully restored samples, we performed ASR-based filtering.

Meng, L., Kang, J., Wang, Y., Jin, Z., Wu, X., Liu, X., & Meng, H. (2024). Empowering Whisper as a Joint Multi-Talker and Target-Talker Speech Recognition System. arXiv, 2407.09817. Retrieved from https://arxiv.org/abs/2407.09817v1

Mittal, A., Prabhu, D., Sarawagi, S., & Jyothi, P. (2024). SALSA: Speedy ASR-LLM Synchronous Aggregation. arXiv, 2408.16542. Retrieved from https://arxiv.org/abs/2408.16542v1

    - The current methods to enhance ASR models with LMs suffer either from the exiensive training requirements, or from high decoding latencies due to second-pass rescoring in ASR error correction. Also, many current methods rely on the n-best predictions and will not fare well on low-resource languages owing to large errors in the n-best predictions.
    - We propose SALSA, a lightweight method to integrate LM model with ASR model (we use Whisper and LLama-2). It keeps both backbones frozen and only train projection layers. It can be used to integrate any pretrained decoder-only LM with a pretrained encoder-decoder ASR model using small amounts of labeled speech in the target languages.
    - We are also the first to apply utilize LMs for ASR of a diverse set of low-resource languages (not only English ASR).
    - We select N different ASR decoder layers and N different LM decoder layers and connect them one-to-one: for each pair, a trainable mapping R^m -> R^m processes an ASR hidden state, and the result is added to the LM hidden state (fig. 1). The LM keeps generating tokens until a valid text piece recognizable by ASR’s tokenizer is formed (often for low resource languages the tokenizers for LM and ASR can use different multi-token sequences to encode a single character in the target language). Then the just generated text is re-tokenized with the ASR’s tokenizer. Thus, in SALSA both decoders (ASR and LM) move forward in tandem albeit having different tokenizations.
    - So, the authors generate with LM until a valid utf-8 character if formed, and then re-tokenize the resulting text with ASR tokenizers.

Mohamed, M., Liu, O. D., Tang, H., & Goldwater, S. (2024). Orthogonality and isotropy of speaker and phonetic information in self-supervised speech representations. arXiv, 2406.09200. Retrieved from https://arxiv.org/abs/2406.09200v1

    - We study how information is represented in self-supervised speech representations, beyond just assessing the linear separability of classes. We use a geometric approach, that is widely used for analyzing self-supervised models of text.
    - We develop a new measure for analyzing high-dimensional distributions, the Cumulative Residual Variance (CRV). Given datasets X and Y embedded in the same embedding space, the CRV of X w.r.t. Y (denoted as X\Y) measures the degree to which the principal components of Y are orthogonal to those of X. Meanwhile, X\X is a measure of the isotropy of X — the degree to which X effectively utilizes all dimensions of the embedding space, i.e., has uniform covariance.
    - Comparing to the previous work "Self-supervised Predictive Coding Models Encode Speaker and Phonetic Information in Orthogonal Subspaces", the CRV measure allows us to better quantify orthogonality.
    - On English LibriSpeech we show that, unlike randomly initialized models, all trained models have a high degree of orthogonality between the speaker and phonetic subspaces. For all 6 trained models, the accuracy of a phone classifier trained on the model representations is significantly correlated with the CRV between the two subspaces.
    - It has been argued in the NLP literature that higher isotropy is desirable in an embedding space. However, we did not find strong evidence for this hypothesis. Instead, having evenly distributed centroids is more important for classification accuracy in these models than having evenly distributed frame representations.

Peng, Y., Tian, J., Chen, W., Arora, S., Yan, B., Sudo, Y., ...Watanabe, S. (2024). OWSM v3.1: Better and Faster Open Whisper-Style Speech Models based on E-Branchformer. arXiv, 2401.16658. Retrieved from https://arxiv.org/abs/2401.16658v2

Ramirez, F. M., Chkhetiani, L., Ehrenberg, A., McHardy, R., Botros, R., Khare, Y., ...Yoshioka, T. (2024). Anatomy of Industrial Scale Multilingual ASR. arXiv, 2404.09841. Retrieved from https://arxiv.org/abs/2404.09841v2

Shah, M. A., Noguero, D. S., Heikkila, M. A., & Kourtellis, N. (2024). Speech Robust Bench: A Robustness Benchmark For Speech Recognition. arXiv, 2403.07937. Retrieved from https://arxiv.org/abs/2403.07937v1

    - We propose Speech Robust Bench (SRB), a benchmark for evaluating the robustness of ASR models to 69 input perturbations that ASR models may encounter in the physical and digital world (fig. 2). The perturbations are of two broad types: 1) non-adversarial and 2) adversarial.
    - SRB is agnostic to the evaluation data and can be used with any dataset that contains utterances and reference transcripts, however, we recommend using datasets with high-quality clean audio and accurate transcripts so that pre-existing corruptions in the dataset do not confound the robustness metrics. Our evaluation in uses clean speech from Librispeech.
    - We measure two aspects of robustness: the transcription accuracy (WER) and the stability under randomized perturbations. When aggregating WER over multiple perturbations, we normalize the WER of the target model by the WER of a baseline model (following the methodology of "Benchmarking neural network robustness to common corruptions and perturbations" on images domain). Doing so penalizes errors on “easy” corruptions more than errors on “harder” corruptions. This normalized metric is called Normalized WER (NWER). Also SRB measures the prediction stability of the model by computing the variance in the WER caused by corrupting the signal with multiple corruption samples drawn from the same distribution. We call this metric WER Variance (WERV).
    - On English speech, Whisper is able to withstand more severe corruptions better than other models. However, it is outperformed by other, smaller, models on several perturbations. It is rather surprising that despite being trained on more than ten times the amount of data, wsp-lg is outperformed by both hubt-lg and w2v2-lg-slf on fairly common perturbations such as RIR, resampling, and tempo reduction. Some models (hubt-lg, w2v2-lg-slf, ds) are more stable on Gaussian noise, while others ( wsp-lg, w2v2-bs, w2v2-lgrob) are more stable on environmental noise. Larger models tend to be more robust than smaller models, even if the latter are trained on significantly more data.
    - On non-English (Spanish) speech, it is interesting that despite having more parameters and being trained on 10× more data, wsp-lg is outperformed by w2v2-lg-es, thus indicating that simply scaling the model and training data is not sufficient to achieve robustness, particularly in the multi-lingual setting. So, wsp-lg is not the most robust model on Spanish, and struggles on simple perturbations, particularly on RIR (room impulse response).
    - We observe noticeable disparities in the robustness across various demographic subgroups, for example, RIR and adversarial attacks disproportionately degrade the performance of models for female speakers.

Tur, A. D., Moumen, A., & Ravanelli, M. (2024). ProGRes: Prompted Generative Rescoring on ASR n-Best. arXiv, 2409.00217. Retrieved from https://arxiv.org/abs/2409.00217v1

    - ASRs are not trained on enough data to deeply capture linguistic information, resulting in challenges when transcribing unknown words and named entities. Language models are commonly used to enhance ASR performance by ensuring that transcriptions maintain linguistic plausibility. Rescoring can be performed in two ways: either during the decoding process, where partial hypotheses are rescored, or after decoding, where the n-best alternatives are rescored. Traditionally, rescoring involved using simple language models, such as n-grams and word graphs. However, rescoring provides minimal benefits, even with LLMs, when the correct transcription is not among the top n hypotheses.
	  - We propose PROmpted Generative REScoring (ProGRes). First, for each audio signal, we extract the n-best hypotheses from a pretrained ASR model. We then prompt an LLM to generate a more diverse set of hypotheses. The LLM-generated hypothesis are added to form an extended set of hypotheses. Then we calculate LLM score and ASR score (either log-likelihood or CTC likelihood) for each hypothesis and combine both scores to select the best hypothesis.

Yang, S.-w., Chang, H.-J., Huang, Z., Liu, A. T., Lai, C.-I., Wu, H., ...Lee, H.-y. (2024). A Large-Scale Evaluation of Speech Foundation Models. arXiv, 2404.09385. Retrieved from https://arxiv.org/abs/2404.09385v2

    - We extend our SUPERB benchmark (see "SUPERB: Speech processing Universal PERformance Benchmark") with the following contributions:
    - 1) We provide a complete platform featuring an online leaderboard supporting submissions
    - 2) We scale the evaluation from the original 14 models [34] to 33 models
    - 3) We observe that the learnable weighted-sum over the frozen layers of the SSL model is better than the conventional evaluation protocol: using the frozen last layer. Furthermore, individual single-layer benchmarking can sometimes yield even better results.
    - 4) We confirm that the layer weights learned by the weighted-sum protocol do not reflect the layer performance precisely across SUPERB tasks (as in "Layer-Wise Analysis of a Self-Supervised Speech Representation Model").
    - 5) We suggest to conduct statistical test when comparing to our baseline numbers.
    
@article{Chen2024Nov,
	author = {Chen, William and Zhang, Wangyou and Peng, Yifan and Li, Xinjian and Tian, Jinchuan and Shi, Jiatong and Chang, Xuankai and Maiti, Soumi and Livescu, Karen and Watanabe, Shinji},
	title = {{Towards Robust Speech Representation Learning for Thousands of Languages}},
	journal = {ACL Anthology},
	pages = {10205--10224},
	year = {2024},
	month = nov,
	url = {https://aclanthology.org/2024.emnlp-main.570}
}
  - Currently, the best performing models like Whisper, Google USM, w2v-BERT 2.0 v1 and and w2vBERT 2.0 v2 are all trained on fully closed data. Whisper and w2v-BERT 2.0 v1/v2 only report pre-training data quantity and the languages covered. The USM report includes much more information about their data sources, but the model checkpoints remain unreleased. XLSR 53 and XLS-R 128 came with checkpoints and only use publicly accessible datasets but did not release training code. MMS released checkpoints but did not release their training code and crawled data. WavLabLM and MR-HuBERT released code and checkpoints but operated on a smaller scale.
  - For our work, will publicly release all of our heavily optimized training code, along with the training configurations and checkpoints for XEUS. We use publicly accessible datasets and release all of the additional pre-training data that we crawled. We release 200+ intermediate checkpoints and training logs for further research in the training dynamics.
  - Robustness to noisy data is relatively unexplored in SSL research. This is important for multilingual models, since the available recordings of low-resource languages tend to be particularly noisy.
  - We propose XEUS (pronounced Zeus) — a Crosslingual Encoder for Universal Speech. Comparing to Meta’s MMS from "Scaling speech technology to 1,000+ languages", we scale the language coverage, use more powerful model architectures and training objectives.
  - We curate the data from 37 existing corpora (150+ languages, 1.074 million hours of data in total, see table 2 and table 11 - main sorces are YODAS and VoxPopuli, both around 400K hours). We thus ensure diversity, including but not limited to spontaneous speech, accented speech, code-switching, indigenous languages, and singing voices.
  - To increase language coverage, we add 3 more data sources:
  - 1) We reproduce the MMS-unlab dataset, which was not publicly released (see "Scaling speech technology to 1,000+ languages"). Like the original, we crawl religious audiobooks from the Global Recordings Network. Since we use it for SSL instead of language identification, we do not filter out languages with low amounts of data. We also perform VAD with an energy-based detector instead of a neural model, which is more computationally expensive and likely less robust to unseen languages. This leads to a total of 6,700 hours of data across 4,023 languages.
  - 2) We crawl data from WikiTongues, where each 2-20 minute recording contains 1-2 speakers casually speaking a particular language/dialect.
  - 3) We collect a Jesus Dramas corpus: the "Story of Jesus" multi-speaker audio drama on many languages, totalling 645 hours.
  - XEUS is an E-Branchformer encoder consisting of a convolutional feature extractor and 19 E-Branchformer layers. Convolution augmented models achieve superior SSL performance, and we choose the E-Branchformer over the Conformer due to the former’s relative ease of training and superior downstream performance.
  - Training combines HuBERT’s masked prediction (with cross entropy loss), WavLM’s denoising objective, and a new dereverberation objective (fig. 2). We also conduct ablations at a smaller scale.
  - To obtain the target phonetic pseudo-labels for HuBERT masked prediction, we first extract encoded representations from a pre-trained WavLabLM MS model. The representations are then clustered using k-means, with k = 2048. The data used for the feature extraction and clustering is a subset of our training data.
  - We also integrate the acoustic denoising task proposed by WavLM. During training, an input utterance has a probability 0.2 to be augmented with either random noise from the Deep Noise Suppression Challenge, or another utterance in the batch as interference. Target labels are obtained solely from uncorrupted speech.
  - We also propose a novel SSL objective: with probability 0.3 we simulate reverberant conditions in the input audio while the target pseudo-labels are again left untouched. It is possible to apply both the noise and reverberation. We use a Room Impulse Response (RIR), see sec. 4.2.
  - XEUS is pre-trained on 64 A100 GPUs using the ESPnet toolkit. We perform a two passes through the training set.
  - We compare XEUS with 3 SOTA multilingual SSL models: XLS-R 128, MMS, and w2v-BERT 2.0 v2. XEUS is the overall best performing model on ML-SUPERB and is competitive on FLEURS (tables 3, 4).
  - We benchmark XEUS on the English-only SUPERB, comparing to WavLM, the SOTA model on the SUPERB leaderboard for almost all tasks. XEUS consistently reaches if not surpasses SOTA scores across a variety of tasks, obtaining the highest score in 4 English-only tasks (Keyword Spotting, Speaker Diarization, Emotion Recognition, Speech Recognition), despite its curse of multilinguality.
  - Also, resynthesized speech from XEUS is higher quality than that from both WavLM and w2v-BERT 2.0 v2 across all metrics (with unit-to-speech HiFiGAN vocoders trained for speech synthesis).

@article{Zelasko2021Oct,
	author = {{\ifmmode\dot{Z}\else\.{Z}\fi}elasko, Piotr and Povey, Daniel and Trmal, Jan "Yenda" and Khudanpur, Sanjeev},
	title = {{Lhotse: a speech data representation library for the modern deep learning ecosystem}},
	journal = {arXiv},
	year = {2021},
	month = oct,
	eprint = {2110.12561},
	doi = {10.48550/arXiv.2110.12561}
}
  - Speech data is notoriously difficult to work. Recordings come in many flavors, audio is encoded with a variety of codecs, the meta-data comes with a different schema for each dataset. Kaldi introduced a standard representation for all datasets, however its learning curve is steep.
  - We present Lhotse, a speech data representation library. It is one of the three libraries that constitute the next-generation Kaldi framework (the remaining two are k2 for differentiable, GPU-accelerated weighted finite state automata (WFSA) algorithms; and Icefall that contains simple, reproducible recipes for training and evaluating speech models).
  - Lhotse provides standard data preparation recipes for publicly available corpora (over 30 at the time of writing).
  - Lhotse defines several types of speeech manifests:
  - 1) `Recording` abstracts away from the physical location of the audio data. It allows reading audios stored on a local disk, remote URL or a cloud storage service, possibly encoded in different format, with `Recording.load_audio()`.
  - 2) `SupervisionSegment` denotes a time span in the Recording that has some associated meta-data usable for supervised training (start, duration, speaker ID, gender, age, transcript, etc).
  - 3) `Features` helps to work with pre-computed features for model training or inference. It contains tensor shape, frame shift, `Recording` ID etc. Similarly to `Recording`, it abstracts away from the storage mechanism and allows to read/write to a local file-system, a cloud storage, and more. Lhotse supports both on-the-fly and pre-computed feature extraction and leverages lilcom, a feature compression engine which lossily compresses floating-point NumPy arrays into byte strings.
  - 4) `Cut` manifests are the core contribution of Lhotse. They may be viewed as "windows" with a certain offset and duration in a `Recording` that have zero or more `SupervisionSegments`. Most operations performed on cuts are lazy - whether it’s mixing, truncation, padding, or augmentation. `Cut` provides greater data preparation flexibility than was possible with Kaldi - it allows to construct training examples with additional acoustic context for each utterance (e.g., background noises in a telephone conversation that could help the network adapt) or even additional speech context (e.g., modelling contextual dependencies between utterances in a podcast).
  - 5) `CutSet` is a collection of cuts. All speech corpora are represented as CutSets. It has over 30 methods that simplify padding, sub-setting, truncating, extracting features, or visualizing and listening to in Jupyter notebooks. `CutSet` also supports data augmentation that correctly adjusts the relevant meta-data, such as speech segments duration. `CutSet` has a few methods for converting long recordings into shorter cuts (fig. 1).
  - Note that Lhotse is not a feature extraction or augmentation library (it leverages external implementations for these tasks), and is not a framework for training, it just provides a set of tools.
  - Lhotse implementы the PyTorch data API: `Dataset`, `Sampler`, and `DataLoader`. Very large datasets (tens of thousands of hours) are seamlessly handled with minimal memory usage. In our implementation `Sampler` not only provides indices, it "owns" the CutSets with training data, determines the batch size dynamically, based on constraints such as the maximum total speech duration in a mini-batch, and `Dataset` simply acts as a function that transforms a mini-batch `CutSet` into a mini-batch tensor. Lhotse implements several CutSamplers. `SingleCutSampler` is used where the model works with single utterances, `CutPairsSampler` uses two CutSets with matching cut IDs (for example, for voice conversion or speech translation), `BucketingSampler`augments sampler types by stratifying the data into similar-duration buckets to minimize the padding. `ZipSampler` draws batches from multiple samplers and combines them together, which is useful in diversifying the data from multiple sources (domains) during the training.

@incollection{Gemmeke,
	author = {Gemmeke, Jort F. and Ellis, Daniel P. W. and Freedman, Dylan and Jansen, Aren and Lawrence, Wade and Moore, R. Channing},
	title = {{Audio Set: An ontology and human-labeled dataset for audio events}},
	booktitle = {{2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}},
	journal = {Published in: 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
	pages = {05--09},
	issn = {2379-190X},
	publisher = {IEEE},
	doi = {10.1109/ICASSP.2017.7952261}
}
  - We present Audio Set, a dataset and ontology of audio events that endeavors to provides comprehensive coverage of real-world sounds at ImageNet-like scale.
  - Our final list contains 632 audio event categories, arranged in a hierarchy with a maximum depth of 6 levels; an example from among the eight level-6 nodes is "Sounds of things" → "Vehicle" → "Motor vehicle" → "Emergency vehicle" → "Siren" → "Ambulance (siren)".
  - For each class we provide a description, typically one or two sentences, and examples. Of the 632 categories, 56 are "blacklisted" because they have turned out to be obscure or confusing (e.g., "Sounds of things").
  - Another 22 nodes are marked "Abstract" (e.g., "Onomatopoeia"), meaning that they exist purely as intermediate nodes to help structure the ontology, and are not expected to ever be used directly as labels.
  - The Audio Set YouTube Corpus consists of labeled YouTube segments with one or more ontology class labels. It includes 1,789,621 segments (4,971 hours). Single segments can have multiple labels (on average 2.7 labels per segment).
  - Human raters were presented with both the video and audio components (audio-only presentation was found to be more difficult) and asked to independently rate the presence of one or more labels. Each segment was rated by three raters and a majority vote was required to record an overall rating. For speed, a segment’s third rating was not collected if the first two ratings agreed for all labels. The raters were unanimous in 76.2% of votes. The "unsure" answer was rare.
  - We provide maximally-balanced train and test subsets (from disjoint videos), chosen to provide at least 50 positive examples (in both subsets) for as many classes as possible.
  - There were some categories for which we were unable to find enough positive examples to fully populate the dataset, but this proportion is now very small (and shrinking).
  - We have trained a simple baseline system.
  
@article{Gong2021Apr,
	author = {Gong, Yuan and Chung, Yu-An and Glass, James},
	title = {{AST: Audio Spectrogram Transformer}},
	journal = {arXiv},
	year = {2021},
	month = apr,
	eprint = {2104.01778},
	doi = {10.48550/arXiv.2104.01778}
}
  - Motivated by the success of purely attention-based models in the vision domain, it is reasonable to ask whether a CNN is still essential for audio classification.
  - We introduce the Audio Spectrogram Transformer (AST), a convolution-free, purely attention-based model (fig. 1). It is applied to an audio spectrogram, which is split into a sequence of 16×16 patches with overlap, and then linearly projected to a sequence of 1-D patch embeddings with learnable positional embeddings. An additional CLS token is prepended to the sequence. The resulting sequence is then input to the Transformer encoder. The output of the CLS token is used for classification with a linear layer with sigmoid activation maps. (IMO, sigmoid is probably used because AudioSet is a multi-label classification dataset; but what about other tasks?)
  - Strictly speaking, the patch embedding layer can be viewed as a single convolution layer with a large kernel and stride size, and the projection layer in each Transformer block is equivalent to 1×1 convolution. However, the design is different from conventional CNNs, and these Transformer models are usually referred to as convolution-free.
  - Transformer needs more data to train than CNNs. We are able to transfer the 2D spatial knowledge from a pretrained ViT (slightly modified for compatibility) to the AST. mageNet pretrained AST noticeably outperforms randomly initialized AST, especially when the training data volume is smaller. We initialize AST with DeiT weights, and further perform ImageNet distillation.
  - Using 128×2 rectangle patches leads to better performance than using 16×16 square patches when both models are trained from scratch. However, there is no 128×2 patch based ImageNet pretrained models, so using 16×16 patches is the optimal solution. Also smaller size patches lead to better performance.
  - AST naturally supports variable-length inputs and can be applied to different tasks.

Zhang, L., Jiang, N., Wang, Q., Li, Y., Lu, Q., & Xie, L. (2024). Whisper-SV: Adapting Whisper for Low-data-resource Speaker Verification. arXiv, 2407.10048. Retrieved from https://arxiv.org/abs/2407.10048v1

## CV: localization (detection, segmentation)

Wang, Jiaqi, Kai Chen, Shuo Yang, Chen Change Loy, and Dahua Lin. 2019. “[GA-RPN] Region Proposal by Guided Anchoring.” arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1901.03278.

    The authors propose a novel method GA-RPN (guided anchoring) for object detection. Let we have a multi-level feature pyramid from FPN. For each level we first predict a probability map that should indicate the center of some object  (denoted as N_L at fig. 1). To train this stage, authors sort all ground truth objects by feature pyramid levels (fig. 2), define positive a small rectangular region inside object's box (green), and do not apply loss for a larger region inside object's box (yellow). The rest of the space is defined as negative (gray). This gives a ground truth annotation for the probability map, and authors apply focal loss due to high positive-negative imbalance.
    
    Secondly, for each level and spatial location we predict log-height and log-width (denoted as N_S at fig. 1). This differs from a regular box regression, since the center of the object is not shifted. For the center point the ground truth W and H are trivial, and for other points the ground truth W and H are defined using formula 5. Then authors add a feature adaptation stage consiting of deformable convolution (denoted as N_T at fig. 1). Finally, we select spatial positions from a probability map based on some threshold, this gives us a set of region proposals and an adapted feature map.
    
    Experientally, the proposed method gives higher recall with lower number of proposals. This method can be applied to any pretrained two-stage object detector by replacing RPN with GA-RPN and re-training RoI head. Also this method can be applied to single-stage detectors, where the sliding window anchoring scheme is replaced with the proposed guided anchoring. Visualization of results can be seen on fig. 4, 7.

Kirillov, Alexander, Ross Girshick, Kaiming He, and Piotr Dollár. 2019. “Panoptic Feature Pyramid Networks.” arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1901.02446.

    Authors complain that every competitive entry in the panoptic challenges used separate networks for instance and semantic segmentation, with no shared computation. They propose a unified architecture Panoptic FPN for panoptic segmentation. This is a modification of Mask R-CNN, when a semantic segmentation branch is attached to FPN (fig. 3). This branch nearly copies the structure of the FPN, adding new top-down connections, and can be viewed as an assymetric, lightweight decoder (comparing to a symmetric decoder in U-Net, fig. 5). The authors argue that high quality semantic segmentation requires high-resolution, rich, multi-scale features - identify exactly the characteristics of FPN.
    
    Authors train their model simultaneuosly on semantic segmentation and instance segmentation, and show it is a robust and accurate baseline for both tasks.
 
Li, Yanghao, Yuntao Chen, Naiyan Wang, and Zhaoxiang Zhang. 2019. “Scale-Aware Trident Networks for Object Detection.” arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1901.01892.
 
    The authors propose TridentNet for object detection. Given an input image, a usual ResNet backbone processes it 3 times, with dilation rates 0, 1 and 2 in 3x3 convolutions. This can be seen as 3 branches with shared weights, but different dilation rates (fig. 2, 3). Then, RPN head and RoI head with shared weights prosesses each of the three outputs. Each dilation rate is assoclated with some object scale range, and region proposals not in the range are filtered out during training and inference. At inference time, we can either run all the 3 branches, concatenate predictions and apply NMS, or run only the second branch with small performance decrease.
    
    The proposed architecture addresses the problem when Image Pyramid (fig. 1a) achieves true scale invariance while being slow, and FPN (fig. 1b) is faster but region features of objects with different scales are extracted from different levels of FPN backbone, which in turn are generated with different sets of paramers.
    
    IMO, TridentNet and Image Pyramid are quite similar, since in both architectures the backbone is applied several times with shared parameters. Dilated 3x3 convolution can be approximated by downsampling + 3x3 convolution + feature upsampling. In comparison with Image Pyramid, TridentNet cannot be used with an arbitrary pretrained backbone, since it use a modified backbone architecture.
 
Lu, Xin, Buyu Li, Yuxin Yue, Quanquan Li, and Junjie Yan. 2018. “Grid R-CNN.” arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1811.12030.
 
    The authors propose Grid R-CNN object detector, which is a modification of Faster R-CNN that uses a different way to refine bounding boxes in RoI head. Authors define 3x3 relative positions (4 corner points, midpoints of 4 edges and the center point) and apply several dilated convolutions and deconvolutions to RoI-aligned features to predict 9 masks of size 56x56 (fig. 2) for each RoI. Each mask corresponds to some relative position and is supervised by a heatmap when a cross of 5 points in the corresponding point is set to 1 and all other points are set to 0. During inference, the predicted heatmaps are projected back to the original image. For example, to predict the upper boundary of a bounding box, authors use weighted average of y-axis coordinates of the three upper grid points, using predicted probabilities as weights. To be able to predict boundaries outside the RoI (i. e. to expand the RoI) authors extend the representation region of the feature map (fig. 4).
    
    On COCO benchmark, the proposed approach gives 4.1% AP gain at IoU=0.8 and 10.0% AP gain at IoU=0.9, while slightly decreasing AP at IoU=0.5. The categories with the most gains usually have a rectangular or bar like shape (e.g. keyboard, laptop, fork, train, and refrigera- tor), while the categories suffering declines or having least gains usually have a round shape without structural edges (e.g. sports ball, frisbee, bowl, clock and cup).
    
    IMO, the approach is very similar to R-FCN, it seems like both these works use the same approach, differing only in details. Also AttractioNet and CornerNet are quite similar, since they also do not rely on box regression. If out goal is to predict boxes very approximately, it may be enough to use box regression, which is some form of long-range prediction. For example, based on the size of the head, we can approximately predict the size of the whole human. However, if our goal is precise box prediction, we probably should switch to heatmaps, like done in this work.
    
    IMO, the task of precise box regression is overrated, since in practice we often need to either just count objects, or segment objects, but not to detect objects with high box quality. However, even in the task of object counting, accurate box prediction can serve as intermediate step, so that the correct boxes are not suppressed by the boxes of other objects during NMS. To solve this problem we either need to predict more accurate boxes by using things like Grid R-CNN or Repulsion loss, or switch to NMS-free detectors, like Relation networks.
    
    IMO, the future of object detectors are generative ones, when predictions are used to "explain" feature maps (the same as in the case when points are explained by mixture of multivariate normal distributions, as usual in EM-algorithm). If feature map is explained by one predicted object, it no longer requires additional explanations, so no more objects are predicted in this place, eliminating the need for NMS.
 
Li, Buyu, Yu Liu, and Xiaogang Wang. 2018. “Gradient Harmonized Single-Stage Detector.” arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1811.05181.
 
    The authors propose GHM-C loss for binary (foreground/background) classification in single-stage object detectors and GHM-R loss for bounding box regression. These losses aim to harmonize the contribution of easy and hard examples and outperform focal loss in most metrics. Collaborate with GHM, the performance of single-stage detector can surpass state-of-the-art two-stage detectors like FPN and Mask-RCNN with the same ResNext backbone. The motivation and formulation are described below.
    
    The gradient of cross-entropy w.r.t. logit equals P-P*, where P is the predicted probability and P* is the ground truth label. The gradient norm G = |P-P*| can be considered as the contribution of the example to the learning process. The distribution of G across all training samples for a converged one-stage detection model is plotted on fig. 2. The easy negatives have a dominant number and have a great impact on the global gradient. Moreover, we can see that a converged model still can’t handle some very hard examples ("outliers"). Authors hypothesize that if the converged model is forced to learn to classify these outliers better, the classification of the large number of other examples tends to be less accurate.
    
    Consider i-th training samle with gradient norm G\_i. The authors propose to calculate the gradient density, which is the ratio of examples lying in the small-size region centered at G\_i (that is y-axis value on fig. 2). GHM-C loss works in the following way: the cross-entropy loss value i-th sample is divided by the calculated gradient density for this sample (formulas 3-8). This can be seen as dynamic re-weighting training samples: the examples with large density are relatively down-weighted. With the GHM-C loss, the huge number of very easy examples are largely down-weighted and the outliers are slightly down-weighted as well, which simultaneously addresses the attribute imbalance problem and the outliers problem. Further, to make training more stable, authors propose to use exponential moving average (EMA) to smooth gradient density values.
    
    For bounding box regression authors propose GHM-R loss, which works in the similar way and uses dynamic weights based on gradient norm from bouding box regression loss. To make gradient computations more robust, authors propose to replace a regular Huber (smooth L1) loss (formula 15), which is essentially a gradient clipping, by authentic smooth L1 (ASL1) loss (formula 17). Further, gradient density and dynamic weights are calculated w.r.t. to ASL1 loss.
    
    IMO, this paper lacks visualizations of classification outliers. They can be either wrong annotations or examples when "shortcuts" do not work (see the paper "Shortcut Learning in Deep Neural Networks"). In the latter case, down-weighting them may hurt performance in the presence of the distributional shift.
 
Law, Hei, and Jia Deng. 2018. “CornerNet: Detecting Objects as Paired Keypoints.” arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1808.01244.
 
    Authors propose CornerNet one-stage object detector (fig. 4). As a backbone authors use two stacked hourglass networks, taking final feature map from the last layer. For each spatial location, CornerNet predicts 3 outputs: (i) if this location is a top-left corner of any object, (ii) if this location is a bottom-right corner of any object, and (iii) embedding vector for the object, that is used for pairing top-left and bottom-right corners. The model is trained to "pull" corner embeddings for the same object and "push away" corner embeddings for different objects. So, the actual values of the embed-dings are unimportant, only the distances between the embeddings are used to group the corners. To produce tighter bounding boxes, the network also predicts offsets to slightly adjust the locations of the corners, so a corner cannot be localized based on local evidence, and some non-local operation is needed.
    
    As a neck, authors use corner pooling which take the maximum values in two directions, each from a separate feature map, and add the two maximums together (fig. 3). The need for this operation is explained as follows: a corner of a bounding box is often outside the object.
    
    Authors hypothesize two reasons why detecting corners would work better than bounding box centers or pro-posals. First, the center of a box can be harder to localize because it depends on all 4 sides of the object, whereas locating a corner depends on 2 sides and is thus easier, and even more so with corner pooling, which encodes some explicit prior knowledge about the definition of corners. Second, corners provide a more efficient way of densely discretizing the space of boxes: we just need O(w*h) corners to represent O(w^2*h^2) possible anchor boxes.
 
Redmon, Joseph, and Ali Farhadi. 2018. “YOLOv3: An Incremental Improvement.” arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1804.02767.
 
    Authors present YOLOv3, which have minor modifications comparing with YOLOv2. Authors use sigmoid instread of softmax for multi-class classification, since sometimes classes overlap (e. g. "woman" vs "human" in Google Images dataset). Also authors use new ResNet-like backbone with good accuracy-speed tradeoff, which is the aim of the whole work.
    
    While not achieving SOTA, YOLOv3 runs significantly faster than RetinaNet with comparable performance. It may be good at counting the number of zebras in a national park, or tracking someone's cat as it wanders around their house.
 
Cheng, Bowen, Yunchao Wei, Honghui Shi, Rogerio Feris, Jinjun Xiong, and Thomas Huang. 2018. “Revisiting RCNN: On Awakening the Classification Power of Faster RCNN.” arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1803.06799.
 
    Authors propose to augment Faster R-CNN with additional correctional classifier (fig. 4) that crops boxes from the image, passes them to CNN and classifies them, without sharing backbone with Faster R-CNN and without propagating gradient from correctional classifier to Faster R-CNN.
    
    Authors argue that this may solve the problem when classification needs translation invariant feature, whereas  localization needs translation covariant feature, and the proposed solution allows to decrease the count of hard false positives (fig. 3). IMO, too complex argumentation for my brain.
 
Liu, Shu, Lu Qi, Haifang Qin, Jianping Shi, and Jiaya Jia. 2018. “[PANet] Path Aggregation Network for Instance Segmentation.” arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1803.01534.
 
    Authors propose PANet, which is a modification of Mask R-CNN for object detection and instance segmentation. Firstly, authors add another bottom-up path to FPN (fig. 1b). While the red line on fig. 1 consists of 100+ layers, the green "shortcut" line consists of less than 10 layers. Authors hyphothesize that this solution "enhances the localization capability of the entire feature hierarchy by propagating strong responses of low-level patterns". As a result, the backbone produces feature pyramid denoted N2, …, N5.
    
    Given a region proposal, authors perform adaptive feature pooling (fig. 1c, fig. 6). This operation performs RoIAlign from all levels of feature pyramid, firstly process each region incependently, then fuse them all with elementwise max operation. It turns out that 50%+ of the features are pooled from lower levels. "This observation clearly indicates that features in multiple levels together are helpful for accurate prediction. It is also a strong support of designing bottom-up path augmentation."
    
    Finally, authors add a head for classification and box prediction (fig. 1d) and a custom head for mask prediction (fig. 1e, fig. 4). Ablation studies justify standalone usefulness of all described modifications.
    
    IMO, the whole backbone is another example when where we add short connections while keeping long ones, which boosts performance (another examples are ResNet, DenseNet, RNN with attention and transformer). The PANet backbone can actually be re-drawn as U-Net + additional stack of layers. As for adaptive feature pooling, "lower levels" and "higher levels" doesn't look like a very appropriate terms here, since all these layers may contain semantically strong features that came from the higher layers P4-P5.

Kirillov, Alexander, Kaiming He, Ross Girshick, Carsten Rother, and Piotr Dollár. 2018. “Panoptic Segmentation.” arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1801.00868.

    Authors propose a panoptic segmentation task where the goal is to segment both "thing" instances (countable objects such as people, animals, tools) and "stuff" areas (amorphous regions of similar texture or material such as grass, sky, road). To make panoptic annotation, one should first choose a set of thing and stuff classes. Each pixel of an image must be assigned a class label and an instance id, when for "stuff" classes instance id is ignored, and instance order does not matter. Ambiguos or out-of-class pixels are labeled as "void". Such annotation format is a strict generalization of the format for semantic segmentation, and does not allow overlapping segments, which are allowed in instance segmentation. Authors note that humans are not always consistent in panoptic segmentation tasks: some inconsistencies are shown in figures 3, 4.
    
    Authors propose a panoptic quality (PQ) metric. To calculate this metric, we first need a predictions in the same format as annotations. We associate predicted and ground truth segments that have mask IoU > 0.5. Authors prove that this IoU threshold produces unique matching: each ground truth segment can have at most one corresponding predicted segment with IoU > 0.5 and vice versa. Then for each class, the unique matching splits the predicted and ground truth segments into three sets: true positives (TP), false positives (FP), and false negatives (FN). PQ metric is a multiplication of segmentation quality (average IoU) and recognition quality terms (fig. 2, formula 2).
    
    Authors propose a baseline that leverages pretrained models for instance segmentation and semantic segmentation. Instance segmentation models may produce overlapped segments, so NMS-like procedure is applied. We iterate over sorted instances, starting from the most confident. For each instance we first remove pixels which have been assigned to previous segments, then, if a sufficient fraction of the segment remains, we accept the non-overlapping portion, otherwise we discard the entire segment. Then, we combine instance segments with semantic segmentation results by resolving any overlap between thing and stuff classes in favor of the thing class (this heuristic is imperfect but sufficient as a baseline).
    
    Authors note that in the pre-deep learning era there was interest in the joint semantic+instance segmentation task described using various names such as scene parsing, image parsing, or holistic scene understanding. however later the schism between semantic and instance segmentation has led to a parallel rift in the methods for these tasks: stuff classifiers are usually built on FCN with dilations while object detectors often use object proposals and are region-based.
    
    IMO, "scene parsing" seems inappropriate term here, because "parsing" sounds like inferring 3D world from the image, and a perfect "image parsing" also requires amodal completion, depth estimation, keypoint segmentation, fine-grained classification etc.

Cai, Zhaowei, and Nuno Vasconcelos. 2017. “Cascade R-CNN: Delving into High Quality Object Detection.” arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1712.00726.

    Authors propose Cascade R-CNN model for object detection (fig. 3d) that extends Faster R-CNN (fig. 3a) with cascade of sequential RoI heads. Firstly, RPN (denoted as H0, omitted in fig. 3d) performs foreground-vs-background classification (C0) and box regression (B0), thus returning a set of proposals. Next, the first head (H1, C1, B1) performs box regression for each proposal to refine its coordinates. Then, refined box proposals are RoI pooled again and passed to the second head (H2, C2, B2), and so on (3 heads in total). During training, box proposals are passed through stages, while each stage has it's own IoU requirement (between predicted and target boxes): 0.5, 0.6, and 0.7 for three stages, respectively. On each stage, we filter box proposals that do not fit the IoU requirements (termed "outliers" on fig. 2).
    
    On inference, we can use outputs from any head. Authors find that outputs from the third head give the highest AP50-95 on MS COCO test set. If we add 4th head, AP50-95 slightly decreases. while AP90 continues to improve (fig. 4).
    
    Authors provide a detailed justification for Cascade R-CNN architecture. When training a bounding box regressor, we usually use IoU hyperparameter to divide boxes into positive (which should be corrected to target boxes) and negative (which should be ignored). Authors found that training IoU matters. Let we train two box regressors: one with IoU=0.5 and another with IoU=0.7. It turns out that the first regressor is good at correcting very inaccurate proposals, but even degredes already accurate proposals. The second regressor is better than the first on accurate proposals, but is much worse on inaccurate proposals (fig. 1a). Authors conclude that each regressor has its own specialization and a single regressor can only be optimal for a single quality level.
    
    (IMO, this conclusion doesn't seem 100% justified, at least we could try to perform box regression in another way, by binning the output space and output probabilities instread of regressing single numbers, as done in AttractioNet. Also, we can try constrained box regression, while each stage cannot perform a large box shifts and can perform only small ones)
    
    A vanilla Faster R-CNN is depicted on fig. 3a. Some works have argued that a single box regression step is insufficient for accurate localization, and applied several regression steps iteratively (fig. 3b). Here the same head is used for each step. This ignores the above problem that a regressor trained at IoU=0.5 is suboptimal for box proposals of higher IoUs (even degrades them), and vice versa. While boxes become mor accurate after fist stages, subsequent stages may degrade them. Due to these problems, iterative box regression requires a fair amount of human engineering, in the form of proposal accumulation, box voting, etc., and has somewhat unreliable gains. Usually, there is no benefit beyond applying the same box regressor twice.
    
    One possible solution is to train a box regressor with multiple IoUs by using integral loss, which is a combination of several losses with different IoU thresholds (formula 6). But the problem here is that different losses operate on different numbers of positives: the set of positive samples decreases quickly with increasing IoU threshold, so we may face overfitting for high-IoU loss terms. Such approach has very little gain over vanilla Faster R-CNN.
    
    This naturally leads to Cascade R-CNN design, when we train a separete box regressor for each refinement stage. The regressors of the deeper stages are optimized for higher IoU thresholds and accept boxes from previous stage, that are filtered by IoU during training. There is no overfitting here, since examples are plentiful at all levels.

Hu, Han, Jiayuan Gu, Zheng Zhang, Jifeng Dai, and Yichen Wei. 2017. “Relation Networks for Object Detection.” arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1711.11575.

    Authors propose a relation modules, which replace RoI head and NMS stage in two-stage object detectors.
    
    1. Instance recognition stage. A relation module accepts as input RoI-pooled feature vectors and bounding boxes from several RoIs, and is essentially a self-attention between feature vector, with one modification: in addition to feature vectors, it uses bounding box coordinates. A usual attention calculates softmax(attn\_logits), which is an L1-normalization of exp(attn\_logits). In relation module, exp(attn_logits) is additionally multiplied by scores that reflect relative position or bounding box pairs (formula 5). Authors use several attention heads and concatenate their outputs (fig. 2). So, a relation module can be viewed as Multi-Head Self Attention (MHSA) with additional usage of bounding box information. In contrast to transformers ("Attention is all you need"), position-wise MLP is not applied after each MHSA layer, and instead fully-connected layer is applied before the first layer and after first several layers. After the last layer, a linear layer is applied to calculate scores and perform bounding box regression (formula 10).
    2. Duplicate removal stage. Authors replace NMS with modified version of relation modules (fig. 3, right). As well as region embeddings and boxes, these modules also use scores from the previous stage. These scores are sorted in descending order, obtaining ranks, which are then embedded into a higher dimensional feature vector and added to region embeddings obtained from the previous stage. Further details see in sec. 4.3.
    
    As a result, authors obtain the first end-to-end object detector without NMS post-processing stage. It may seem like end-to-end training presents a problem, because the goals of instance recognition step and duplicate removal step seem contradictory (the former expects all objects matched to the same ground truth object to have high scores, but the latter expects only one of them does). Nevertheless, authors found the end-to-end training works well.
    
    Authors hypothesize that the proposed approach can be applied to other tasks such as instance segmentation, action recognition, object relationship detection, caption, VQA, etc.

Liu, Songtao, Di Huang, and Yunhong Wang. 2017. “Receptive Field Block Net for Accurate and Fast Object Detection.” arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1711.07767.

    Based on neuroscience studies, authors propose Receptive Field Block (RFB) by concatenating outputs of multiple convolutions with different weights and dilation rates (fig. 2). In fig. 3, authors compare RFB with Inception block, atorous spatial pyramid pooling and deformable convolution. Authors incorporate RFB in SSD detectior with a lightweight backbone, getting RFB Net: an object detector with high quality and computational efficiency.

Wang, Xinlong, Tete Xiao, Yuning Jiang, Shuai Shao, Jian Sun, and Chunhua Shen. 2017. “Repulsion Loss: Detecting Pedestrians in a Crowd.” arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1711.07752.

    Authors propose a Repulsion Loss (RepLoss) for box regression in object detection (formula 1). It adds two additional terms to a regular (L1 or IoU) loss. Let we have a predicted box and an associated target box. The first term (RepGT) penalizes IoU between predicted box and other targets (fig. 1), that could effectively stop a predicted bounding box from shifting to its neighboring objects which are not its target. The second term (RepBox) penalizes IoU between the current predicted box and predicted boxes with different associated targets. This term is able to reduce the probability that the predicted bounding boxes with different regression targets are merged into one after NMS.
    
    In the results of RepBox, there are fewer predictions lying in between two adjacent ground-truths, which is desirable in crowd scenes in pedestrian detection (fig. 7). Also, RepLoss improves mAP on PASCAL VOC detection.

Lin, Tsung-Yi, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár. 2017. “[RetinaNet] Focal Loss for Dense Object Detection.” arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1708.02002.

    Authors propose a focal loss for classification head in one-stage object detectors. This loss is a dynamically scaled cross entropy loss, where the scaling factor decays to zero as confidence in the correct class increases (fig. 1). This scaling factor can automatically focus the model on hard examples during training. So, the focal loss performs the opposite role of a robust loss (for example, Huber loss) that reduce the contribution of outliersby down-weighting the loss of examples with large errors. Authors describe the focal loss for binary classification, while extending it to multi-class classification is straighforward. Focal loss have two hyperparameters: alpha and gamma (see formula 5 when p_t is defined by formula 2 and alpha_t is described in 3.1). When alpha=0.5 and gamma=0, focal loss reduces to the logloss.
    
    Focusing on hard examples is important for one-stage object detectors because of high foreground-vs-background class imbalance: these detectors evaluate 10-100 thousands of candidate locations per image but only a few locations contain objects. Also, to increase stability in early training, authors propose changes in initialization (sec. 3.3).
    
    Authors propose a RetinaNet model for object detection, which incorporates focal loss and have anchor box classification and regression heads (for 9 anchors) attached to each level of FPN on top of ResNet architecture (fig. 3). As for focal loss, gamma=2 works well in practice and the RetinaNet is robust to gamma from 0.5 to 5. Authors report SOTA on MS COCO detection, surpassing even two-stage architectures.
    
    IMO, focal loss is a controversial solution. In my experiments for few-shot object detection with YOLOv5, applying focal loss affects a confidence interval that yields good F1 measure on a test set. Focal loss makes this interval (i) narrow, (ii) dataset-dependent, and (iii) epoch-dependent (usually shifts to the right during training). Hence, we usually cannot be satisfied with threshold 0.5 and need a large enough validation set to select the threshold, which is not accessible in few-shot learning. Just to mention, focal loss is disabled by default in YOLOv5.

He, Kaiming, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. 2017. “Mask R-CNN.” arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1703.06870.

    Authors propose a model Mask R-CNN to perform simultaneous object detection and instance segmentation. Comparing to vanilla Faster R-CNN, it has several changes.
    
    Firstly, RoIPool is replaced by RoIAlign. Just like RoIPool, it takes a feature map and region of interest and returns a feature map of fixed size, but it has no qualization and instead use bilinear interpolation (fig. 3). This boosts performance even for object detection, but for segmentation it has even larger effect.
    
    To predict masks, authors add another branch to RoI head (that accepts fixed-size feature map from RoIAlign). It consists of conv and deconv layers and outputs 80 masks corresponding to 80 MS COCO classes (fig. 4). The model is trained on both object detection and instance segmentation annotations. For mask predictions, authors use sigmoid instead of softmax over classes, because we already have a brach to predict a class for an object, so there is no need to predict class for every mask point. Loss is applied only on ground truth class mask. Authors note that if segmentation head returns only one class-agnostic mask, this decreases performance just for a little bit. However, replacing sigmoid with softmax over classes decreases performance very significantly.
    
    Authors train two variants of Mask R-CNN with two different backbones. First variant (ResNet-50-C4) uses ResNet as a backbone, and feature map is obtained from C4 layer, when the 5-th stage of ResNet (which is compute intensive) is moved into RoI head (fig 4., a). Output mask has a size of 14x14 for each RoI. Second variant (ResNet-50-FPN) uses Feature Pyramid Network (FPN) as a backbone, concatenates features from multiple FPN outputs, has a lighter RoI head and outputs mask of size 28x28 for each RoI (fig 4., b). Second variant is less compute and memory intensive, so it allows backpropagating through more RoI at each step: "each image has N sampled RoIs, with a ratio of 1:3 of positive to negatives. N is 64 for the C4 backbone and 512 for FPN". FPN version yields better quality than C4. ResNeXt-101-FPN backbone achieves the best quality overall (table 2a).
    
    Mask R-CNN outperforms another competitive models with multi-scale train/test, horizontal flip test, and online hard example mining (OHEM). Authors expect such improvements to be beneficial also for Mask R-CNN, leaving this work for the future.
    
    Also, authors extent Mask R-CNN framework to predict human keypoints by training mask head to predict a mask for each keypoint. In this case, for each keypoint mask the target is only one pixel corresponding to the position of the keypoint, and authors apply softmax over spatial axes.

Dai, Jifeng, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, and Yichen Wei. 2017. “Deformable Convolutional Networks.” arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1703.06211.

    Authors propose a deformable convolution layer (fig. 2, 5). For simplicity, consider a specific filter size 3x3 and C output channels. In a regular 3x3 convolution we have 9 relative spatial locations: R = {(-1, 1), (-1, 0), ..., (1, 1)}. In contrast, a deformable 3x3 convolution filter is two-stage. At the first stage, we apply a regular convolution with 2x9 output channels and obtain float-point (x, y) offsets for each relative location in R. It worth noting that these offsets are calculated independently for each spatial location.
    
    At the second stage, we apply a convolution with C output channels, while adding offset for each relative location in R. This gives us fractional offsets, and to handle them we apply bilinear interpolation to input feature map. This can be seen as sampling positions from feature map with float-point offsets and then applying a regular convolution. Both stages are learnable and differentiable, so are easy to integrate into any CNN architectures
    
    Also, authors propose a deformable RoI pooling (fig. 4) that works the same way. As well as in deformable convolution, this allows to select spatial locations at runtime for each image.
    
    In fig. 6, 7, authors visualize selected offsets for deformable convolution and selected regions for deformable RoI pooling. An initial motivation was better robustness to geometrical transformations, and authors demonstrate the superiority of their methods for DeepLab, class-aware RPN, faster R-CNN and R-FCN models.

Fu, Cheng-Yang, Wei Liu, Ananth Ranga, Ambrish Tyagi, and Alexander C. Berg. 2017. “DSSD : Deconvolutional Single Shot Detector.” arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1701.06659.

    This is a concurrent work with FPN, when authors also aim to combine low-level, but spatially accurate features from low levels of CNN, and high-level features from high layers, to better detect small objects. Proposed architecture is very similar to FPN, but a module that combines features from different layers is more complex. It uses deconvolution, batch normalization and combines features with product instead of sum, because the experimental results show that the element-wise product provides the best accuracy (fig. 1, 2, 3). As in SSD, when training, each layer is responsible for some scale of anchor boxes.

Redmon, Joseph, and Ali Farhadi. 2016. “[YOLOv2] YOLO9000: Better, Faster, Stronger.” arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1612.08242.

    Authors propose YOLOv2 model for object detection. While first version of YOLO predicts two boxes per spatial location and use bipartite matching to apply loss, YOLOv2 predicts 5 or 9 anchor boxes per spatial location, as in RPN. But in contrast to RPN, anchor box sizes and aspect ratios were obtained automatically  from MS COCO by K-means clustering, which gives 5% mAP gain comparing to hand-crafted boxes. Authors say that K-means anchor boxes allow to largely increase maximum recall, while slightly decrease mAP, comparing to no anchors and bipartite matching. Secondly, authors propose to increase feature map resolution from 13x13 to 26x26 by concatenating features from two last layers. Finally, YOLOv2 has new backbone with batch normalization and is trained on varying resolutions from 320 to 608. All improvements and metric gains are listed in table 2.
    
    In addition, authors propose YOLO9000 (as an extension for YOLOv2) that was trained simultaneously for detection and image-level classification and is able to detect >9000 classes. They take 9000 classes from ImageNet and combine these classes with MS COCO classes by using "WordTree" hierarchical classification scheme (fig. 6). where each class has a list of parents, starting from "physical object". At each spatial location YOLO9000 predicts scores for classes and their parents. Given an image with image-level class C, we search for a predicted box with the highest score for this class and apply classification and objectness losses to this box, so this is a form of weak supervision. Authors note that YOLO9000 struggle to localize objects that are completely different from MS COCO classes: "COCO does not have bounding box label for anytype of clothing, only for person, so YOLO9000 struggles to model categories like “sunglasses” or “swimming trunks”.

Lin, Tsung-Yi, Piotr Dollár, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. 2016. “[FPN] Feature Pyramid Networks for Object Detection.” arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1612.03144.

    Authors propose Feature Pyramid Network (FPN), see fig. 1 (d) as a backbone for obect detection. Comparing to a regular CNN backbone, FPN contains top-down connections, which are implemented as nearest neighbor upsampling, 1x1 conv and addition (see fig. 3). This is very similar to SharpMask model for segmentation from the same authors.
    
    FPN returns multiple feature maps of different spatial resolution. Then, RPN and RoI pooling heads from Faster R-CNN are attached to all feature maps with shared parameters. Faster R-CNN based on FPN achieves both good quality and good FPS on detection task.
    
    Fig. 1 compares FPN with different alternatives. Fig. 1 (b) is an object detection head on top of CNN encoder. To increase quality, we can scale an image to different sizes and apply CNN + detection head to each size (a), but this is slow. Instead, we could attach detection head to different CNN layers (c), but authors complain that in this case first feature maps will be produced by pretty shallow CNN encoder, so are not "semantically strong". We could add more CNN layers before the first feature map, as done in SSD architecture, but thus we either need a lot of FLOPS (if there is no pooling layers) or the first feature map will already have low spatial resolution. FPN is aimed at solving these problems.
    
    IMO, FPN closely resembles U-Net, up to some minor implementation details, such as addition instead of concatenation in top-down path. Authors do compare with U-Net, noticing that in FPN predictions are made independently at all levels.

Zhang, Liliang, Liang Lin, Xiaodan Liang, and Kaiming He. 2016. “Is Faster R-CNN Doing Well for Pedestrian Detection?” arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1607.07032.

    Authors develop a network for object detection based on RPN plus boosted forest. Interesting in this work is that by default RPN performs well as a stand-alone pedestrian detector, but the downstream classifier degrades the results. Authors say that this is because (i) insufficient resolution of feature maps for handling small instances which can lead to “plain” features caused by collapsing bins, and (ii) lack of any bootstrapping strategy for mining hard negative examples.

Gidaris, Spyros, and Nikos Komodakis. 2016. “[AttractioNet] Attend Refine Repeat: Active Box Proposal Generation via In-Out Localization.” arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1606.04446.

    Authors propose AttractioNet: an iterative scheme to generate region proposals for two-stage object detectors. Given an image feature map and some region, the object location refinement module first expands the region, then performs bilinear pooling (similar to RoI pooling) to obtain a feature map of the fixed size, processes it and and predicts 2M probabilities. Each of these probabilities means whether the specific row or column belongs to the object bounding box (fig. 3). This allows to refine box coordinates and can be viewed as a form of bounding box regression. Also, for each region the objectness score is predicted (does this region enclose some object or not).
    
    Starting from the set of "seed" regions, uniformly distributing across the image, we apply the described operations several times (fig. 4) to get the final region proposals.
    
    With AttractioNet and a VGG16-Net based detector authors report the detection performance on COCO that significantly surpasses all other VGG16-Net based detectors while even being competitive with a heavily tuned ResNet-101 based detector. Also, AttractioNet generalizes to unseen categories to a certain extent.

Chen, Liang-Chieh, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L. Yuille. 2016. “DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs.” arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1606.00915.

    Authors propose a DeepLab model for semantic segmentation. This model is based on a simple baseline when CNN directly converts image to class probabilities for each spatial position, and logloss is applied. Authors propose several modifications for this baseline.
    
    Firstly, authors propose atrous convolution layer (fig. 2). Comparing to the regular convolution, atorous convolution has larger receptive field, where some spatial positions are "disabled" (are constant zero). This gives large receptive field with a relatively small number of trainable weights. Comparing to pool + conv layer combination, atorous convolution also enlarges receptive field, but gives higher-resolution feature map (fig. 3). However, it is requires more computations, so authors don't rely solely on them, and add with bi-linear interpolation layers for upsampling (fig. 1).
    
    Secondly, authors propose Atrous Spatial Pyramid Pooling (ASPP) (fig. 4). Instead of running the whole model for an image resized to different sizes, authors apply atorous convolution of different rate to feature map, and merge predictions.
    
    Thirdly, authors propose to refine predictions with Conditional Random Field (CRF). This operation accepts the original image and the output probability map as inputs, defines an energy function (formulas 2, 3) and iteratively minimizes it. The first term in (3) forces pixels with similar color and position to have similar labels, while the second term enforces smoothness. More information on CRFs is available in the paper "Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials" (2012). Examples can be seen in fig. 6, 11. 
    
    Also, authors note that they use CRF only as post-processing, while some other works pursue joint learning of the CNN and CRF, by either unrolling the CRF mean-field inference steps to convert the whole system into an  end-to-end trainable network, or by approximating one iteration of the dense CRF mean field inference by convolutional layers with learnable filters.

Dai, Jifeng, Yi Li, Kaiming He, and Jian Sun. 2016. “R-FCN: Object Detection via Region-Based Fully Convolutional Networks.” arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1605.06409.

    Authors propose two-stage network for object detection, when RoI network is shallow and contains only one not trainable layer. Usually shallow RoI networks fail to perform well (see sec. 1), so authors develop a special scheme with interpretable feature map. There are 3 heads attached to the backbone: (i) region proposal network, (ii) score map, and (iii) box regression head. All these heads are run once per image. Score map contains k*k*(C+1) output channels (fig. 2), when C is the number of classes and k=3. So, for each class score map returns 3x3 scores, when each score is associated with some object corner (top-left, bottom-center, center-center etc.). For example, if some spatial location contains a top-left corner for a ground truth box of class "car", then we want a corresponding cell in score map to be activated.
    
    Score map is computer wor the whole image. Given a region proposal, we apply a "position-sensitive RoI pooling", that is illustrated on fig. 3. For each class we do the following. Firstly, we perform average pooling from "top-left" score map and top-left area of a region proposal. Then, we perform average pooling from "top-center" score map and top-center area of a region proposal, and so on. As a result, for each class we get 3x3 scores. Then we average them (denoted as "vote" on fig. 3) and get final score for a class and a region proposal.
    
    So the whole RoI head is made from shallow and not learnable operations with negligible per-RoI computation. Nearly the same operation is performed for box regression head (do not described in details here).
    
    All network is trained end-to-end, so, we do not apply loss to the score map and instead propagate a gradient through RoI head. Also, authors propose to adopt online hard example mining (OHEM) for training, because negligible per-RoI computation enables nearly cost-free example mining.
    
    This approach closely resembles InstanceFCN instance segmentation model from the same authors. In comparison, no segmentation annotation is used here, but the resulting score maps (fig. 3, 4) are noisy and not suitable for segmentation task, so R-FCN is not shown to be effective as box-supervised segmentation method.

Li, Ke, and Jitendra Malik. 2016. “Amodal Instance Segmentation.” arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1604.08202.

    Authors generate training data for amodal instance segmentation in the following way. They start from the dataset annotated with modal instance segmentation annotations. Then they crop random objects by their masks and paste on another objects, partially occluding them. Then they pose the following ML task: given a modal bounding box, the task is to predict amodal mask and bounding box.
    
    Since initial masks may also be partially occluded by another objects, authors mark pixels belonging to another objects as "unknown": no loss is applied for these points (blue color on fig. 2). Authors claim that object may be occluded by another object, but not by background (IMO it's debatable).
    
    Authors use the following pipeline: given a modal bounding box, a pretrained frozen model produce modal segmentation heatmap, and then trainable model predicts the amodal segmentation mask and bounding box in an iterative fashion using a new strategy that will be referred to as Iterative Bounding Box Expansion (see details in sec. 4).

Shrivastava, Abhinav, Abhinav Gupta, and Ross Girshick. 2016. “[OHEM] Training Region-Based Object Detectors with Online Hard Example Mining.” arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1604.03540.

    Authors propose a method called online hard example mining (OHEM) to incorporate hard sample mining to SGD to fight foreground-vs-background class imbalance in object detection task (in object-proposal-based detectors imbalance may be 70:1, while in detectors based on sliding windows imbalance may be 100K:1).
    
    In OHEM, given a set of region proposals, we calculate loss for each proposal, take N highest losses and backpropagate them. However, there is a small caveat: co-located RoIs with high overlap are likely to have correlated losses. To deal with this, we perform NMS before selecting RoIs with the highest losses. To save computations and memory, we calculate losses for all RoIs in no-grad mode, then after sorting losses we again perform forward pass, then backward pass.
    
    OHEM achieves higher test mAP than both (i) random sampling foreground and background region proposals in some ratio, like in Fast R-CNN, and (ii) training the network on all region proposals, that is calculating and backpropagating RoI head losses for each region proposal.
    
    IMO, the problem of foreground-vs-background class imbalance is not justified enough in this work. Probably it is metric-dependent: should we face it when using FP+FN (a close analogue of accuracy) instead of mAP as metric?

Dai, Jifeng, Kaiming He, Yi Li, Shaoqing Ren, and Jian Sun. 2016. “[InstanceFCN] Instance-Sensitive Fully Convolutional Networks.” arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1603.08678.

    Authors propose InstanceFCN: a fully-convolutional architecture for class-agnostic instance segmentation. At each spatial position InstanceFCN predicts 9 classification scores corresponding to 9 relative positions (top-left, top-center, center-center etc.). A classifier should predict true if there is some foreground object in this position, and this position relates to a specified relative position of this object. Also, an objectness score is predicted for each spatial position. At inference time, all predictions are assembled to produce instance segmentation (fig. 4).

Liu, Wei, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C. Berg. 2015. “SSD: Single Shot MultiBox Detector.” arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1512.02325.

    Authors proposed SSD architecture for object detection. This is very silimar to YOLO (see fig, 1, 2), but instead of one feature map, authors apply detection head to several feature maps from several last layers of CNN encoder. Also, authors perform classification for each anchor box, instead of each spatial position in YOLO.
    
    Later, the idea of producing several feature maps with a single backbone forward pass will come to Feature Pyramid Network (FPN) architecture.

Huang, Lichao, Yi Yang, Yafeng Deng, and Yinan Yu. 2015. “DenseBox: Unifying Landmark Localization with End to End Object Detection.” arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1509.04874.

    Authors propose a model DenseBox for object detection. Spatial resolution of CNN outputs are increased by upsampling, and for each spatial location (corresponding to 4x4 pixels) model peforms bounding box regression and object-vs-background classification (for a single class "face"). For classification, the ground-truth region is a circle inside ground truth bounding box.
    
    IMO, this is very similar to YOLO. One difference is that YOLO use bipartite matching, while DenseBox uses groud truth mask for object-vs-background classification. Comparing with YOLO, authors say that "Our DenseBox uses up-sampling layers to keep a relative high-resolution output, with a down-sampling scale factor of 4 in our model. This enables our network capable to detect very small objects and highly overlapped objects, which YOLO is unable to deal with."

Zhu, Yan, Yuandong Tian, Dimitris Mexatas, and Piotr Dollár. 2015. “Semantic Amodal Segmentation.” arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1509.01329.

    Authors pose a task of amodal segmentation: segmenting the full extent of objects, not just visible pixels (fig. 2). Authors ask humans to annotate regions in images amodally and find that this is a well-posed annotation task: the agreement between independent annotators is high. Naturally, annotations are most consistent for regions with simple shapes and little occlusions. On the other hand, when the object is highly articulated and/or severely occluded, annotators tend to disagree more (fig. 9). So, the authors collect an MS COCO subset of 5000 images with amodal annotations.
    
    As a baselines, authors propose two ways: (i) to take modal mask from another model and output an amodal mask, or (ii) to directly predict amodal masks from image patches. Authors evaluate models for varying occlusion levels q: none (q=0), partial (0<q≤ .25), and heavy (q>.25) and summarize metric in table 3.

Pinheiro, Pedro O., Ronan Collobert, and Piotr Dollar. 2015. “[DeepMask] Learning to Segment Object Candidates.” arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1506.06204.

    This is the concurrent work with Faster R-CNN, authors here also aim to extract object proposals for two-stage object detectors. The proposed architecture and training data is shown in fig. 1. Given an image patch, VGG encodes in into a feature map. Classification head predicts if the patch contains any object that is located approximately in the center and has a size of approximately 1/2 of the whole patch. If such object exists, segmentation head predicts it's mask (otherwise no loss is appplied to the segmentation head, so it is free to predict anything, which is good for generalization to unseen categories). The segmentation head starts with 1x1 convolution, then goes 14x14 convolution without activation and finally goes classification output of size 56x56. Last two layers are linear, so they are low-rank version of a single fully connected layer.
    
    During inference, the model is applied densely at multiple locations and scales. For each scale, VGG can be applied once, since it's fully convolutional. Ane example output can be seen in fig. 2.
    
    IMO, the connection between DeepMask and RPN (from Faster R-CNN paper) is interesing. While RPN performs several classifications and box regressions for each spatial position, DeepMask performs a classification and outputs 56x56 mask, but requires segmentation annotations for training. The weights from the last segmentation layer could be visualized as 512 grayscale patches of size 56x56. Probably, these patches could be even made predefined and untrainable. Also, we could add box regression on top of the output segmentation mask, obtaining something similar to RPN.
    
    The recent "Segment Anything" model (2023) is somewhat similar, because it can also return a mask given some point as prompt, but instead of convolution with fixed-size kernel it is based on attention, therefore it is more flexible.

Redmon, Joseph, Santosh Divvala, Ross Girshick, and Ali Farhadi. 2015. “[YOLO] You Only Look Once: Unified, Real-Time Object Detection.” arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1506.02640.

    Authors propose YOLO network for object detection, which resembles region proposal network (RPN) in Faster R-CNN, but performs multi-class classification. An image can be divided into grid of spatial locations, and YOLO predicts B bounding boxes for each grid cell (B is a hyperparemeter, by default B=2). Each box is predicted as 1) box regression (x, y, w, h), 2) confidence score and 3) class probabilities (summed to 1), which are the same for all B boxes in a grid cell. So, given N classes, for each grid cell YOLO predicts 4B + B + N numbers.
    
    In contrast to RPN, YOLO does not use anchor boxes. "We assign one predictor to be “responsible” for predicting an object based on which prediction has the highest current IOU with the ground truth. This leads to specialization between the bounding box predictors. Each predictor gets better at predicting certain sizes, aspect ratios, or classes of object, improving overall recall." Such runtime matching between set or list of predictions and set of ground truths is called bipartite matching (see more details in DETR paper, section 2.3).
    
    After assigning ground truth boxes to predicted boxes, we can apply losses for box regression, confidence and classification, when confidence ground truth value is IoU between predicted and ground truth boxes.
    Comparing to OverFeat, authors say that "OverFeat … is still a disjoint system. Over-Feat optimizes for localization, not detection performance. … the localizer only sees local information when making a prediction. OverFeat cannot reason about global context". Comparing to Fast and Faster R-CNN, authors say that "While they offer speed and accuracy improvements over R-CNN, both still fall short of real-time performance."
    
    IMO, YOLO architecture is almost identical with RPN: the only difference between them is the presence of multi-class classification and a way to apply losses. Also, the problem with YOLO is that it cannot distinguish between box coordinates uncertainty and object presence uncertainty (confidence is a combination of both uncertainties). Also it would be interesting to train 1x1 conv YOLO output layer on the top of the frozen pretrained backbone, because it is a good way to check the ability of the backbone to disentangle scale and object class, because both scale and class will be produced as a dot product, so are related to some directions in embedding space.

Ren, Shaoqing, Kaiming He, Ross Girshick, and Jian Sun. 2015. “Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks.” arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1506.01497.

    Authors propose Faster R-CNN network for object detection. They augment Fast R-CNN with Region Proposal Network (RPN), that is used for first stage: extracting region proposals (fig. 2). RPN accepts feature map from backbone as input, consists of 3x3 conv layer and two output 1x1 conv heads (actually they can be viewed as one): classification head and regression head (fig. 3). So, RPN and ROI head share the same backbone.
    
    For each spatial location in RPN output, we associate k "anchor boxes": rectangular regions of 3 different scales and 3 different aspect ratios, centered in this location. There are typically 2400 spatial positions in RPN, and so 2400x9 anchors. Classification head predict a score for each position and anchor box, and regression head performs bounding box regression for each position and anchor box.
    
    During training, every anchor box is either marked positive and associated with some ground truth box, or marked as negative, or marked as ambigous (see 3.1.2 for details). RPN learns to classify positively anchors marked as positive and classify negatively anchors marked as negative. For anchors marked as positive, RPN learns to perform bounding box regression to refine it's coorinates. When training RPN, each mini-batch arises from a single image, where the sampled positive and negative anchors have a ratio of up to 1:1.
    Authors propose several ways to train the whole model (see 3.2): (i) alternating training, when we alter between training RPN stage and training RoI head stage, and (ii) approximate joint training with multi-task loss. Joint training is called "approximate" because RoI pooling layer by default is not differentiable w.r.t. the box coordinates, but still achieves good results.
    
    IMO, RPN network is equivalent to single-class YOLO, so 1) in single-class scenario RPN can be used as final predictor supplemented by NMS stage, 2) in multi-class scenario RPN's classifier may be made multi-class, getting YOLO as a result. So, a drawback of this paper is absence of ablation studes, when in single-class detection tasks only RPN is used without ROI pooling. See also paper "Is Faster R-CNN Doing Well for Pedestrian Detection?". Also it could be interesting to check if combining box scores from RPN and RoI head can benefit.

Oquab, Maxime, Léon Bottou, Ivan Laptev, and Josef Sivic. 2015. “Is Object Localization for Free? - Weakly-Supervised Learning with Convolutional Neural Networks.” In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 685–94.

    Authors train a weakly-supervised image recognition model. During training, only image-level annotations are used in form of binary vector where i-th element indicate the presence of i-th class. So, we don't use any information about the number and position of objects of this class. During inference, for each class present in the image, a model should return coordinates (x, y) pointing to any object of this class (fig. 6).
    
    To achieve this, author's model returns an array of class logits for each spatial position of size (W, H, N), when (W, H) is a size of spatial grid (32x less than size of image) and N is the number of classes. After that, sigmoid is applied to get multi-class classification for each spatial position. Then global max pooling is applied, thus we get (i) multi-class image-level classification, and (ii) coordinates for each class. Finally, logloss is applied to resulting image-level class probabilities. Interestingly, for negative images thiscan be seen as hard-negative mining, when we select a point that is the most confusing to the model and optimize it. On inference, such model may return points or even masks for every class (fig. 1).
    
    Authors train model on PASCAL VOC and MS COCO without box annotations. In addition, they rendomly rescale images, instead of costly multi-scale training. It turns out that the quality of the resulting model is not far from R-CNN trained with box supervision in terms of localization (specifying some point), but is much worse in terms of bounding box quality. In authors' experiments, introducing box supervision with masked pooling instead of global pooling doesn't help too much.
    
    Authors say that a problem with this approach is the possibility to classify based on background: "Fore example, the presence of a baseball field is a strong indicator for presence of a baseball bat and a baseball glove." (see also paper "Object Recognition with and without Objects"). IMO, another problem is that if we imagine a perfect model that, given a car, will always classify as "car" some point above the car, it will achieve ideal loss on author's training task, but will still localize incorrectly.

Ronneberger, Olaf, Philipp Fischer, and Thomas Brox. 2015. “U-Net: Convolutional Networks for Biomedical Image Segmentation.” arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1505.04597.

    Authors propose a model U-Net for semantic segmentation (fig. 1), which combines feature maps from different layers (instead of combining final predictions, as done in FCN paper). In U-Net, the expansive path is more or less symmetric to the contracting path, and yields a u-shaped architecture. To train on small amount of biomedical segmentation data, authors use augmentations like cropping and elastic deformations.

Gidaris, Spyros, and Nikos Komodakis. 2015. “Object Detection via a Multi-Region & Semantic Segmentation-Aware CNN Model.” arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1505.01749.

    Authors propose several modifications for Fast R-CNN network for object detection. With their methods, authors report SOTA on PASCAL VOC detection.
    
    Firstly, authors propose new RoI pooling method. Given a region proposal, they perform max pooling (that is finding maximum activation inside region for each filter in feature map) for several sub-regions and surrounding regions (fig. 3) and concatenate resulting vectors. IMO, this is not very different from SPP.
    
    Secondly, authors train semantic segmentation FCN backbone (from paper "Fully Convolutional Networks for Semantic Segmentation") in a weakly-supervised fashion, when ground truth annotations for semantic segmentation are obtained by just full filling of ground truth boxes. After such training model is able to predict meaningful object masks (fig. 5). Then segmentation head is removed and we freeze the model. Given a region proposal, we perform max pooling for this region from FCN feature map and concatenate resulting vector to the vector from multi-region CNN.
    
    Thirdly, authors propose to iteratively refine boxes on inference time. The idea is that since RoI head is able to refine box coordinates, we can further use refined boxes as new box proposals, and so on. After performing N such steps we take boxes and their scores from each step and merge them: B = B_1 U B_2 U … U B_n. Each box has it's score from RoI head. Then we perform NMS and obtain subset of boxes: B' = NMS(B). Finally, authors propose to perform box voting: for each b' in B', we search for all similar boxes in B (with IoU > 0.5) and calculate weighted average of these boxes with their scores as weights. This way we get final predictions.
    
    IMO, looking at fig. 6 with false positive dection, it looks very easy  to determine that the car extends beyond the left margin of the box and to reject the box based on this observation. The inability to do this indicates some problems in the objective or inference scheme. Probably, iterative box proposals updates will shift the box leftwards, so we should take the boxes from the last stage and not from all stages. The idea of iterative box regression was further developed in AttractioNet (from the same authors), Cascade R-CNN and DiffusionDet models.

Girshick, Ross. 2015. “Fast R-CNN.” arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1504.08083.

    Autor proposed Fast R-CNN architecture for object detection, that is a modification of SPP-Net (which is a modification of R-CNN). Fast R-CNN differs from SPP-Net by collapsing 3 training stages (training classifier head, training SVM and training bouding box regression) into one stage with multi-task loss. 
    
    Author introduces a term "RoI (region-of-interest) pooling" for extraction a fixed-length feature vector from the feature map. RoI pooling is a simplification of SPP: instead of using multiple pooling levels, it crops region of interest from the feature map, splits into 7x7 spatial cells, performs max pooling in each cell and concatenates results (see section 2.1). It can be viewed as "resize+flatten", bug when resizing is performed using max operation instead of average. Whole architecture is depicted on fig. 1.
    
    Importantly, RoI pooling has no interpolation (in contrast to RoIAlign and RoIWarp from subsequent works). RoI pooling first quantizes a floating-number RoI to the discrete granularity of the feature map, this quantized RoI is then subdivided into spatial bins which are themselves quantized, and finally feature values covered by each bin are aggregated (usually by max pooling).

Ren, Shaoqing, Kaiming He, Ross Girshick, Xiangyu Zhang, and Jian Sun. 2015. “Object Detection Networks on Convolutional Feature Maps.” arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1504.06066.

    This is a concurrent work with Fast R-CNN (authors also overlap). Paper is about designing object detection subnetworks ("Networks on Convolutional feature maps", NoCs), which are applied to fixed-size feature maps (fig. 1) produced by RoI pooling (see Fast R-CNN). As NoCs, authors try MLPs, ConvNets and ConvNets with maxout. Results suggest that a deeper region-wise NoC is useful and is in general orthogonal to deeper feature maps, and convolutional NoCs are more effective than MLPs.
    
    Authors show that naive version of Faster R-CNN with MLP has low accuracy, because its region-wise classifier is shallow and not convolutional. On the contrary, a deep and convolutional NoC is an essential factor for Faster R-CNN + ResNet to perform accurate object detection.

Long, Jonathan, Evan Shelhamer, and Trevor Darrell. 2014. “[FCN] Fully Convolutional Networks for Semantic Segmentation.” arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1411.4038.

    In this work pre-trained CNN is fune-tuned for pixelwise prediction for semantic segmentation. Firstly, authors view fully-connected layers  as 1x1 convolutions. Authors combine outputs from different layers by using trainable upsampling (initialized by bilinear upsampling) and addition (fig. 3). Thus authors try to combine high-level semantic imformation with low-level precise spatial information, achieving SOTA performance at that time on PASCAL VOC segmentation.
    
    Later, U-Net architecture will be based on FCN, while combining feature maps instead of predictions.

He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2014. “[SPP-Net] Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition.” arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1406.4729.

    Authors propose to adapt Spatial Pyramid Pooling (SPP) method from classic computer vision to CNN architectures, as an alternative to global pooling. This is intended to solve cropping vs warping problem in image classification (fig. 1) and to allow training and inference with variable-size images. SPP (fig. 3) is a concatenation of global pooling (avg or max), global 2x2 pooling and global 4x4 pooling. If we use average pooling, it actually can be viewed as "flatten" operation applied to feature map resized to spatial sizes 1x1, 2x2 and 4x4.
    
    This work is also important for object detection (fig. 5). Given region proposals in R-CNN, it's a natural idea to crop regions in feature map, instead of cropping image regions. At first, this will boost speed, secondly it will give more context information (for example, we want to use context information when detecting document parts or very small objects). A question is what operation we want to apply to feature map regions.
    
    IMO, if we apply "flatten" operation, we are limited to a certain region size. If, instead, we apply global pooling operation, we are facing the following problem. Let we have several identical objects placed in a row with bounding boxes B1, …, Bn. Let we perform average pooling on feature map for these boxes, obtaining embeddings e1, …, en. Let also we have a box proposal containing two objects: B1+B2. It's embedding will be (e1+e2)/2. So, e1 and e2 should be classified as object and (e1+e2)/2, (e1+e2+e3)/3 etc. should be rejected. We see that such classes are not linearly separable, and if e1=e2, this task is even unsolvable. SPP method seems to solve this problem.

Sermanet, Pierre, David Eigen, Xiang Zhang, Michael Mathieu, Rob Fergus, and Yann LeCun. 2013. “OverFeat: Integrated Recognition, Localization and Detection Using Convolutional Networks.” arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1312.6229.

    Authors propose a method OverFeat for object detection task. Given some image and square region, authors train CNN to perform classification and bounding box regression (which means regress 4 numbers corresponding to 4 sides of a square). In principle, we could apply this CNN to each possible square region, but this is resource-intensive. Instead (thanks to translation equivariance of CNNs) we can extract CNN feature map only 6 times for 6 different scales, obtaining 6 feature map of size 9x9x1024, and then apply classification and regression head to different regions on the feature maps, using sliding window of size 5x5 and flatten operation. On inference time, such model will return a lot of bounding boxes, and we apply NMS.
    
    OverFeat is inferior in quality to R-CNN proposed in the same year, so R-CNN became more widespread. IMO, flatten layer applied to each 5x5 region can be viewed as 5x5 convolution, so in principle this model is not very different from YOLO published 2 years later.

Girshick, Ross, Jeff Donahue, Trevor Darrell, and Jitendra Malik. 2013. “[R-CNN] Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation.” arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1311.2524.

    Authors propose two-stage R-CNN architecture for object detection task.
    
    Stage 1. The pipeline starts from extracting ~2 thousands of box proposals for an image. In this work, this stage is not trainable and is performed by selective search algorithm from classic computer vision. This algorithm is based on detecting areas with similar texture and iteratively merging them.
    
    Stage 2. For each box proposal, we crop corresponding image region (with padding) and resize it to 227x227. The resulting image is fed into CNN encoder and classifier, which classifies a region into N object classes + "background" class. We train a classifier to classify as background any crop with IoU < 0.5 with all known ground truth boxes. After training, authors replace classifier with N linear SVMs. one for each class. These SVMs are trained the same way as previous classifier, but use IoU threshold 0.3 instead of 0.5, authors argue that this boosts quality. After training, SVMs can be represented as fully connected layer on top of CNN encoder. Also, R-CNN uses bounding box regression to refine the boundaries of object's box and NMS for box filtering.
    
    Authors initialize backbone with ImageNet-pretrained network before training on object detection.
    
    Comparing to OverFeat, R-CNN have higher quality but is much slower, as it requires multiple CNN forward passes for each image. Later, Fast R-CNN and Faster R-CNN will achieve better performance.

## CV: few-shot

Sariyildiz, M. B., Kalantidis, Y., Larlus, D., & Alahari, K. (2020). Concept Generalization in Visual Representation Learning. arXiv, 2012.05649. Retrieved from https://arxiv.org/abs/2012.05649v2

    A benchmark ImageNet-CoG for concept generalization, a task which means learning a parameter-efficient (linear) classifier to classify novel classes. Benchmark defines a set of seen concepts along with sets of unseen concepts that are semantically more and more distant from the seen ones.  It is designed to evaluate models pretrained on IN-1K out-of-the-box and draws unseen concepts from the rest of the IN-21K dataset. Authors compare several models. To learn new concepts authors use from 1 to 128 samples, so it is a few-shot learning task.
    
    A question: if we have a known class A and similar new class B, then splitting A to A1, ..., An during pretraining will help? (or predictng not inly the presence of A but also its file-grained properties)

## CV: Image editing

Geng, Z., Yang, B., Hang, T., Li, C., Gu, S., Zhang, T., ...Guo, B. (2023). InstructDiffusion: A Generalist Modeling Interface for Vision Tasks. arXiv, 2309.03895. Retrieved from https://arxiv.org/abs/2309.03895

    The authors propse to modify the InstructPix2Pix architecture to enable it to solve not only image editing tasks, but also segmentation and keypoint detection. More specifically, the authors propose to unify computer vision tasks as image editing task following a natural language instruction. This contrasts to the previous approaches (Pix2Seq2, Unified IO) where computer vision tasks were unified as seq-to-seq tasks on tokens.
    
    For segmentation, the latent diffusion model performs denoising (starting form the latent noise) while conditioning on the image and on the natural language instruction like "apply a blue semi-transparent mask to the rightmost dog while maintaining the remainder unaltered". This allows to formulate segmentation as image editing task. Since the model returns RGB image, an additional postprocessing step (performed by a lightweight U-Net) is applied to extract mask. For keypoint detection, the conditioning instuction looks like "Please use red to encircle the left shoulder of the man”. The output image should exhibit a red circle at the corresponding location. This allows to formulate keypoint detection as image editing task.
    
    The training stages are as follows (fig. 2). Step 1 is a regular pretraining for a diffusion model. During step 2 (see sec. 3.3), the model adapts to the domain of images with colored masks or circles placed in random positions. This allows the model to adapt to such images, which substantially deviate from typical natural images. During this stage, the original image caption is augmented with a suffix, such as ”with a few different color patches here and there” or ”surrounded with a red circle”, and this stage is performed in a self-supervised fashion.
    
    Step 3 is a task-specific training, when segmentation and keypoint detection datasets (like MS COCO) are used to produce inputs, targets and instructions. The model is also trained on deblurring, denoising, and watermark removal task, formulated as editing instructions. To enhance the diversity of instructions, the authors first manually write 10 instructions for each task, then use GPT-4 to rewrite and expand the diversity of these instructions, thereby mimicking user input to the system. Authors also collect their own dataset for object removal and editing, based on SA-1B and OpenImages (see sec. 3.2) and use it for training. Finally, the autors use web crawling to collect 23000 image editing examples performed by humans using Photoshop. These data are also used for training and benchmarking. The final step 4 is human alignment, when the model is fine-tuned on its own cherry-picks selected by humans.
    
    Authors note that the trained model is able to generalize to keypoints not present in the training data (for example, animals' body parts and car logos). Also the model turns out to be able to solve classification and detection tasks, if we formulate them as segmentation and post-process outputs to profuce classes and bounding boxes. This, authors conclude that the model exhibits AGI capabilities to some extent, being able to generalize to tasks not seen during training. IMO, this is too strong of a statement, since the model actually performs the same task (segmentation) , just another post-processing is applied.
    
    IMO, a big shortcoming of this work is that recognition tasks (segmentation, keypoint detection) are performed as iterative diffusion process, that is pretty slow. While for image editing tasks this may be OK (since humans also take a lot of time to draw pictures), for image recognition tasks this seems a bad approach. U-Net inside diffusion models should probably extract high-level imformation from the image to denoise it, so we need to find a way to obtain this information in one forward pass (or, maximum, in 2-3 passes). For example, ODISE model is designed in this way.

Brooks, Tim, Aleksander Holynski, and Alexei A. Efros. 2022. “InstructPix2Pix: Learning to Follow Image Editing Instructions.” arXiv [cs.CV]. arXiv. http://arxiv.org/abs/2211.09800.

    The authors fine-tune Stabele Diffusion v1.5 (based on latent diffusion architecture) to perform instructional image editing, that is mapping from original image and editing instruction (like "Make his jacket out of leather") to the edited image.
    
    Authors modify latent diffusion to condition on image. To do this, they add more input channels to the first convolutional layer (initializing their output weights with zeros). This means that U-Net now accepts channel-wise concatenation of two VAE-encoded images. As usual, the model performs iterative denoising, starting from the latent noise, but now, instead of conditioning on the text description, we condition on both the original image and the editing instruction.
    
    To gather training data, authors start with 700 human-made text triplets (original description, editing instruction, result description). They fine-tune GPT-3 Davinci on this dataset and then apply fine-tuned model to various image descriptions from the LAION-Aestetics V2 6.5+, to collect. As a result, authors obtain hundreds of thousands of editing triplets. Then, authors generate images from both input description and edited description, using Stable Diffusion with Prompt2Prompt technique (fig. 3), using different values of Prompt2Prompt hyperparameter "p" (the fraction of denoising steps p with shared attention weights).
    
    Author drop input and edited descriptions and retain triplets (input image, editing insctruction, edited image). Further, these data aree filtered by aestetics score and aestetics score difference between image pairs, and also with CLIP-based directional similariy metric. In this way, the authors obtain a large (450K samples) model-generated training dataset.
    
    Authors train the above described model on this data. Authors introduce 2 parameters which control classifier-free guidance scales over image conditioning and instruction conditioning. They can be adjusted to trade off how strongly the generated samples correspond with the input image and how strongly they correspond with the edit instruction (fig 4).
    
    The paper contains a lot of editing examples, as well as failure cases (fig. 13, 14). For example, model is not capable of performing viewpoint changes, can make undesired excessive changes to the image, can sometimes fail to isolate the specified object, and has difficulty reorganizing or swapping objects with each other. It struggles with counting numbers of objects and with spatial reasoning (e.g., “move it to the left of the image”, “swap their positions”, or “put two cups on the table and one on the chair”), just as in Stable Diffusion and Prompt2Prompt.
    
    An important peculiarity of this work is that almost all data were collected using another models.

## CV: pretraining

Kornblith, S., Shlens, J., & Le, Q. V. (2018). Do Better ImageNet Models Transfer Better? arXiv, 1805.08974. Retrieved from https://arxiv.org/abs/1805.08974v3

    Downstream classification performance generally correlates with imagenet performance, however on two small fine-grained image classification datasets, pretraining on ImageNet provides minimal benefits. ImageNet architectures generalize well across datasets, but ImageNet features are less general than previously suggested.

## SSM, World models with CV, NLP

Wong, L., Grand, G., Lew, A. K., Goodman, N. D., Mansinghka, V. K., Andreas, J., & Tenenbaum, J. B. (2023). From Word Models to World Models: Translating from Natural Language to the Probabilistic Language of Thought. arXiv, 2306.12672. Retrieved from https://arxiv.org/abs/2306.12672v2

    A context-sensitive mapping from natural language into a probabilistic language of thought (PLoT)--a general-purpose symbolic substrate for generative world modeling.
    
    We model thinking with probabilistic programs, an expressive representation for commonsense reasoning; and we model meaning construction with large language models (LLMs), which support broad-coverage translation from natural language utterances to code expressions in a probabilistic programming language.
    
    We show that LLMs can generate context-sensitive translations that capture pragmatically-appropriate linguistic meanings, while Bayesian inference with the generated programs supports coherent and robust commonsense reasoning.

Lin, Jessy, Yuqing Du, Olivia Watkins, Danijar Hafner, Pieter Abbeel, Dan Klein, and Anca Dragan. 2023. “[Dynalang] Learning to Model the World with Language.” arXiv [cs.CL]. arXiv. http://arxiv.org/abs/2308.01399.

    The authors propose an RL agent named Dynalang. It is a modification of DreamerV3 with additional text inputs. This means that each observation o_t is a combination of one video frame and one text token (one token per each frame!). The world model is able to predict video, reward and text in the future (fig. 1). By design, Dynalang can be pretrained on text-only or video-only data without actions or task reward.
    
    "While previous settings specify that language such as instructions arrive at the beginning of an episode, we are interested in enabling agents to act in more flexible settings where they face a continuous stream of video and text, as in the real world."
    
    This is motivated in the following way. If the environment contains language signals, then language is not always directly connected to instructions, but may describe a world state or knowledge about how the world works (for example, "the top left button turns off the TV" or "I already vacuumed the living room"). So, authors include language into world modeling.
    
    Authors evaluate Dynalang on tasks with virtual environments, for example on vision-language navigation (fig. 7). Also authors evaluate on Messenger task, which tests whether agents can read text manuals describing game dynamics to achieve high scores. In this task, pretraining on TinyStories (a dataset of 2M short stories generated by GPT-3.5 and GPT-4) increase performance. Authors do not evaluate language modeling performance of Dynalang, but provide sampled 10-token generations conditioned on a prefix of 50 tokens for validation examples in TinyStories (Appendix E).

Hafner, Danijar, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. 2023. “[DreamerV3] Mastering Diverse Domains through World Models.” arXiv [cs.AI]. arXiv. http://arxiv.org/abs/2301.04104.

    The authors propose an algorithm DreamerV3 that is applicable to a wide range of diverse tasks (robot locomotion, manipulation, Minecraft etc.) out of the box - without hyperparemeters tuning. Authors evaluate DreamerV3 on 7 domains that include that continuous and discrete actions, visual and low-dimensional inputs, dense and sparse rewards, different reward scales, 2D and 3D worlds, and procedural generation.
    
    DreamerV3 achieves strong performance on all domains and demonstrate favorable scaling properties: increasing the model size monotonically improves both final performance and data-efficiency. DreamerV3 is the first algorithm to collect diamonds in Minecraft from scratch without human data or curricula, solving a long-standing challenge in artificial intelligence.
    
    DreamerV3 consists of 3 neural networks: the world model based on the recurrent state space model (RSSM), the critic and the actor (fig. 3). In RSSM a hidden state consists of deterministic and stochastic parts. Like in the DreamerV2, the stochastic state is a vector of multiple categorical variables (see fig. 2 in DreamerV2 paper). Given a hidden state, the actor estimates the distribution in action space to maximize the expected reward. Given a hidden state, the critic learns to predict the expected reward under the current actor behavior.
    
    IMO next. While i'm not a specialist in RL, it seems like the goal of the actor is to produce actions with a cheap network. Instead of using actor, we could use computational-heavy inference-time lookahead planning, as done in PlaNet. The goal of the critic is to help the actor learn: it predicts the estimated reward beyond the prediction horizon of 16 steps to provide a target for training the actor.
    
    Autors carefully tune the algorithm. Some but not all details:
    
    1. Observations are preprocessed by the symlog function (fig. 4), and the restoration MSE loss is applied to the preprocessed observation (eq. 1). The same goes for reward. This allows the optimization process to quickly move the network predictions to large values when needed.
    2. The KL-divergence loss in RSSM  is split into two terms by applying the stop-gradient to the first and to the second operand (eq. 5). These terms are weighted with different weights. So, L_dyn and L_rep is actually the same loss, but backpropagated through different submodules.
    3. The reward is predicted using twohot encoding to better capture distributions with multiple modes (eq. 9, 10). 
    4. Because the critic regresses targets that depend on its own predictions, authors stabilize learning by regularizing the critic towards predicting the outputs of an exponentially moving average of its own parameters.
    
    The example predictions of the DreamerV3 world model are shown at fig. 5. From 5 input frames, the model predicts 45 frames into the future given the action sequence.

Hafner, Danijar, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James Davidson. 2018. “[PlaNet, RSSM] Learning Latent Dynamics for Planning from Pixels.” arXiv [cs.LG]. arXiv. http://arxiv.org/abs/1811.04551.

    The authors propose RSSM (Recurrent State Space Model) and use it to build PlaNet (Deep Planning Network), a model-based agent that is able to learn from videos and actions.
    
    (In RL, a model-based agent is an agent that learns a model of the world. If, after learning, the agent can make predictions about what the next state and reward will be before it takes each action, it's a model-based RL algorithm. If it can't, then it’s a model-free algorithm.)
    
    Authors argue that a model-based approach offer several benefits over model-free RL. Firstly, it can be more data-efficient, because world modeling usually require predicting in pixel space, and loss in pixel spae provides rich training signal. Secondly, learned world model can be independent of any specific task, and thus have more potential to transfer to other tasks in the same environment. (read even more reasons in the paper, sec. 1)
    
    To to enable fast planning, authors propose to learn the world dynamics in a compact latent space, instead of directly predicting pixels in the future moments. The authors start with formulating a model with discrete time, hidden states (latents) s_t, image observations o_t, actions a_t and scalar rewards r_t, that follow the stochastic dynamics with a fixed initial state s_0:
    
    Transition function: s_t ~ p(s_t | s_{t-1}, a_{t-1})
    Observation function: o_t ~ p(o_t | s_t)
    Reward function: r_t ~ p(r_t | s_t)
    Action policy: a_t ~ p(a_t | o_{≤t}, a_{≤t})
    
    Transition function, observation function and reward function are learnable, parametrized by neural networks (deconvolutional network for observation function). Action policy is not learned. Instead, given given a world and reward models, optimal actions can be obtained by inference-time optimization: searching for action sequence that maximizes summary reward.
    
    To train the above model by MLE (maximum likelihood estimation), we should search for the combination of unknown values (hidden states and weights of neural nets) that maximize the log-probability of the dataset, that is sum of log-probabilities for all samples. To achieve this, we could learn hidden states for all samples by gradient descent. But if we go this way, how do we obtain hidden states on inference time from observations? Given an observation and transition function, obtaining posterior probabilities for hidden states is intractable.
    
    The same problem arose in autoencoders and were solved by training encoder that approximate posterior for hidden states. The authors go the same way and train convolutional encoder q(s_t | s_{t-1}, a_{t-1}, o_t) to infer approximate posteriors for hidden states.
    
    So, in total, we get 4 trainable functions: transition function, observation function, reward function and encoder. Interestingly, the observation function is not used for planning, so may be dropped after training.
    
    To train all the functions, authors use variational lower bound of the data log-probability (eq. 3). This loss consists of two terms, when expectations can be estimated by a single reparemetrized sample. The first term is a reconstruction error: if we sequentially apply encoder, sampling and observation function, how well do we recover the original input? The second term is a KL-divergence between the hidden state distributions obtained from encoder and from transition function: they should match to minimize this term.
    
    Importantly, the transition function is stochastic: it returns distribution parameters that we sample from to obtain the next state. Such models are called state space models (SMM) and are different from RNN, where the transition function is deterministic. If in SSM we reduce the predicted variance to zero, we end up with RNN. The authors argue that the stochastity is very important, since it allows the model to capture different possible futures. But SSM have a problem to pass some information through many hidden states, since multiple sampling operations may distort information. So, authors propose to combine SSM and RNN giving RSSM, where the hidden state in each moment consists of the deterministic and stochastic parts: firstly we predict the deterministic part from the previous time moment, and then predict the stochastic part (fig. 2).
    
    The authors point out another shortcoming of the RSSM. If observations at every time moments are accessible, this makes learning the transition function less important. In eq. 3 the loss even does not propagate through multiple time steps. To cure this, authors propose latent overshooting: a technique when we predict the future hidden states through N time steps (from t to t+N) without accessing the intermediate observations, and then minimize KL-divergence between the predicted state distribution and the distribution obtained from the encoder at time t+N (fig. 3c, loss at eq. 6, 7). An alternative to this is the earlier proposed observation overshooting: predicting pixels through N time steps without accessing the intermediate observations (fig. 3b) - but this is too expensive, since we should make a pass through observation model too many times.
    
    Based on RSSM, authors propose PlaNet algorithm (Algorithm 1). Starting from some episodes, authors fit RSSM on them, and then do actions using fitted model to collect new training data. This approach require acting and running the true world model (game algorithm), that is possible in virtual environments.
    
    In the experimental parts, authors train the model to solve multiple image-based continuous control tasks of the DeepMind control suite. In all tasks, the only observations are third-person camera images of size 64x64x3 pixels. The authors conclude that both deterministic and stochastic parts are important. The stochastic component is even more important – the agent does not learn without it.
    
    IMO thoughts next.
    
    It's interesting to think about the taxonomy of "world model". This is the model of how some environment of interest works. So it requires a transition function between world states S, when the states may contain fixed and stochastic (this means not fully known) parts. To train the model we use sequences of observations O (videos, for example). There may be 4 cases.
    
    1. We use observations as world states: S = O. In this case we come to autoregressive video modeling.
    2. We train only a function O -> S. If we train such model end-to-end, the model may collapse to the solution when world states are noninformative, for example are constantly zero, and are trivially predicted. In general, the prediction loss will encourage O -> S function to extract features that are most easily predicted.
    3. We train only a function S -> O. In this case, how do we obtain S given O? Exact algorithm may be intractable, and approximate algorithm may require inference-time optimization, that is costly.
    4. We train both functions O -> S and S -> O. In this case we come to the autoencoder with time axis, as done in SSM and in this work.
    
    There is one possible reason why splitting the internal state into determinstic and stochastic parts is beneficial. Each observation may not show us the full world, while observation history may allow the model to capture the full world. In this case, h_t contains information about the full world, and s_t ~ p(s_t | h_t) contains only currently visible information. So, calculating KL-divergence between s_t and encoded observation seems more suitable. RSSM also looks similar to the world model from Ha and Schmidhuber, 2018 that also contains deterministic h_t and stochastic z_t.
    
    While transition in SSM requires sampling, this is quite similar to dropout. Dropout can be also viewed as sampling from distribution. So, it turns out that RNN with inference-time dropout is a special case of SSM?
    
    When pixels are observations, the problem in RSSM is the pixel-wise loss. Some aspects of the environment may be irrelevant and better should not be stored in the hidden state, while other aspects (for example, the shape of the key) may be only slightly visible in pixel space, so they do not contribute much to the pixel-wise loss.
    
    The second problem here is a general RNN problem: how do we compress the world state, that may be very complex, into a vector of the fixed size? A common sense tells that the more is the size of uncompressed information, the more is the size of compressed information. A hidden state is the compressed information about the agent's experience. From this side, transformer architecture seems more suitable, since in transformer the agent's memory grows with the growth of the context. The optimal way seems to compress information nearly logarithmically, when compressed experience size should be proportional to the logarithm of uncompressed experience size. However, this is not the case both in transformers and in RNN/SSM.
    
    The notion of internal (hidden) state seems also ambiguous in some tasks. Should an internal state contain information about precise pixel boundaries? If yes, it should be very large. If no, how will the agent shoot enemies, if the task require precise shooting?

Ha, David, and Jürgen Schmidhuber. 2018. “World Models.” arXiv [cs.LG]. arXiv. http://arxiv.org/abs/1803.10122.

    Authors propose a model-based reinforcement learning agent that successfully solves car racing and VisDoom tasks with visual input in the OpenAI Gyn environment (fig. 9, 14).
    
    If the network that learns action policy is large, with high representational capacity, then it will probably suffer from the credit assignment problem. Hence in practice, smaller networks are used as they iterate faster to a good policy during training. However, to unleash the power of deep learning and to model complex environments we want a large neural network. To solve this dilemma, authors train a large task-independent world model and a small, task-dependent controller network on top of it. A small controller lets the training algorithm focus on the credit assignment problem on a small search space, while not sacrificing capacity and expressiveness via the larger world model.
    
    Authors use a probabilistic generative world model that includes several components. Firstly, a variational autoencoder (VAE) compresses each video frame to a small latent vector z_t. Authors train VAE on a given set of frames and then freeze it. Secondly, authors use RNN with hidden state h_t: each time step in accepts latent vector z_{t-1}, action a_{t-1} and previous hidden state h_{t-1} and returns new hidden state h_t and a probability distribution p(z_t) in form of gaussian mixture. Such RNN that outputs gaussian misture are called Mixture Density Networks (MDN-RNN) and were previously used in several works. From Graves, 2013: "A subset of the outputs are used to define the mixture weights, while the remaining outputs are used to parameterise the individual mixture components. The mixture weight outputs are normalised with a softmax function to ensure they form a valid discrete distribution, and the other outputs are passed through suitable functions to keep their values within meaningful range (for example the exponential function is typically applied to outputs used as scale parameters, which must be positive)."
    
    On top of the world model authors train a controller that accepts z_t and h_t and linearly projects them to the action a_t (eq. 1). A little number of parameters of controller allows to explore more unconventional ways to train it – for example, even using evolution strategies.
    
    The overall training procedure (sec. 3.2) is the following:
    
    1. Collect 10K rollouts from a random policy (that is, what happened if we act randomly). As a result, we obtain 10K frame sequences.
    2. Train VAE on all the frames.
    3. Train MDN-RNN to model P(z_t) (probably using KL-loss).
    4. Train controller with CMA-ES method (evolution strategy) to maximize the expected cumulative reward.
    
    Interestingly, a controller may be trained using only the learned world model, that is, without the game engine. This may be especially useful for the agent acting in the read world. Authors add extra uncertainty into such virtual environment, thus making the training more challenging. They do this by increasing the temperature parameter during the sampling process of z_t. Agents that perform well in higher temperature settings generally perform better in the normal setting. Increasing temperature helps prevent controller from taking advantage of the imperfections of the learned world model.
    
    VAE is trained independently of RNN world model, and so it may encode parts of the observations that are not relevant to a task. By training together with a world model that predicts rewards, the VAE may learn to focus on task-relevant areas of the image, but the tradeoff here is that we may not be able to reuse the VAE effectively for new tasks without retraining.
    
    Using a gaussian mixture in RNN allows to capture multiple different futures with random discrete events, such as whether a monster decides to shoot a fireball or stay put. "For instance, if we set the temperature parameter to a very low value of 0.1, ... the monsters inside this dream environment fail to shoot fireballs, no matter what the agent does, due to mode collapse. The M model is not able to jump to another mode in the mixture of Gaussian model where fireballs are formed and shot. Whatever policy learned inside of this dream will achieve a perfect score of 2100 most of the time, but will obviously fail when unleashed into the harsh reality of the actual world, underperforming even a random policy."

## NLP: interpreting, robustness, uncertainty, shortcuts

Kalai, A. T., & Vempala, S. S. (2023). Calibrated Language Models Must Hallucinate. arXiv, 2311.14648. Retrieved from https://arxiv.org/abs/2311.14648v1

    The authors aim to prove that statistically calibrated generative model of some text corpus should hallucinate rare facts. Howeverr, authors' formalization is rather limited, since they consider only plain generation of facts without any previous context or prompt. Also, authors consider their own (computationally intractable) notion of calibration, different from standard token-based ones.
    
    In this simplified setting, the authors prove that any calibrated model, regardless of its architecture, will hallucinate on arbitrary facts. This means that in unconditional generation unexisting facts may appear, and the probability of generating a hallucination is close to the fraction of facts that occur exactly once in the training data.
    
    "Arbitrary facts" means that the truthfulness of the facts cannot be valiated by rules (like 572 < 120523 can be valiated by math rules). Examples of these facts are "5W facts" (Who-What-When-Where-Why), for example "Alexa Wilkins had a tuna sandwich at Salumeria for lunch last Tuesday".
    
    The authors note that they only study one statistical source of hallucination, while there are many other types of hallucination and reasons LMs may hallucinate beyond pure statistics.

PR/ROC заметки

    1) TPR = recall = sensitivity = true positive / all really positive
        доля отобранных позитивных примеров среди всех позитивных примеров
        монотонно растет от 0 до 1 с ослаблением порога отбора
        не завсисит от баланса классов
    2) 1 - FPR = reversed recall = specifity = true negative / all really negative
        доля отобранных как негативные примеров среди всех негативных примеров
        монотонно растет от 0 до 1 с ослаблением порога отбора
        не завсисит от баланса классов
    3) Precision = true positive / all predicted as positive
        доля отобранных позитивных примеров среди всех отобранных примеров
        завсисит от баланса классов
    4) Accuracy = truly predicted / all
        завсисит от баланса классов
        
    ROC (TPR, FPR) измеряет качество ранжирования в том плане, насколько сильно отличаются распределения для really positive и really negative.

    PR (recision, recall) измеряет насколько часто мы будем ошибаться. Даже если распределения отличаются сильно, но really negative класса в 100 раз больше, то мы все равно будем часто ошибаться.

**Face verification**

Wang, X., Peng, J., Zhang, S., Chen, B., Wang, Y., & Guo, Y. (2022). A Survey of Face Recognition. arXiv, 2212.13038. Retrieved from https://arxiv.org/abs/2212.13038v1

    - A survey paper on face recognition (FR)
    - In general, FR consists of 1) face images preprocessing, 2) model inference to get face embedding, 3) recognition by matching features between test image and images with known labels. Also, face anti-spoofing is usually added before inference to protect against attacks (print, video replay, 3D mask, etc).
    - Mtcnn is one of the most famous face detection methods that predict both face and landmark location in a coarse-to-fine manner. Also, other networks are discussed.
    - For face anti-spoofing, STDN proposed an adversarial learning network to extract the patterns differentiating a spoof and live face. PatchNet rephrased face anti-spoofing as a fine-grained material recognition problem. SSAN extracts content and style features to get a stylized feature space, which is used to distinguish a spoof and live face.
    - Merely using RGB signals which belong to visible spectrum has a lot of limitations in face anti-spoofing, therefore an increasing numbers of methods tried to adopt multi-channel signals to achieve higher accuracy.
    - Common way to align face images is using a 2D transformation to calibrate facial landmarks to predefined frontal templates or mean face mode. Deepface employs a 3D alignment method in order to align faces undergoing out-of-plane rotations. As the mainstream FR training sets are becoming larger gradually, some FR methods choose to omit face alignment step and train (or test) with face patches directly from face detection.
    - After preprocessing, face images with their ground truth IDs can be used to train a FR deep model. Different backbones with similar parameter amount have similar accuracy on FR. As a result, you can use Resnet, ResNext, SEResnet, Inception net, Densenet, etc. to form your FR system. Training FR model is a process to learn a compact euclidean feature space, where distance directly correspond to a measure of face similarity (metric learning). The last part of face inference pipeline can be divided into face verification and face identification.
    - Different loss functions are discussed: cross entropy, contrastive loss, triplet loss, center loss. Also, face recognition is naturally treated as a classification problem, and thus using softmax as the loss to train a FR model is an intuitive consideration: larger margin loss is derived from softmax. Other modifications are proposed, for example, NormFace normalized both face embeddings and classification weights. Further, ArcFace incorporated margin between classes (eq. 16). However, ArcFace includes sensitive hyperparameters which can make training process unstable. P2SGrad was proposed to address this challenge by directly designing the gradients for training in an adaptive manner. CurricularFace adopted the idea of curriculum learning and weighted the class score of negative samples. In the earlier stage of training CurricularFace, the loss fits easy samples. And at the later training stage, the loss fits hard samples. Many more alternatives are discussed.
    - Large-scale face datasets usually exhibit a massive number of classes with unbalanced distribution. Features with non-dominate IDs are compressed into a small area in the hypersphere, leading to training problems. It was shown that including all tail data (faces with small number of images) in training can not help to obtained a better FR model by contrastive loss, triplet loss, and center loss. Several modifications are proposed. The second data unbalance phenomenon is shallow data. (IMO not clear what this means).
    - ... (not finished)

Wang, Z., Wang, Z., Yu, Z., Deng, W., Li, J., Gao, T., & Wang, Z. (2022). Domain Generalization via Shuffled Style Assembly for Face Anti-Spoofing. arXiv, 2203.05340. Retrieved from https://arxiv.org/abs/2203.05340v4

    - The previous face anti-spoofing (FAS) methods have achieved promising performance in intra-domain scenarios, but may encounter dramatic degradation under the cross-domain settings. Domain adaptation techniques are used to alleviate the discrepancy between source and target domains, but usually we can't collect sufficient unlabeled target data for training. Thus, we need domain generalization (DG) to generalize well on the unseen target domain.
	- Style information in FAS tasks can be divided into two parts: domain-specific and liveness-related (is spoofing or not) style information.
	- We propose shuffled style assembly network (SSAN) for FAS (fig. 2). The feature generator is a shallow embedding network that captures multi-scale low-level information. We adopt adversarial learning to make generated content features indistinguishable for different domains.
	- IMO: very complex architecture, not fully understood, requires detailed reading
	
**Extractive summarization and input sequence coverage**

Vinyals, O., Fortunato, M., & Jaitly, N. (2015). Pointer Networks. Advances in Neural Information Processing Systems, 28. Retrieved from https://proceedings.neurips.cc/paper_files/paper/2015/hash/29921001f2f04bd3baee84a12e98098f-Abstract.html

    - Seq2seq RNNs still require the size of the output dictionary to be fixed a priori. Because of this constraint we cannot directly apply this framework to combinatorial problems where the size of the output dictionary depends on the length of the input sequence. (IMO not clear what does this mean)
	- We propose Pointer Networks. We repurpose the attention mechanism to create pointers to input elements (fig. 1b).
	- We apply the Pointer Net model to three distinct non-trivial algorithmic problems involving geometry.

Tu, Z., Lu, Z., Liu, Y., Liu, X., & Li, H. (2016). Modeling Coverage for Neural Machine Translation. arXiv, 1601.04811. Retrieved from https://arxiv.org/abs/1601.04811v6

    - NMT has a serious problem, namely lack of coverage. Some words are unnecessarily translated for multiple times, and some words are mistakenly untranslated. This happen because of ignoring the "coverage" of source words.
	- We propose NMT-coverage: we append a coverage vector to the intermediate representations of an NMT model, which are sequentially updated after each decoding step to keep track of the attention history (fig. 2). It significantly improves the overall alignment between the source and target sentences .

See, A., Liu, P. J., & Manning, C. D. (2017). Get To The Point: Summarization with Pointer-Generator Networks. arXiv, 1704.04368. Retrieved from https://arxiv.org/abs/1704.04368v2

    - We propose an architecture for summarization task that augments the standard seq2seq attentional model in two orthogonal ways.
	- 1) We use a hybrid pointer-generator network that can copy words from the source text via pointing (see "Pointer networks"), which aids accurate reproduction of information, while retaining the ability to produce novel words (fig. 3). For each decoder timestep a generation probability p_gen ∈ [0, 1] is calculated, which weights the probability of generating words from the vocabulary, versus copying words from the source text. This can be viewed as a balance between extractive and abstractive summarization approaches.
	- 2) We propose a novel variant of the coverage vector from NMT (see "Modeling coverage for neural machine translation"), which we use to track and control coverage of the source document for eliminating repetition.
	
**Phonetic level in TTS and ASR**

Bengio, Y., De Mori, R., Flammia, G., & Kompe, R. (1992). Global optimization of a neural network-hidden Markov model hybrid. IEEE Trans. Neural Networks, 3(2), 252–259. doi: 10.1109/72.125866

    - We propose NN-HMM hybrid model for ASR. The outputs of NN constitute the observation sequence for the continuous density HMM (Hidden Markov Model). The parameters of both NN and HMM are optimized, and NN is optimized by backprop through HMM.
	
Mohamed, A.-R., Dahl, G., & Hinton, G. (2010). Deep Belief Networks for phone recognition. Science, 4. Retrieved from https://www.researchgate.net/publication/228871422_Deep_Belief_Networks_for_phone_recognition

    - A SOTA ASR system typically uses Hidden Markov Models (HMMs) to model the sequential structure of speech signals, with local spectral variability modeled using mixtures of Gaussian densities. The first assumption is that the hidden state sequence can be well-approximated using a first order Markov chain where each state S_t at time t depends only on S_{t−1}. Second, observations at different time steps are assumed to be conditionally independent given a state sequence. Although these assumptions are not realistic, they enable tractable decoding and learning even with large amounts of speech data.
	- We apply Deep Belief Networks (DBNs) to model the spectral variabilities in speech. DBNs are probabilistic generative models that are composed of multiple layers of stochastic latent variables with Restricted Boltzmann Machines (RBMs, a particular type of Markov Random Field) as their building blocks. DBNs have a greedy layer-wise unsupervised learning algorithm as well as a discriminative finetuning procedure for optimizing performance on classification tasks.
	- In order to apply DBNs with fixed input and output dimensionality to phone recognition, a context window of n successive frames of feature vectors is used to set the states of the visible units of the lower layer of the DBN.
	- Phone recognition experiments were performed on the TIMIT corpus.

Hinton, G., Deng, L., Yu, D., Dahl, G. E., Mohamed, A.-r., Jaitly, N., ...Kingsbury, B. (2012). Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups. IEEE Signal Process. Mag., 29(6), 82–97. doi: 10.1109/MSP.2012.2205597

	- This paper provides an overview of ASR progress and represents the shared views of four research groups.
    - Most current speech recognition systems use hidden Markov models (HMMs) to deal with the temporal variability of speech and Gaussian mixture models (GMMs) to determine how well each state of each HMM fits a frame or a short window of frames of coefficients that represents the acoustic input. With enough components, GMMs can model probability distributions to any required level of accuracy and they are fairly easy to fit to data using the EM algorithm.
	- The acoustic input is typically represented by concatenating Mel Frequency Cepstral Coefficients (MFCCs) or Perceptual Linear Predictive coefficients (PLPs).
	- The recognition accuracy of a GMM-HMM system can be further improved if it is discriminatively fine-tuned after it has been generatively trained to maximize its probability of generating the observed data.
	- GMMs have a serious shortcoming – they are statistically inefficient for modeling data that lie on or near a non-linear manifold in the data space. For example, modeling the set of points that lie very close to the surface of a sphere only requires a few parameters using an appropriate model class, but it requires a very large number of diagonal Gaussians or a fairly large number of full-covariance Gaussians. We believe that other types of model may work better than GMMs for acoustic modeling if they can more effectively exploit information embedded in a large window of frames.
	- DNNs have the potential to learn much better models of data that lie on or near a non-linear manifold. An alternative way is to use a feedforward NN that takes several frames of coefficients as input and produces posterior probabilities over HMM states as output.
	- Two decades ago, researchers achieved some success using NNs with a single layer of non-linear hidden units to predict HMM states from windows of acoustic coefficients. At that time, however, neither the hardware nor the learning algorithms were adequate for training NNs with many hidden layers on large amounts of data and the performance benefits of using NNs with a single hidden layer were not sufficiently large to seriously challenge GMMs. As a result, the main practical contribution of NNs at that time was to provide extra features in tandem or bottleneck systems.
	- The paper starts by describing the two-stage training procedure that is used for fitting the DBN (fig. 1). In the first stage (generative pre-training), layers of feature detectors are initialized, one layer at a time, by fitting a stack of generative models, each of which has one layer of latent variables. These generative models are trained without using any information about the HMM states that the acoustic model will need to discriminate. In the second stage, each generative model in the stack is used to initialize one layer of hidden units in a DNN and the whole network is then discriminatively fine-tuned to predict the target HMM states. These targets are obtained by using a baseline GMM-HMM system to produce a forced alignment.
	- We review exploratory experiments on the TIMIT database. The DNNs worked well on all of these tasks when compared with highly-tuned GMM-HMM systems and on some of the tasks they outperformed the state-of-the-art by a large margin.

Chorowski, J., Bahdanau, D., Cho, K., & Bengio, Y. (2014). End-to-end Continuous Speech Recognition using Attention-based Recurrent NN: First Results. arXiv, 1412.1602. Retrieved from https://arxiv.org/abs/1412.1602v1

    - The authors propose seq2seq RNN with attention for ASR. The RNN decoder directly emits a stream of phonemes. This model is closely related to the RNN Transducer, however, with an attention mechanism.
	- The accuracy on TIMIT phoneme recognition is comparable to the SOTA DNN-HMM systems and is slightly worse than the best reported error rates obtained using RNNs.
	- However, the model is easy to implement, tune and apply. It requires a narrow beam search, and its accuracy deteriorates very slightly when greedy search.
	- Fig. 2 shows alignments produced by the model: (a) when the alignment was successfully encouraged to be monotonic, and (b) when the model is free to select any frame in the input sequence. In (b), we observe how the absence of the learned preference for monotonicity makes the model confused by the repeated occurrence of the phonemes “cl k”, and to a lesser degree, by the repetition of “w”. (IMO, interestingly, Whisper is also subject to this).
	- IMO, a good Background section with a review of previous approaches. However, there is nothing about the justification of predicting phonemes instead of graphemes.

Miao, Y., Gowayyed, M., & Metze, F. (2015). EESEN: End-to-End Speech Recognition using Deep RNN Models and WFST-based Decoding. arXiv, 1507.08240. Retrieved from https://arxiv.org/abs/1507.08240v3

    - ASR has traditionally leveraged the HMM/GMM paradigm for acoustic modeling. HMMs act to normalize the temporal variability, whereas GMMs compute the emission probabilities of HMM states. In recent years, the performance of ASR has been improved dramatically by the introduction of deep neural networks (DNNs) as acoustic models. In the hybrid HMM/DNN approach, DNNs are used to classify speech frames into clustered context-dependent (CD) states (i.e., senones). However, at first, acoustic modeling typically requires various resources such as dictionaries and phonetic questions. Second, in the hybrid approach, training of DNNs still relies on GMM models to obtain initial frame-level labels, while building GMM models normally goes through multiple stages. Third, the development of ASR systems highly relies on ASR experts to determine the optimal configurations of a multitude of hyper-parameters, for instance, the number of senones and Gaussians in the GMM models.
	- We need to reduce the complexity of ASR. We focus on end-to-end ASR, i.e., modeling the mapping between speech and labels (words, phonemes, etc.) directly without any intermediate components (e.g., GMMs). Research on end-to-end ASR faces two major obstacles. First, it is challenging to incorporate lexicons and language models into decoding. When decoding CTC-trained models, past work has successfully constrained search paths with lexicons. However, how to integrate word-level language models efficiently still is an unanswered question. Second, the community lacks a shared experimental platform for the purpose of benchmarking.
    - We present our Eesen framework which drastically simplifies the existing pipeline to build SOTA ASR systems. In Eesen, RNN predicts phonemes or characters with CTC objective.
	- A distinctive feature of Eesen is a generalized decoding method based on weighted finite-state transducers (WFSTs). In this method, individual components (CTC labels, lexicons and language models) are encoded into WFSTs, and then composed into a comprehensive search graph.
	- Eesen results in superior performance than the existing end-to-end ASR pipelines, being on a par with strong hybrid HMM/DNN baselines.

Yao, K., & Zweig, G. (2015). Sequence-to-Sequence Neural Net Models for Grapheme-to-Phoneme Conversion. arXiv, 1506.00196. Retrieved from https://arxiv.org/abs/1506.00196v3

    - We use bidirectional encoder-decoder LSTM (fig. 3) to improve SOTA on Grapheme-to-Phoneme Conversion.
	- We experiment on the CMUDict, NetTalk, and Pronlex datasets.

Novak, J. R., Minematsu, N., & Hirose, K. (2015). Phonetisaurus: Exploring grapheme-to-phoneme conversion with joint n-gram models in the WFST framework. Nat. Lang. Eng., -1(06), 1–32. doi: 10.1017/S1351324915000315

    - Grapheme-to-Phoneme (G2P) conversion is an important problem in both the areas of ASR and TTS. In the case of ASR, the true vocabulary is often dynamic in nature. This means that new words, or new pronunciation candidates for existing words may need to be added to the system on a regular basis. Analogous problems arise in the case of TTS (IMO not a clear explanation why do we need G2P).
	- We introduce Phonetisaurus, an open-source G2P conversion toolkit. We syntesize the most effective components of previously proposed solutions in the literature, with a clear focus on achieving a balance between speed, accuracy and flexibility.
	
Zhang, Y., Chan, W., & Jaitly, N. (2016). Very Deep Convolutional Networks for End-to-End Speech Recognition. arXiv, 1610.03022. Retrieved from https://arxiv.org/abs/1610.03022v1

    - The seq2seq model with attention sidesteps the complicated machinery developed for classical ASR, because it is not restricted by the classical independence assumptions of HMM and CTC models.
	- While very deep CNNs have been successfully applied to ASR, recently there have been several advancements in the CV community on very deep CNNs hat have not been explored in the speech community.
	- In our deep CNN speech model based on Listen, Attend and Spell (LAS) we use 1x1 convolutions (Network-in-Network), BN, ResNets, and Convolutional LSTM that use convolutions to replace the inner products within the LSTM unit. The model learns to transcribe an audio sequence to a word sequence, one character at a time.
	- We experiment with the WSJ ASR task and achieve 10.5% WER without any dictionary or language.
	
Chorowski, J., & Jaitly, N. (2016). Towards better decoding and language model integration in sequence to sequence models. arXiv, 1612.02695. Retrieved from https://arxiv.org/abs/1612.02695v1

	- Seq2seq ASR  networks can typically be decomposed into an encoding module that transforms its inputs into a hidden representation, a decoding (spelling) module which emits target sequences and an attention module that computes a soft alignment between the hidden representation and the targets. This discriminative training mode is fundamentally different from the generative "noisy channel" formulation used to build classical SOTA ASR systems. Discriminative training allows seq2seq models to focus on the most informative features. However, it also increases the risk of overfitting to those few distinguishing characteristics.
	- We analyse an attention-based seq2seq ASR system that directly transcribes recordings into characters.
	- We observe two shortcomings: overconfidence in predictions (that reduces the diversity of transcripts obtained using beam search) and a tendency to produce incomplete transcriptions when external LMs are used (model skips some words).
	- Model overconfidence is promoted by the the cross-entropy training criterion. This leads to very peaked probability distributions, effectively preventing the model from indicating sensible alternatives to a given character, such as its homophones. Model overconfidence can have two consequences. First, next-step character predictions may have low accuracy due to overfitting. Second, overconfidence may impact the ability of beam search to find good solutions and to recover from errors.
	- We first investigate the impact of confidence on beam search by varying the temperature of the SoftMax function. As temperature increases beam search finds better solutions, however care must be taken to prevent truncated transcripts: we constrained the search to emit the EOS token only when its probability was within a narrow range from the most probable token.
	- A solution to model overconfidence is label smoothing. Originally label smoothing is uniform. Better results can be obtained with unigram smoothing which distributes the remaining probability mass proportionally to the marginal probability of classes (see "Regularizing neural networks by penalizing confident output distributions"). We propose a neighborhood smoothing scheme that uses the temporal structure of the transcripts: the remaining probability mass is assigned to tokens neighboring in the transcript. (IMO not clear).
	- When training with neighborhood smoothing, greedy decoding leads to nearly 3 percentage smaller error rate. Second, the entropy of network predictions is higher, allowing beam search to discover good solutions without the need for temperature control. Moreover, since model is trained and evaluated with the same temperature 1 we didn’t have to control the emission of EOS token.
	- When a language model is used wide beam searches often yield incomplete transcripts. With narrow beams, the problem is less visible due to implicit hypothesis pruning.
	- We compare three strategies designed to prevent incomplete transcripts. The first strategy doesn’t change the beam search criterion, but forbids emitting the EOS token unless its probability is within a set range of that of the most probable token. Hoever, this is inefficient against omissions in the middle of the transcript. Alternatively, beam search criterion can be extended to promote long transcripts, but in this case beam search is looping over parts of the recording and additional constraints are needed. To prevent looping we propose to use a coverage term that counts the number of frames that have received a cumulative attention greater than some value. This prevents looping because once the cumulative attention bypasses the threshold τ a frame is counted as selected and subsequent selections of this frame do not reduce the decoding cost.
	- We observe that at large beam widths constraining EOS emissions is not sufficient. In contrast, both promoting coverage and transcript length yield improvements with increasing beams. However, simply maximizing transcript length yields more word insertion errors and achieves an overall worse WER.
	
Deri, A., & Knight, K. (2016). Grapheme-to-Phoneme Models for (Almost) Any Language. ResearchGate, 399–408. doi: 10.18653/v1/P16-1038

    - Grapheme-to-phoneme (G2P) models are typically language-specific. They are trained on a pronunciation dictionary consisting of word-pronunciation pairs. Building such a dictionary for a new language is both time-consuming and expensive, because it requires expertise in both the language and a notation system like the International Phonetic Alphabet.
    - Using data scraped from Wiktionary, we clean and normalize pronunciation dictionaries for 531 languages.
	- We develop a language-independent distance metric between IPA (International Phonetic Alphabet) phonemes.
	- We create two sets of g2p models for "high resource" languages and adapt them to low-resource languages through output mapping and training data mapping.

Toshniwal, S., Tang, H., Lu, L., & Livescu, K. (2017). Multitask Learning with Low-Level Auxiliary Tasks for Encoder-Decoder Based Speech Recognition. arXiv, 1704.01631. Retrieved from https://arxiv.org/abs/1704.01631v2

	- Traditional ASR systems include components like frame classifiers, phonetic acoustic models, lexicons (which may or may not be learned from data), and LMs. Recently, completely integrated end-to-end training approaches, where all parameters are learned jointly using a loss at the final output level, have become viable and popular. Typical end-to-end models are based on RNN encoder-decoders or CTC-based models. However, end-to-end training is less interpretable and ignores potentially useful domain-specific information about intermediate representations, as well as existing intermediate levels of supervision.
	- We use a multitask learning approach that combines the final task loss (log loss on the output labels) with losses corresponding to lower-level tasks applied on lower layers. We demonstrate this approach on an attention-based encoder-decoder LSTM character-level ASR model (fig. 1).
	- We use phoneme-level supervision obtained from the word-level transcriptions and pronunciation dictionary. Also, we apply sub-phonetic type of supervision at the frame level, as shown in Figure 1, using state alignments obtained from a standard HMM-based system.
	- Results on Switchboard and CallHome show consistent improvements over baseline attention-based models. We obtain the best performance with a combination of 2 tasks: a phoneme decoder and frame-level state loss. Analysis of model training and performance suggests that the addition of auxiliary tasks can help in either optimization or generalization.
	
Peters, B., Dehdari, J., & van Genabith, J. (2017). Massively Multilingual Neural Grapheme-to-Phoneme Conversion. arXiv, 1708.01464. Retrieved from https://arxiv.org/abs/1708.01464v1

    - Accurate grapheme-to-phoneme conversion (g2p) is important for any application that depends on the sometimes inconsistent relationship between spoken and written language. Most prominently,this includes text-to-speech and automatic speech recognition (IMO not a clear explanation why do we need G2P for ASR).
	- We present a neural seq-to-seq approach to g2p which is trained on spelling–pronunciation pairs in hundreds of languages. The system shares a single encoder and decoder across all languages, allowing it to utilize the intrinsic similarities between different writing systems.
	- For our model, the source sequences are words in the standard orthography in any language, and the target sequences are the corresponding representation in the International Phonetic Alphabet (IPA).
	
Milde, B., Schmidt, C., & Köhler, J. (2017). Multitask Sequence-to-Sequence Models for Grapheme-to-Phoneme Conversion. doi: 10.21437/Interspeech.2017-1436

    - A crucial component of most ASR systems is the phoneme lexicon, mapping words to their phonetic representation (e.g. Thursday → TH ER Z D EY). Training and using a G2P model is often directly integrated into the ASR training procedure, as phonetic out-of-vacabulary (OOV) words in the training set hamper the alignment of training data to its transcriptions.
	- we investigate how multitask learning can improve the performance of Seq2Seq G2P models. A single Seq2Seq model is trained on multiple phoneme lexicon datasets containing multiple languages and phonetic alphabets.
	- Multi-language learning does not show improved error rates.
	- Combining standard datasets and crawled data with different phonetic alphabets of the same language shows promising error reductions on English and German Seq2Seq G2P conversion.
	- Combining Seq2seq G2P models with standard n-grams based models yields significant improvements.
	
Sainath, T. N., Prabhavalkar, R., Kumar, S., Lee, S., Kannan, A., Rybach, D., ...Chiu, C.-C. (2017). No Need for a Lexicon? Evaluating the Value of the Pronunciation Lexica in End-to-End Models. arXiv, 1712.01864. Retrieved from https://arxiv.org/abs/1712.01864v1

	- Traditional automatic speech recognition (ASR) systems are comprised of an acoustic model (AM), a language model (LM) and a pronunciation model (PM), all of which are independently trained on different datasets. AMs take acoustic features and predict a set of sub-word units, typically context-dependent or context-independent phonemes. Next, a hand-designed lexicon (i.e., PM) maps a sequence of phonemes produced by the acoustic model to words. Finally, the LM assigns probabilities to word sequences.
	- How do end-to-end models perform if we incorporate a separate PM and LM into the system? This question can be answered by training an end-to-end model to predict phonemes instead of graphemes. The output of the end-to-end model must then be combined with a separate PM and LM to decode the best hypotheses from the model.
	- The present work is the first to explore end-to-end systems trained with phonemes for a large vocabulary continuous speech recognition (LVCSR) task, where models are directly decoded in the first-pass.
	- When predicting phonemes, we train our model to predict a set of 44 CI phonemes, as well as an extra <eow> token, specifying the end of a word, analogous to the <space> token in graphemes. Because it is hard to predict <eow>, we found it is better to make it optional.
	- Because of the homophone issue with phonemes (e.g., phoneme ey can map to the words ‘I’ or ‘eye’), using a language model, G, is critically important. There are two ways we can incorporate it during decoding. We can either process final phoneme sequences, ir incorporate LM ducing each step of the beam search (eq. 3) with additional "coverage" term to promote longer transcripts. As noted in "Towards better decoding and language model integration in sequence to sequence models", the latter way can become quite challenging if the ASR model becomes over-confident, in which case, the weight from the LM component will be ignored. In experiments we found that the first way (processing final phoneme sequences with LM) is slighly better.
	- Our experiments show that the performance of grapheme systems is slightly better than phoneme systems. On a multi-dialect English task we once again confirm the superiority of graphemes (see examples in tables 3, 4, 5).
	
Mortensen, D. R., Dalmia, S., & Littell, P. (2018). Epitran: Precision G2P for Many Languages. ACL Anthology. Retrieved from https://aclanthology.org/L18-1429

	- Epitran is a massively multilingual, multiple back-end system for G2P (grapheme-to-phoneme) transduction that takes word tokens in the orthography of a language and outputs a phonemic representation in either IPA or X-SAMPA. Out of the box, it supports 61 languages.
	
Zhou, S., Dong, L., Xu, S., & Xu, B. (2018). A Comparison of Modeling Units in Sequence-to-Sequence Speech Recognition with the Transformer on Mandarin Chinese. arXiv, 1805.06239. Retrieved from https://arxiv.org/abs/1805.06239v2

	- Conventional ASR systems consist of three independent components: an acoustic model (AM), a pronunciation model (PM) and a language model (LM), all of which are trained independently. CD-states and CD-phonemes are dominant as their modeling units in such system. However, it has been challenged by seq2seq attention-based models, which integrate an acoustic, pronunciation and language model into a single NN.
	- On English ASR tasks, previous attempts have already shown that the modeling unit of graphemes can outperform that of phonemes by seq2seq attention-based model (see "No Need for a Lexicon? Evaluating the Value of the Pronunciation Lexica in End-to-End Models"), so a hand-designed lexicon might be removed from ASR systems (as we known, it is very laborious and time-consuming to generate a pronunciation lexicon). Furthermore, the latest work use the word piece models, which are sub-word units ranging from graphemes all the way up to entire words.
	- We investigate five modeling units for Speech-Transformer model on Mandarin Chinese ASR tasks, including CI-phonemes, syllables (pinyins with tones), words, sub-words and characters.
	- We confirm that the lexicon free modeling units, i.e. words, sub-words and characters, can outperform lexicon related modeling units, i.e. CI-phonemes and syllables. Character based model achieves the best result and establishes a new SOTA CER on HKUST dataset.
	
Xu, Q., Baevski, A., & Auli, M. (2021). Simple and Effective Zero-shot Cross-lingual Phoneme Recognition. arXiv, 2109.11680. Retrieved from https://arxiv.org/abs/2109.11680v1

    - We are fine-tuning wav2vec 2.0 to transcribe languages unseen during fine-tuning. We start from self-supervised representations trained on data in many languages (wav2vec 2.0). Next we simultaneously fine-tune the model to perform phoneme recognition on data in multiple training languages, building a global phoneme recognizer by simply considering all possible phonemes of the training languages. At inference time, we test the fine-tuned model on unseen languages using a mapping of the phonemes from the training vocabulary to the ones in the target languages. We decode with a LM to generate the final phoneme sequence.
    - Our approach performs on par to the recently introduced unsupervised speech recognition work ("Unsupervised speech recognition") which does not use labeled data from related languages and requires training separate models for each target language.

Yolchuyeva, S., Németh, G., & Gyires-Tóth, B. (2020). Transformer based Grapheme-to-Phoneme Conversion. arXiv, 2004.06338. Retrieved from https://arxiv.org/abs/2004.06338v2

	- We are first to apply encoder-decoder transformer for Grapheme-to-Phoneme Conversion (G2P). For evaluation, the CMU pronunciation and NetTalk datasets were used (see examples in table 4)
	
Zeineldeen, M., Zeyer, A., Zhou, W., Ng, T., Schlüter, R., & Ney, H. (2020). A systematic comparison of grapheme-based vs. phoneme-based label units for encoder-decoder-attention models. arXiv, 2005.09336. Retrieved from https://arxiv.org/abs/2005.09336v3

    - The disadvantage of grapheme-based ASR might be limited scalability to low-resource situations. Phoneme-based modeling on the other hand easily enables adaptation to new pronunciations or inclusion of words with uncommon pronunciations.
	- We compare grapheme-based and phoneme-based output labels with an encoder-decoder-attention model. We found their performance to be similar.
	- We also compared single units vs. subword (BPE) units vs. whole words, and found that subword units are best, both for phonemes and graphemes.
	- The search should be restricted to valid sequences from the lexicon, this is crucial for good performance.
	- IMO. Looks like the authors map the phonemes to words to make the comparison possible, however а quick glance did not allow me to understand how exactly they do it.
	
Fang, A., Filice, S., Limsopatham, N., & Rokhlenko, O. (2020). Using Phoneme Representations to Build Predictive Models Robust to ASR Errors. ACM Conferences. Association for Computing Machinery. doi: 10.1145/3397271.3401050

    - In Amazon Alexa, Apple Siri or Google Home, the Spoken Language Understanding (SLU) is usually performed in two steps: first an Automatic Speech Recognition (ASR) is used to transcribe human speech; then Natural Language Understanding (NLU) models are applied on ASR transcriptions to interpret users’ requests. Applying NLU it on ASR transcriptions poses new challenges, as ASR systems often generate transcriptions with errors that can cause failures in downstream applications of virtual assistants, such as intention classification or slot filling.
	- ASR errors are just an outcome of a phonetic confusion, causing a phrase in a human speech to be incorrectly transcribed to a "quasi-oronym", i.e., phrases with different meanings that sound very similar. Therefore, classic approaches that operate on word or even character-level representations cannot recover from such errors. Similarly sounding words may give very dissimilar word embeddings. We argue that representing text as a sequence of phoneme embeddings can help when dealing with ASR errors.
	- We propose to represent ASR transcriptions as sequences of phonemes. We map phonemes to phoneme embeddings and propose several methods to train phoneme embeddings that are able to capture pronunciation similarities. Finally, we use these pre-trained embeddings as inputs to Neural Network architectures for solving NLU tasks.
	- To learn phoneme embeddings we design phoneme2vec. We propose 4 variants listed below (p2vc, p2vm, and p2va, s2s). These training procedures for learning phoneme embeddings require a corpus of corresponding REF and ASR utterances. We propose an automatic data generation pipeline (fig. 5). Using this pipeline we created noisy versions of four NLU datasets that we plan to make available to the community.
	- 1) p2vc: phoneme2vec on surrounding phonemes. Given a phoneme we want to predict its surrounding phonemes with the traditional word2vec procedure. We decided not to limit the context of a phoneme to its word as the ASR might have failed the word segmentation. However, we explicitly consider the padding symbol (fig. 1) as it represents the "absence of sound". We aim to capture pronunciation similarities. Intuitively, two phonemes are similar if the ASR often confuses them. Following this intuition we further propose two variants of phoneme2vec.
	- 2) p2vm: phoneme2vec on mixed REF and ASR utterances. We mix REF and ASR utterances at phoneme level in an alternating way (fig. 2).
	- 3) p2va: phoneme2vec on aligned REF and ASR utterances. This is a more general solution that involves an explicit alignment. We directly pair phonemes in a REF utterance with their aligned phonemes in the ASR utterance using the Needleman-Wunsch alignment algorithm (fig. 3).
	- 4) s2s. Another very intuitive way of training phoneme embeddings is to use a seq2seq model. After reading the entire REF utterance, the last hidden state of the LSTM is passed to decoder (fig. 4). The targets are ASR utterance. So, we train the seq2seq model to predict the next correct phoneme of the ASR utterance given the REF utterance and the previous ASR phonemes. In this sequence-to-sequence architecture, we add phoneme embedding layers before the encoder and decoder; so, utterances are transformed into sequences of phonemes that are given as inputs to the encoder and decoder. Finally, the embedding layer of the decoder is used as pre-trained phoneme embeddings.
	- Varying hyperparameters, overall, we create 10 different pre-trained phoneme embeddings. We set the dimension size of the embedding vector to 20 for all the methods. Analysis suggests that p2vc is not suited for learning pronunciation aspects (fig. 7), while the other proposed models more effectively capture these desired properties (fig. 6).
	- We show that models exploiting our phoneme representation can significantly improve classification performance on datasets containing ASR errors compared to models operating only on standard character or word representations.
	- IMO, if we convert ground truth transcription to phonemes, then we obtain "ideal" transcription, where all phonemes are well pronounced. This may not match real spoken words. In this case, it is not clear if phonetic NLU inputs are better than letters. Indeed, phoneme input may represent phonetic uncertainty, but text input also can: if we input "A", we mean "all words that sound similar as A". Maybe, text represented as phonetic embeddings is richer than text of letters, since it contains uncertainty, but this is just because this it is not a result of argmax operation. So, this paper needs ablations: if ASR stage outputs letters or BPE tokens (not phonemes), and we pass output token embeddings into NLU stage and train this stage, will it be worse?
	
Sundararaman, M. N., Kumar, A., & Vepa, J. (2021). Phoneme-BERT: Joint Language Modelling of Phoneme Sequence and ASR Transcript. arXiv, 2102.00804. Retrieved from https://arxiv.org/abs/2102.00804v2

    - In spoken language understanding (SLU), usually the pipeline consists of ASR and NLU stages. However, ASR errors degrade the performance of the  NLU stage. The approaches tried by scientific community to address the errors: 1) Modelling word confidence in the ASR stage, 2) ASR correction by LM, 3) end-to-end NLU models, 4) Phoneme enhanced representations.
	- We propose PhonemeBERT that jointly models the phoneme and ASR sequence (fig. 1). We generate phoneme sequence from a separate phonemic listen-attend-spell (LAS) model. The phonemic LAS model is a sequence-to-sequence model with phoneme as its output unit.
	- IMO, need full reading to understand anything in this paper.
	
Zhu, J., Zhang, C., & Jurgens, D. (2022). ByT5 model for massively multilingual grapheme-to-phoneme conversion. arXiv, 2204.03067. Retrieved from https://arxiv.org/abs/2204.03067v2

    - G2P (grapheme-to-phoneme conversion) is a fundamental to the pipeline for a variety of speech processing tasks that depend on phonemic inputs, including speech synthesis and speech recognition (IMO, why so?).
	- To create a training dataset, we aggregated pronunciation dictionaries previously published or made available in around 100 languages.
    - We have curated a G2P dataset from various sources that covers around 100 languages and trained large-scale multilingual G2P models based on ByT5. It significantly outperformed the token-based mT5 model.
	- Multilingual models can perform zero-shot G2P on unseen low-resource languages with seen writing systems (IMO not clear how to)
	- See examples in https://github.com/lingjzhu/CharsiuG2P

Chiu, C.-C., Sainath, T. N., Wu, Y., Prabhavalkar, R., Nguyen, P., Chen, Z., ...Bacchiani, M. (2017). State-of-the-art Speech Recognition With Sequence-to-Sequence Models. arXiv, 1712.01769. Retrieved from https://arxiv.org/abs/1712.01769v6

    - To date, none of end-to-end ASR models (LAS, RNN-T, Neural Transducer, Monotonic Alignments, RNA) has been able to outperform a SOTA conventional systems on a large vocabulary continuous ASR.
	- We explore a variety of improvements to the LAS model, since previous work showed that LAS offered improvements over other seq2seq models.
	- Word piece models can be used instead of graphemes, giving a modest improvement.
	- Multi-head attention architecture, which allows the model to learn to attend to multiple locations of the encoded features, offers improvements over the single-head attention.
	- Minimum WER (MWER) optimization significantly improves performance. The loss function can then be approximated using the set of N-best hypotheses computed using beam-search decoding.
	- We include scheduled sampling (SS), which feeds the previous label prediction during training rather than ground truth.
	- Label smoothing helps to make the model less confident in its predictions.
	- Synchronous SGD offer improvements over asynchronous SGD.
	- Overall, we get 13% relative improvement in WER with structure improvements (word piece, multi-head attention) and 27.5% relative improvement with optimization strategies (MWER, label smoothing, SS, synchronous SGD).
	- Finally, we incorporate a LM to rescore N-best lists in the second pass (eq. 3), which results in a further 3.4% relative improvement in WER (IMO very modest improvement). The external LM is a large 5-gram LM trained on text data from a variety of domains. Domain-specific LMs are first trained, then combined together using Bayesian-interpolation. 
	- We also present results with a unidirectional LSTM encoder for streaming recognition.
	- The training utterances are anonymized and hand-transcribed, and are representative of Google’s voice search traffic. This data set is created by artificially corrupting clean utterances using a room simulator, adding varying degrees of noise and reverberation such that the overall SNR is between 0dB and 30dB, with an average SNR of 12dB. The noise sources are from YouTube and daily life noisy environmental recordings.
	
Bengio, S., Vinyals, O., Jaitly, N., & Shazeer, N. (2015). Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks. arXiv, 1506.03099. Retrieved from https://arxiv.org/abs/1506.03099v3

    - In seq2seq models, during inference, true previous target tokens are unavailable, and are thus replaced by tokens generated by the model itself, yielding a discrepancy between how the model is used at training and inference. If a wrong decision is taken at time t−1, the model can be in a part of the state space that is very different from those visited from the training distribution and for which it doesn’t know what to do. Worse, it can easily lead to cumulative bad decisions - a classic problem in sequential Gibbs sampling type approaches to sampling, where future samples can have no influence on the past.
	- So, mistakes made early in the sequence generation process are fed as input to the model and can be quickly amplified.
	- We propose to flip a coin and use the true previous token with probability E, or an estimate coming from the model itself with probability (1-E). The estimate of the model can be obtained by sampling or argmax.
	- Intuitively, at the beginning of training, sampling from the model would yield a random token since the model is not well trained, which could lead to very slow convergence.
	- We propose Scheduled Sampling: a curriculum learning approach to gradually force the model to deal with its own mistakes. We thus propose to use a schedule to decrease E during training.
	- Future work includes back-propagating the errors through the sampling decisions, as well as exploring better sampling strategies including conditioning on some confidence measure from the model itself.
	
Jaitly, N., Le, Q. V., Vinyals, O., Sutskever, I., Sussillo, D., & Bengio, S. (2016). An Online Sequence-to-Sequence Model Using Partial Conditioning. Advances in Neural Information Processing Systems, 29. Retrieved from https://papers.nips.cc/paper_files/paper/2016/hash/312351bff07989769097660a56395065-Abstract.html

    - Seq2seq models are unsuitable for tasks where it is important to produce outputs as the input sequence arrives (online prediction).
    - We present a Neural Transducer. Unlike sequence-to-sequence models, it computes the next-step distribution conditioned on the partially observed input sequence and the partially generated sequence.
	- The inputs to the transducer RNN come from two sources: the encoder RNN and its own recurrent state. At each time step, the transducer can decide to emit zero to many output symbols (fig. 1, 2).
	- During training, alignments of output symbols to the input sequence are unavailable. We show how a dynamic programming algorithm can be used to compute "approximate" best alignments (sec. 3.5).
	- We achieve SOTA for unidirectional models on the TIMIT phoneme recognition task, using a Neural Transducer with 3 layered unidirectional LSTM encoder and 3 layered unidirectional LSTM transducer.
	- We find that the Neural Transducer performs well for long sequences even when attention mechanisms are not used.
	- IMO, such a method does not allow to disambiguate previous transcriptions when new words arrive, so this will give strictly lower quality than conditioning on the full inputs.
	
Raffel, C., Luong, M.-T., Liu, P. J., Weiss, R. J., & Eck, D. (2017). Online and Linear-Time Attention by Enforcing Monotonic Alignments. arXiv, 1704.00784. Retrieved from https://arxiv.org/abs/1704.00784v2

	- Seq2seq RNN requires the model to effectively compress all important information about the input sequence into a single vector. In practice, this often results in the model having difficulty generalizing to longer sequences than those seen during training. An effective solution to these shortcomings are attention mechanisms. Similar mechanisms have been used as soft addressing schemes in memory-augmented NN architectures. A common criticism of soft attention is that the model must perform a pass over the entire input sequence when producing each element of the output sequence. This gives quadratic time complexity, and also soft attention cannot be used in online prediction, when the input is only partially observed.
    - In seq2seq tasks, the alignment between input and output sequence elements is roughly monotonic in many problems of interest (sentence summarization, NMT, ASR). Also, in neural networks, in many cases the attention is assigned mostly to a single entry.
	- In standard attention, eqs. (2) and (3) are computing the expected output of a simple stochastic process, when a memory index is sampled from a categorical distribution.
	- We then formulate a stochastic process which explicitly processes the memory in a left-to-right manner. Our novel process can be computed in an online manner; i.e. we do not need to wait to observe the entire input sequence before we start producing the output sequence. (IMO this means that the encoder should be autoregressive?)
	- We then propose training with respect to the expected value of the described online sampling process. Out training algorithm still has a quadratic complexity, but it allows linear-time attention process at test time.
	- We need our mechanism to encouraging discreteness to exhibit similar behavior when training in expectation and when using the hard monotonic attention process at test time. A straightforward way to encourage this behavior is to add noise before the sigmoid. This approach is similar to the recently proposed Gumbel-Softmax trick, except we did not find it necessary to anneal the temperature.
	- IMO, at test time the information flow is bottlenecked since the decoder can only attend to one encoder state (fig. 3).
	
Sak, H., Shannon, M., Rao, K., & Beaufays, F. (2017). Recurrent Neural Aligner: An Encoder-Decoder Neural Network Model for Sequence to Sequence Mapping. doi: 10.21437/Interspeech.2017-1705

    - We propose an encoder-decoder RNN called Recurrent Neural Aligner (RNA). 
	- Like CTC models, it defines a probability distribution over target label sequences including blank labels corresponding to each time step in input. The probability of a label sequence is calculated by marginalizing over all possible blank label positions. Unlike CTC, RNA does not make a conditional independence assumption for label predictions.
	- RNA is capable of streaming recognition since the decoder does not employ attention mechanism.
	- RNA grapheme model with greedy search and no external language model can match the accuracy of a CTC word model which uses a word n-gram language model for rescoring when we have large amount of acoustic training data with transcripts available.
	
Dong, L., Xu, S., & Xu, B. . Speech-Transformer: A No-Recurrence Sequence-to-Sequence Model for Speech Recognition. 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE. doi: 10.1109/ICASSP.2018.8462506

    - We apply encoder-decoder transformer to the ASR task. Additionally, we propose a 2D-Attention mechanism which can jointly attend to the time and frequency axes of the 2-dimensional speech spectrograms (fig. 3).
	- On the WSJ dataset, our model has small training costs and achieves competitive WER performance.
	
Weng, C., Cui, J., Wang, G., Wang, J., Yu, C., Su, D., & Yu, D. (2018). Improving Attention Based Sequence-to-Sequence Models for End-to-End English Conversational Speech Recognition. doi: 10.21437/Interspeech.2018-1030

    - Current end-to-end ASR systems still cannot achieve a comparable performance to a conventional ASR system on the Switchboard dataset, a widely used English conversational speech benchmark.
	- We propose to use the input-feeding architecture which feeds not only the previous context vector but also the previous decoder hidden state information as inputs (fig. 1, baseline is similar to LAS).
	- We propose a better hypothesis generation scheme for sequential minimum Bayes risk (MBR) training. We use a simple beam-search algorithm to generate the hypothesis set and rescore with eq. 14. We’ve also tried the attention coverage penalty for re-scoring but it never worked in our experiment. Seq2seq model tends to make over-confident predictions. For beam-search, over-confident predictions will lead to too many alike hypothesized sequences among N-best which might prevent the MBR training procedure from seeing a more diverse hypothesis space. To this end, we introduce softmax smoothing during N-best generation, using softmax temperature (eq. 15). (IMO there are similar works, like "Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models").
	
Sabour, S., Chan, W., & Norouzi, M. (2018). Optimal Completion Distillation for Sequence Learning. arXiv, 1810.01398. Retrieved from https://arxiv.org/abs/1810.01398v2

    - Maximum Likelihood Estimation (MLE) is still considered the dominant approach for training seq2seq models. Alternative approaches typically do not offer a substantial performance improvement over a well tuned MLE baseline, especially when label smoothing and scheduled sampling are used.
    - We present Optimal Completion Distillation (OCD), a training procedure for optimizing sequence to sequence models based on edit distance.
	- In OCD, we always train on prefixes generated by sampling from the model that is being optimized. For each generated prefix, we identify all of the optimal suffixes that result in a minimum total edit distance v.s. the ground truth target using an efficient dynamic programming algorithm. We then maximize the average log probability of the first token of each optimal suffix (table 1).
	- Generally, OCD excels at training from scratch, which makes it an ideal substitution for MLE. Hence, OCD is orthogonal to methods which require MLE pretraining or joint optimization.
	- Assuming that the generated prefix sequence is perfectly matched with the ground truth sequence, then the OCD targets would simply be the following tokens of the ground truth sequence. Hence, OCD becomes equivalent to MLE. However, during training, the generated prefixes sampled from the model do not match the ground truth sequence, even at the end of training. This suggests that OCD and MLE are training on very different input prefix trajectories.
	- On both WSJ and Librispeech, our proposed OCD algorithm significantly outperforms our own strong baselines including MLE (Maximum Likelihood Estimation with label smoothing) and SS (scheduled sampling with a well-tuned schedule). Even at the same training CER, we observe better validation error for OCD, which suggests that OCD improves generalization of MLE, possibly because OCD alleviates the mismatch between training and inference.
	- IMO, this may be implemented in BPE decoders either on token level, or on character level. This may be similar to training on random tokenizations, instead of the deterministic BPE.
	
Wiseman, S., & Rush, A. M. (2016). Sequence-to-Sequence Learning as Beam-Search Optimization. arXiv, 1606.02960. Retrieved from https://arxiv.org/abs/1606.02960v2

    - There are major, previously known issues with seq2seq models:
	- 1) Exposure Bias: the model is never exposed to its own errors during training (this was also addressed in "Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks").
	- 2) Loss-Evaluation Mismatch: training uses a word-level loss, while at test-time we target improving sequence-level evaluation metrics, such as BLEU.
	- 3) Label bias: word probabilities at each time-step are locally normalized, guaranteeing that successors of incorrect histories receive the same mass as do the successors of the true history (see "Globally Normalized Transition-Based Neural Networks").
	- We define a loss function in terms of errors made during beam search. We provide an efficient algorithm to backpropagate through the beam-search procedure during seq2seq training.
	- We learn to produce (non-probabilistic) scores for ranking sequences. We do not use softmax, thereby allowing the model to avoid issues associated with the label bias problem. Ideally we would train by comparing the gold sequence to the highest-scoring complete sequence. However, finding the argmax sequence according to this model is intractable. We propose to adopt a LaSO-like scheme to train, which we will refer to as beam search optimization. We define a loss that penalizes the gold sequence falling off the beam during training (fig. 1).
	- We run experiments on three very different problems: word ordering, syntactic parsing, and machine translation. The version with beam search optimization shows significant improvements on all three tasks, compared to a highly tuned seq2seq system with attention.
	
Ravanelli, M., Parcollet, T., & Bengio, Y. (2018). The PyTorch-Kaldi Speech Recognition Toolkit. arXiv, 1811.07453. Retrieved from https://arxiv.org/abs/1811.07453v2

	- Kaldi currently represents the most popular ASR toolkit. It relies on finite-state transducers (FSTs) and provides a set of C++ libraries.
    - Our PyTorch-Kaldi project aims to bridge the gap between Kaldi and PyTorch. It implements acoustic models in PyTorch, while feature extraction, label/alignment computation, and decoding are performed with Kaldi, making it suitable to develop SOTA DNN-HMM speech recognizers (fig. 1).
	
Sun, H., Tan, X., Gan, J.-W., Liu, H., Zhao, S., Qin, T., & Liu, T.-Y. (2019). Token-Level Ensemble Distillation for Grapheme-to-Phoneme Conversion. arXiv, 1904.03446. Retrieved from https://arxiv.org/abs/1904.03446v3

    - We propose the token-level ensemble distillation for grapheme-to-phoneme (G2P) conversion task. Specifically, we train a teacher model to generate the phoneme sequence as well as its probability distribution given unlabeled grapheme sequence, and regard the unlabeled grapheme sequence and the generated phoneme sequence as pseudo labeled data, and add them into the original training  data.
	- We train a variety of models (CNN, RNN and Transformer) for ensemble to get higher accuracy, and transfer the knowledge of the ensemble models to a light-weight model.
	
Pundak, G., Sainath, T. N., Prabhavalkar, R., Kannan, A., & Zhao, D. (2018). Deep context: end-to-end contextual speech recognition. arXiv, 1808.02480. Retrieved from https://arxiv.org/abs/1808.02480v1

    - Speech recognition performance can be improved by incorporating information about the speaker’s context, such as the dialog state, the speaker’s location, personalized information about the user.
	- We propose Contextual-LAS (CLAS) for ASR task, which can leverage a list of contextual phrases to improve recognition performance.
	- Our technique consists of first embedding each phrase, represented as a sequence of graphemes, into a fixed-dimensional representation, and then employing an attention mechanism to summarize the available context at each step of the model’s output predictions (fig. 1).
	- Our method does not require careful tuning of rescoring weights, while still being able to incorporate out-of-vocabulary (OOV) terms.
	
Chen, Z., Jain, M., Wang, Y., Seltzer, M., & Fuegen, C. (2019). Joint Grapheme and Phoneme Embeddings for Contextual End-to-End ASR. Interspeech. Retrieved from https://www.semanticscholar.org/paper/Joint-Grapheme-and-Phoneme-Embeddings-for-ASR-Chen-Jain/4af08d465168c9b5fffe8cf1de6ee649ca4a8ac9

    - We improve over the CLAS approach for contextual ASR.
	- In CLAS, because the embeddings are learned from graphemic information only, they do not discriminate well among similar sequences of graphemes, nor do they generalize well to unseen pronunciations of words. (IMO not clear why pronunciations matter here, since context should be used on the language modeling internal stage, not on the acoustic stage).
	- To overcome these problems, 1) we extract embeddings based on a grapheme-to-phoneme (G2P) encoder-decoder, and 2) we try to leverage the power of the bidirectional LSTM instead of LSTM proposed in CLAS.
	
Vesik, K., Abdul-Mageed, M., & Silfverberg, M. (2020). One Model to Pronounce Them All: Multilingual Grapheme-to-Phoneme Conversion With a Transformer Ensemble. arXiv, 2006.13343. Retrieved from https://arxiv.org/abs/2006.13343v1

    - We are participating in the SIGMORPHON 2020 Shared Task, when sequences of graphemes are mapped to corresponding phonemes. A clear challenge is the limited size of the shared task training data for each of the 15 individual languages.
	- We build a single transformer model mapping from input to output across all the languages simultaneously. We also use pseudo-labeling on Wikipedia data, selecting sequences of phonemes predicted with our models above a certain confidence threshold.
	
Gorman, K., Ashby, L. F. E., Goyzueta, A., McCarthy, A. D., Wu, S., & You, D. (2020). The SIGMORPHON 2020 Shared Task on Multilingual Grapheme-to-Phoneme Conversion. ACL Anthology, 40–50. doi: 10.18653/v1/2020.sigmorphon-1.2

	- ASR requires mappings between written words and their pronunciations, either explicit or implicit (in end-to-end models). For open-vocabulary applications, these mappings must generalize to unseen words, and so must be expressed as mappings between sequences of graphemes and phonemes or phones. We note that the term phoneme is a well-defined object in linguistic theory, which may not be appropriate for a given pronunciation dictionary. Therefore, in what follows we use the term phone to refer to transcriptions symbols.
	- Rule-based systems require linguistic expertise to develop and maintain, and may be brittle or inaccurate. Therefore, modern speech engines usually treat grapheme-to-phoneme conversion as a machine learning problem (IMO the authors still do not explain why we need this in end-to-end systems).
	- The vast majority of published research focuses on English or a few other highly-resourced, globally hegemonic languages for which free pronunciation dictionaries are available.
	- We present SIGMORPHON 2020 shared task: a multilingual grapheme-to-phoneme conversion task with data sets, evaluation metrics, and strong baselines. The task included data from 15 languages and scripts. 9 teams submitted 23 G2P systems and achieved substantial improvements over the provided baselines.
	
Omachi, M., Fujita, Y., Watanabe, S., & Wiesner, M. (2021). End-to-end ASR to jointly predict transcriptions and linguistic annotations. ACL Anthology, 1861–1871. doi: 10.18653/v1/2021.naacl-main.149

    - Our primary goal is to provide aligned transcripts (phonemes) and linguistic annotations (graphemes) with minimal degradation in ASR performance. Given these outputs, we can easily combine ASR with downstream NLP tasks and also conduct an intuitive error analysis (e.g., detecting the error caused due to the homonym by checking the word and the corresponding phoneme output).
    - We consider several existing options to make the ASR system predict both phonemes and graphemes (fig. 1). The options A and B do not align both output sequences. The third option, in contrast, does not require postprocessing to align the label sequences.
	- We choose the option C and adopt a model to output phonemic transcripts and POS tags, as well as graphemes (fig. 2f).
	- Our approach predicts linguistic annotations correctly even though corresponding graphemes are wrong. This feature is helpful for the downstream NLP system like slot filling or intent detection.
	
Karita, S., Soplin, N. E. Y., Watanabe, S., Delcroix, M., Ogawa, A., & Nakatani, T. (2019). Improving Transformer-Based End-to-End Speech Recognition with Connectionist Temporal Classification and Language Model Integration. doi: 10.21437/Interspeech.2019-1938

    - We address two problems with Transformer end-to-end ASR (see "Speech-transformer: a no-recurrence sequence-to-sequence model for speech recognition"):
	- 1) Slower convergence, namely its slower increase in validation accuracy over wall clock time, than RNN-based ASR. Transformer takes less time per iteration, but it takes many more epochs to converge (fig. 1).
	- 2) The difficulty of LM integration in joint beam search decoding. The scores provided by Transformer and LM had drastically different behaviours that make them difficult to combine.
	- We implement CTC joint training as a multi-task learning by adding a new branch from Transformer encoder. It can make the convergence of the seq2seq model faster because CTC learns to align the speech feature and transcription explicitly.
	- We found that the joint decoding with CTC could perform better with LM integration than the one without CTC in our experiments. We followed the common joint decoding approach, which simply takes the sum of log probabilities from the CTC, decoder and LM (eq. 15).
	
Raissi, T., Zhou, W., Berger, S., Schlüter, R., & Ney, H. (2022). HMM vs. CTC for Automatic Speech Recognition: Comparison Based on Full-Sum Training from Scratch. arXiv, 2210.09951. Retrieved from https://arxiv.org/abs/2210.09951v1

	- We pursue the goal of a simple from-scratch trainable ASR system producing both good WER and alignment.
    - In CTC, the peaky behavior caused by the limited label emissions can affect the quality of the alignment, which can shift with respect to the evidence in the input.
	- We compare the CTC and HMM topologies for from-scratch full-sum training. In addition to the standard hybrid HMM, we propose a discriminative HMM modeling as a more direct contrast to CTC models.
	- We then address the convergence issue from point of view of difficulty of alignment modeling. To this end, we propose minimum label duration and prior knowledge-based probability approximation. These approaches benefit both HMM and CTC models.
	
Nguyen, L. T., Pham, T., & Nguyen, D. Q. (2023). XPhoneBERT: A Pre-trained Multilingual Model for Phoneme Representations for Text-to-Speech. arXiv, 2305.19709. Retrieved from https://arxiv.org/abs/2305.19709v1

	- In current TTS systems, the pre-trained BERT is used to provide additional contextual information and helps increase the quality of the output synthesized speech (for example, see "Pre-trained Text Embeddings for Enhanced Text-to-Speech Synthesis"). Therefore, it might be better if the contextualized phoneme representations are directly produced by a pre-trained BERT-type model that is learned from unlabeled phoneme-level data.
	- Among recent works, these are PnG BERT (takes both phonemes and graphemes as the input), Mixed-Phoneme BERT (takes both phonemes and sup-phoneme tokens as the input), Phoneme-level BERT (only taking phonemes as the input and employs an additional auxiliary task that predicts the corresponding grapheme for each phoneme).
	- It is worth exploring pre-trained models for phoneme representations in languages other than English.
    - We present and publicly release XPhoneBERT trained using the RoBERTa pre-training approach on 330M phoneme-level sentences from nearly 100 languages and locales. To convert texts into phonemes, we employ the CharsiuG2P toolkit that supports 90+ languages and locales.
	- Employing XPhoneBERT as an input phoneme encoder significantly boosts the performance of a strong TTS model in terms of naturalness and prosody.
	
Ploujnikov, A., & Ravanelli, M. (2022). SoundChoice: Grapheme-to-Phoneme Models with Semantic Disambiguation. arXiv, 2207.13703. Retrieved from https://arxiv.org/abs/2207.13703v1

    - Popular end-to-end speech synthesis models often fail to perform disambiguation of the homographs - a sequence of graphemes that can yield different pronunciations depending on the context (e.g. "read" - past vs present).
	- Grapheme-to-Phoneme (G2P) models can improve the system’s performance in these cases. However, these models are typically trained and evaluated on word-level lexicons (e.g., CMUDict), making it impossible to resolve homograph disambiguation.
	- We propose SoundChoice, a novel G2P model that operates at the sentence level. It enables the model to exploit the context and better resolve homograph disambiguation. SoundChoice models the sentence context using mixed representation composed of characters and BERT word embeddings (fig. 1).
	- SoundChoice uses CTC loss on top of the encoder and the standard sequence-to-sequence loss computed after the decoder. To further improve disambiguation, we propose a homograph loss that penalizes errors made on homograph words.
	- SoundChoice gradually switches from word- to sentence-level G2P using a curriculum learning strategy.
	- We also release the new LibriG2P dataset that combines data from LibriSpeech Alignments and the Wikipedia Homograph.
	
Gale, R. C., Salem, A. C., Fergadiotis, G., & Bedrick, S. (2023). Mixed Orthographic/Phonemic Language Modeling: Beyond Orthographically Restricted Transformers (BORT). ACL Anthology, 212–225. doi: 10.18653/v1/2023.repl4nlp-1.18

    - Explicit representation of phonology (such as processing ambiguous and noisy output from ASR systems, or handling of names and neologisms) are under-served by the current pre-training paradigm. LLM support for the international phonetic alphabet (IPA) ranges from poor to absent. The data used to pre-train an LLM incidentally contains little IPA content if any at all. Also, a task like MLM has little to gain from learning the sound relationships between words, so we have no reason to expect these models to adapt to phonetic tasks as well as they do semantic ones.
	- We propose BORT (Beyond Orthographically Restricted Transformers) by extending the pre-training of an existing LLM, BART. Given a document, we transform some words into IPA, then train the model to restore the orthography.
	- We evaluate the utility of BORT by fine-tuning to two clinically-motivated tasks. The model learned the task that the human annotators performed in AphasiaBank. In "hard" variant, we train the model to fill in paraphasias (i.e., incorrect pronunciations) with the intended orthographic word (given the surrounding context).
	
Vallés-Pérez, I., Beringer, G., Bilinski, P., Cook, G., & Barra-Chicote, R. (2023). SCRAPS: Speech Contrastive Representations of Acoustic and Phonetic Spaces. arXiv, 2307.12445. Retrieved from https://arxiv.org/abs/2307.12445v2

    - On the speech generation side, one of the main difficulties is to build a model that correctly aligns the phonetic and acoustic sequences, leading to a natural prosody with fluent speech and high intelligibility. On the opposite side, ASR systems struggle with long-tail words recognition, and speech vs background disentanglement.
	- We propose SCRAPS (Speech Contrastive Representation of Acoustic and Phonetic Spaces) (fig. 1). To the best of our knowledge, our work is the first attempt to use a CLIP-based strategy to learn joint phonetic and acoustic spaces.
	- For phonetic encoder, the input sequences are derived from the text transcriptions by using a simplified grapheme to phoneme processor (G2P) based on a dictionary of pronunciations.
	- In our model, LSTMs of the two encoders share weights to propagate the latent spaces compatibility back to the output of the transformer outputs (we empirically prove this assumption). That helps the time-dependent vectors live in the same space for both encoders. We use a contrastive loss to maximize the scores of the matching pairs, while minimizing the scores of the non-matching pairs.
	- We have trained a SCRAPS model on a large proprietary dataset composed of recordings of untrained speakers with variety of background noises, unnormalized pauses, and in some cases, even some samples with concurrent speech where one of the speakers dominates over the others. Each recording is accompanied with its corresponding transcription.
	- We perform a sensitivity and robustness analysis to study how does the model react against perturbation of the input data.
	- Downstream application 1: pretrained phonetic embeddings for speech generation. We have trained an autoregressive multispeaker text-to-speech model with attention on a proprietary dataset with 180,000 hours of de-identified en-US recordings. The acoustic decoder is autoregressive and attends to the output of the phonetic encoder and the speaker embedding. Then we have substituted the phonetic encoder by a SCRAPS pretrained phonetic encoder (only the transformer backbone, not the LSTM integrator). We observe that both architectures get a very similar final performance, but when using SCRAPS, the model converges much faster.
	- Downstream application 2: text-less intelligibility evaluation for voice conversion systems. Although SCRAPS is trained to match a sequence of phonemes to the corresponding audio, at inference time it can also be used to compute correspondence between two audio files without requiring any text. In this scenario, the SCRAPS score is computed between vectors of synthetic audio (VC) and source audio (pre-conversion).
	- IMO, it could be possible to use dynamic matching for both backbone output vector sequences, alleviating the need to compress all vector sequence in a single vector, which seems unreasonable.
	
Kinoshita, K., Ochiai, T., Delcroix, M., & Nakatani, T. (2020). Improving noise robust automatic speech recognition with single-channel time-domain enhancement network. arXiv, 2003.03998. Retrieved from https://arxiv.org/abs/2003.03998v1

    - There is a need for more research on effective single-channel speech enhancement (SE) front-ends for ASR. Although frequency masking approaches have successfully improved SE evaluation metrics, e.g., signal-todistortion ratio (SDR), this improvement did not lead to better ASR performance. This suggests that most single-channel SE approaches tend to introduce distortions that create a mismatch with the ASR back-end, therefore limiting their effect on ASR. Time-domain approaches have not been sufficiently investigated in the context of noise-robust ASR.
	- We adapt Conv-TasNet for the noise reduction task and call it Denoising-TasNet. We investigate two variants of Denoising-TasNet, one predicting only the enhanced speech and one with two outputs predicting speech and noise. The latter enables defining a multi-task loss, which can regularize the network training and is shown to achieve better ASR performance.
	- We perform experiments on CHiME-4 data. Denoising-TasNet significantly reduces WER on real recordings. It can improve ASR performance even without retraining the ASR back-end.
	- These demonstrates that single-channel noise reduction can still improve ASR performance.
	
Lee, W., Lee, G. G., & Kim, Y. (2023). Optimizing Two-Pass Cross-Lingual Transfer Learning: Phoneme Recognition and Phoneme to Grapheme Translation. arXiv, 2312.03312. Retrieved from https://arxiv.org/abs/2312.03312v1

    - We propose two-pass ASR system that first performs phoneme recognition and then translates the recognized phonemes into graphemes. Our methodology aims to advance ASR systems’ effectiveness in low-resource languages.
	- Rather than relying on grapheme units, which may not leverage the advantages of cross-lingual transfer learning, we choose to utilize phoneme units as the final output of our ASR system. We incorporate a dedicated translation model that converts phoneme outputs into grapheme units.
	- The IPA phoneme representation may not efficiently capture the shared phonetic characteristics across languages. Moreover, phoneme recognition results can be inaccurate, leading to error propagation during the phoneme-to-grapheme translation step.
	- We propose a novel approach called Pivot Phoneme Merging (PPM) to address these challenges. It groups phonemes based on shared articulatory features, facilitating improved vocabulary sharing across languages.
	- We also present a Global Phoneme Noise (GPN) generator that enables the pseudo-labeling of external text corpora, incorporating realistic ASR noise into the training process for P2G translation.
	
Futami, H., Tsunoo, E., Kashiwagi, Y., Ogawa, H., Arora, S., & Watanabe, S. (2023). Phoneme-aware Encoding for Prefix-tree-based Contextual ASR. arXiv, 2312.09582. Retrieved from https://arxiv.org/abs/2312.09582v1

    - End-to-end ASR systems have difficulties in recognizing uncommon words. Contextual biasing is a method to incorporate a contextual knowledge. We pass the model a list of words that are likely to appear in the context.
	- 1) One approach is shallow fusion with a contextual LM, where biasing words are compiled into Weight Finite State Transducer. However, they require some heuristics and careful tuning of an LM weight to avoid under- or over-biasing.
	- 2) In attention-based deep context approaches, each biasing word is converted into an encoding vector, and an ASR decoder attends to the encodings (word-level biasing). However, they have an issue handling a large number of biasing words.
	- 3) To efficiently handle them, a prefix tree, or a trie, -based deep biasing methods have been considered (subword-level biasing). 
	- 4) TCPGen further extends prefix-tree-based biasing. It works with a pre-trained ASR model such as Whisper without modifying their architecture.
	- Existing prefix-tree-based biasing methods rely solely on their textual representations. As rare biasing words sometimes have pronunciations that are difficult to estimate from text, it is important to provide their pronunciation information as a clue to recognize such words. This is especially common for ideographic characters such as Japanese kanji. For subword-level biasing, phonemes aligned to each subword are required. This is not trivial because pronunciation is typically defined for the entire word.
	- We propose subword-level phoneme-aware encodings for TCPGen. To obtain alignment from a subword to phonemes, we consider using the attention weights of seq2seq G2P model or EM algorithm-based alignment. It would be preferable for queries to be also explicitly aware of phonemes. To this end, we train the end-to-end ASR model with auxiliary CTC loss whose target is a phoneme sequence, and the CTC predictions are incorporated into the formulation of query in TCPGen.

Yusuyin, S., Ma, T., Huang, H., Zhao, W., & Ou, Z. (2024). Whistle: Data-Efficient Multilingual and Crosslingual Speech Recognition via Weakly Phonetic Supervision. arXiv, 2406.02166. Retrieved from https://arxiv.org/abs/2406.02166v1

    - In multilingual and crosslingual ASR (MCL-ASR), while requiring pronunciation lexicons, pre-training with phonetic supervision is more advantageous for information sharing between different languages. There have been no solid experiments to study which approach is better or if they yields similar results.
	- Phoneme-based models naturally overcome language imbalance and can be efficiently trained on natural data mixing, while subword-based models need careful tokenization and data mixing in training.
	- We propose Whistle (Weakly phonetic supervision strategy for multilingual and crosslingual speech recognition) to explore supervised pre-training with weakly phonetic supervision, towards data-efficient MCL-ASR (this is in spirit similar to weakly graphemic supervision in Whisper). We obtain the IPA phonetic transcripts by leveraging the LanguageNet G2P models available for 142 languages with the phoneme error rates (PERs) ranging from 7% to 45%.
	- Besides the performance advantage of phoneme-based supervision over subword-based supervision, we find that phoneme-based models tend to be more training efficient, i.e., they can converge with fewer optimzation steps, with 24% reduction.
	
He, Y., Prabhavalkar, R., Rao, K., Li, W., Bakhtin, A., & McGraw, I. (2017). Streaming Small-Footprint Keyword Spotting using Sequence-to-Sequence Models. arXiv, 1710.09617. Retrieved from https://arxiv.org/abs/1710.09617v1

    - Keyword spotting is the task of detecting specific words or phrases in speech utterances. An example of such technology is speech-enabled assistant “Okay/Hey Google” on Google Home.
	- We explore RNN-T, to build a streaming keyword spotting system which can be used to detect arbitrary keywords.
	- We find that RNN-T system trained to predict phonemes, when augmented with an additional "end-of-word" symbol strongly outperforms a strong keyword-filler baseline (fig. 4). The <eow> token is useful, for example, to prevent detection of the keyword "Erica (E r\ @ k @)" inside the word "America (@ m E r\ @ k @)" Additionally, we propose a novel technique to bias the search towards a specific keyword of interest using an attention mechanism (fig. 1c).
	
Sriram, A., Jun, H., Satheesh, S., & Coates, A. (2017). Cold Fusion: Training Seq2Seq Models Together with Language Models. arXiv, 1708.06426. Retrieved from https://arxiv.org/abs/1708.06426v1

    - Because LMs can be trained from abundantly available unsupervised text corpora, they can improve Seq2Seq’s performance. A standard way is to linearly combine the score of the task-specific Seq2Seq model with that of an auxiliary langauge model to guide beam search (Shallow Fusion).
	- Deep Fusion (see "On using monolingual corpora in neural machine translation") learns to fuse the hidden states of the Seq2Seq decoder and a neural LM with a gating mechanism, after the two models are trained independently.
	- The biggest disadvantage with Deep Fusion is that the task-specific model is trained independently from the LM. This means that the Seq2Seq decoder needs to learn a LM from the training data labels, which can be rather parsimonious compared to the large text corpora available for language model training. For example, if a Seq2Seq model fully trained on legal documents is later fused with a medical language model, the decoder still has an inherent tendency to follow the linguistic structure found in legal text. Thus, in order to adapt to novel domains, Deep Fusion must first learn to discount the implicit knowledge of the language.
    - Also, in Deep Fusion a considerable portion of the decoder capacity is wasted, since decoder learns an implicit language model.
	- We introduce Cold Fusion to overcome both these limitations. Cold Fusion encourages the Seq2Seq decoder to learn to use the external language model during training.
	- Cold Fusion can almost completely transfer to a new domain for the speech recognition task with 10 times less data.
	- At each time step t, we obtain LM output probabilities and process them with DNN (eq. 4a). Then we concatenate the state of the task specific model with the DNN outputs, with gating (eq. 4b, 4c). We then process the result with another DNN (eq. 4d), obtaining the final probabilities. Since we use LM probabilities, we can generalize to new LMs (with the same vocabulary) at inference time. Since LM logits can have arbitrary offsets, the maximum value is subtracted off before feeding into the layer. While DNNs can have any depth, we found a single affine layer with ReLU to be helpful.
	- We collected two data sets with audio recordings: one based on search queries which served as our source domain, and another based on movie transcripts which served as our target domain.
	
Xu, H., Chen, T., Gao, D., Wang, Y., Li, K., & Goel, N. . A Pruned Rnnlm Lattice-Rescoring Algorithm for Automatic Speech Recognition. 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE. doi: 10.1109/ICASSP.2018.8461974

    - Lattice-rescoring is a common approach to take advantage of RNN LM in ASR, where a word-lattice is generated from 1st-pass decoding and the lattice is then rescored with a neural model.
	- Because an RNN LM theoretically encodes infinite history lengths, it is virtually impossible to compile it to a static decoding graph; for this reason, RNNLMs are usually not directly used in decoding. (IMO not clear) The common method is to decode on a precompiled decoding graph which is usually generated from n-gram LM as the first pass; then use neural model to rescore the hypotheses (N-best list rescoring and lattice-rescoring).
	- We propose a pruning-based lattice rescoring method.
	- This project is developed as part of ASR toolkit Kaldi.
	
Zhao, D., Sainath, T. N., Rybach, D., Rondon, P., Bhatia, D., Li, B., & Pang, R. (2019). Shallow-Fusion End-to-End Contextual Biasing. doi: 10.21437/Interspeech.2019-1209

    - In ASR, contextual biasing to a specific domain, including a user’s song names, app names and contact names, is an important component of any production-level system.
	- Compared to conventional models, end-to-end models make more errors in rare, context-dependent words and phrases. Also, proper nouns are usually pruned during beam search before contextual biasing can be applied, as biasing happens at the end of a word (rather than the grapheme/wordpiece units the E2E model predicts).
	- We explore biasing at the sub-word unit level (grapheme, wordpiece) rather than the word-level.
	- Second, we explore applying the contextual finite state transducer (FST) before beam pruning rather than after.
	- Third, because contextual n-grams are typically used with a common set of prefixes ("call", "text"), we investigate incorporating these prefixes into shallow fusion. This helps tremendously to avoid degradation for anti-context (on utterances we do not want to bias).
	- Fourth, by improving proper noun modeling by training with a large amount of unsupervised data, we can improve performance further.
	
Gulcehre, C., Firat, O., Xu, K., Cho, K., Barrault, L., Lin, H.-C., ...Bengio, Y. (2015). On Using Monolingual Corpora in Neural Machine Translation. arXiv, 1503.03535. Retrieved from https://arxiv.org/abs/1503.03535v2

    - Incorporating monolingual corpora can improve a translation system on a low-resource, as well as high-resource language pair, and a domain restricted translation problem (Chinese-English SMS chat).
    - We propose two ways to integrate a LM trained only on monolingual data (target language) into an NMT system: shallow fusion and deep fusion.
	- Both models are word-based: vocabularies are constructed with the most common words in the parallel corpora (sec. 6.1.1, 6.2.3).
	- Without loss of generality, we use a LM based on RNN.
	- Shallow fusion is analogous to how LMs are used in the decoder of a usual statistical machine translation system. At each time step, we have a set of M hypotheses (beams?) and N possible next words for each hypothesis. This allows us to calculate MN scores: each score is the summation of the score of the hypothesis and the score given by the NMT to the next word. These scores are then sorted, and top K ones are selected as candidates. Then the candidates are rescored with LM by weighted sum of the scores given by the translation model and the language model, where weight is a hyper-parameter (eq. 5).
	- In deep fusion, we concatenate hidden states of NMT decoder and LM. The model is then finetuned to use the hidden states from both of these models when computing the output probability of the next word. In this paper, we tune only the output parameters to ensure that the structure learned by the LM from monolingual corpora is not overwritten.
	- In deep fusion, we also augment the decoder with a "controller" mechanism. For example, if a noun is to be translated, it may be better to ignore any signal from the LM, as it may prevent the decoder from choosing the correct translation. Intuitively, this mechanism helps the model dynamically weight the different models depending on the word being translated. At each time step, the controller takes the hidden state of the LM as input and outputs a scalar. It is then multiplied with the hidden state of the LM.
	- Where the domain of the bilingual and monolingual corpora were similar (De-En, Cs-En), we observed improvement with both deep and shallow fusion methods. In the case where they were dissimilar (Zh-En), the improvement using shallow fusion were much smaller.
	
Pundak, G., Sainath, T. N., Prabhavalkar, R., Kannan, A., & Zhao, D. (2018). Deep context: end-to-end contextual speech recognition. arXiv, 1808.02480. Retrieved from https://arxiv.org/abs/1808.02480v1

    - We propose Contextual-LAS (CLAS), all-neural mechanism which can leverage contextual information – provided as a list of contextual phrases – to improve ASR performance.
	- Our technique consists of first embedding each phrase into a fixed-dimensional representation, and then employing an attention mechanism to summarize the available context at each step of the model’s output predictions (fig. 1b).
	- The proposed method does not require that the particular context information be available at training time, and the method does not require careful tuning of rescoring weights.
	- This is a generalization of the previously proposed method in "Streaming small-footprint keyword spotting using sequence-to-sequence models".
	
Gandhe, A., & Rastrow, A. (2019). Audio-attention discriminative language model for ASR rescoring. arXiv, 1912.03363. Retrieved from https://arxiv.org/abs/1912.03363v2

	- We proposed to use an attention-based LM for second-pass rescoring of N-best lists generated by a conventional ASR system.
    - An RNNLM style model is trained using word-level contextual input, while simultaneously attending to audio, using a minimum word error rate criterion, which learns to rescore the N-best hypotheses list from a first pass system.
	
McDermott, E., Sak, H., & Variani, E. (2020). A Density Ratio Approach to Language Model Fusion in End-To-End Automatic Speech Recognition. arXiv, 2002.11268. Retrieved from https://arxiv.org/abs/2002.11268v3

    - There are many situations where we would like to use a separate LM to complement or modify a given ASR system to 1) make use of text-only training data, 2) bias the recognition grammar towards a list of specific words or phrases for a specific context.
	- Approaches such as Deep Fusion, Cold Fusion and Component Fusion have not replaced the simple Shallow Fusion method as the go-to method in most of the ASR community, because Shallow Fusion does not require model retraining.
	- Sec. 2 describes the Noisy Channel generative model underlying the origins of statistical ASR. It combines generative acoustic and language models. Though lacking in discriminative power, the paradigm provides a clear theoretical framework for decoupling the acoustic model p(X|W) and LM p(W) (where X is audio and W is text). Also this section describes the "hybrid" approach that allows to estimate a "pseudo-generative" score with a NN and then use eq. 1. In comparison, the popular Shallow Fusion approach is not justified according to probability theory.
	- We propose Density Ratio method which produces consistent gains over Shallow Fusion in a cross-domain scenario.
	- Let we have a source domain ASR model p_source(W|X), called a "posterior", a source domain LM p_source(W), a target domain LM p_target(W), and let p(X|W) be equal in both domains: the domains are acoustically consistent.
	- We can estimate a target posterior with eq. 6, where we call p_target(W) / p_source(W) a "density ratio". We can inject this term to RNN-T decoding with eq. 11.
	- So, our method is purely a decode-time method, no joint training is involved, but it does require tuning of the LM scaling factor(s), as does Shallow Fusion. A held-out set can be used for that purpose.
	- IMO, in eq. 1 it is not clear how do we obtain the final prediction; also in eq. 1, while there is no length penalty, it is not really clear what does p(text) means; usually LMs include <eos> token that has no connection with real life; for example, what is larger: the probability of all texts of length 10 or the probability of all texts of length 20? a problem is that LMs do not model the presence vs absence of text.
	
Bai, Y., Yi, J., Tao, J., Wen, Z., Tian, Z., & Zhang, S. (2019). Integrating Knowledge into End-to-End Speech Recognition from External Text-Only Data. arXiv, 1912.01777. Retrieved from https://arxiv.org/abs/1912.01777v2

    - In end-to-end ASR is worth to investigate a different method to integrate the knowledge from the text-only data without external modules (LMs) at the inference stage. Another issue is that autoregressive decoders are difficult to leverage the right context.
	- We propose Learn Spelling from Teachers (LST): transferring the knowledge from the LM to the attention-based encoder-decoder via teacher-student learning. The LM  provides soft labels of training transcriptions which are used to train the "student" model.
	- We propose a LM called Causal clOze completeR (COR), which models the whole context of a sentence. It uses the whole context (including the left context and the right context).
	- This is an extension of the paper "Learn Spelling from Teachers: Transferring Knowledge from Language Models to Sequence-to-Sequence Speech Recognition".
	
Kanda, N., Lu, X., & Kawai, H. (2017). Maximum-a-Posteriori-Based Decoding for End-to-End Acoustic Models. IEEE/ACM Trans. Audio Speech Lang. Process., 25(5), 1023–1034. doi: 10.1109/TASLP.2017.2678162

    - In end-to-end ASR, "external" LM is still essential to obtain the best results during the decoding stage although an "internal" LM is implicitly trained in the end-to-end acoustic modeling, because the transcriptions in a speech corpus are normally insufficient for training high quality LMs (CTC has limited representation ability because of the independence assumption in the model; however, even in such case, we think the model somehow learns an internal LM as long as the training criterion of the model is based on the distribution of subword sequences).
	- How can we integrate the word-level LM score into the subword-level end-to-end ASR? Previous approaches, such as log-linear interpolation, lack theoretical justification and hence has led the systems to produce inconsistent results.
	- IMO, the formulas in the proposed method are strange, since the transition from eq. 8 to 9 is not clear: 1) why ":=" ? 2) why do we need alpha? 3) if there is a one-to-one mapping between s ans W, why do we need a term Pr(W|s)? It seems like only one term in the sum will be non-zero.
	- We have previously published two papers on the preliminary work: "Training data pseudo-shuffling and direct decoding framework for recurrent neural network based acoustic modeling" and "Maximum a posteriori Based Decoding for CTC Acoustic Models". Due to the lack of sufficient experiments in various conditions, our previous study had inadequate insights to the framework.

Gong, Y., Khurana, S., Karlinsky, L., & Glass, J. (2023). Whisper-AT: Noise-Robust Automatic Speech Recognizers are Also Strong General Audio Event Taggers. arXiv, 2307.03183. Retrieved from https://arxiv.org/abs/2307.03183v1

	- We show (fig. 1) the intermediate representations of Whisper lead to the best linear probing environmental sound classification accuracy, comparing to other model checkpoints such as Hubert, Wav2vec2 etc., indicating Whisper encodes most background sound information.
	- In addition, for all other ASR models, representations from deeper layers led to lower sound classification accuracies, showing that the models are learning to encode speech information, and ignore background sound information. Whisper does not have this behavior: it encodes background sound information even in its deepest (encoder) layer.
	- We observe a positive correlation between Whisper’s robustness against a specific background sound type and its potential ability to recognize it. (see fig. 2 and its description, note that the Y axis is inverted). We find the potential ability to recognize a sound type is a necessary but not sufficient condition for Whisper to be robust to it.
	- It is commonly believed that the representation of a robust ASR model should be noise-invariant. However, the above results reveals that the robustness mechanism of Whisper is different from other ASR models: Whisper first encodes the background sound and then transcribes text conditioned on the type of noise.
	- Based on the results, we build a unified model for ASR and Audio Tagging (i.e., recognize general audio events).
	- IMO, 1) two types of background noise that sound very similar are two different classes, the F1 recognition score (fig. 2, X axis) will drop, comparing to both types merged in one class, but Y axis (robustness) will not change. This makes X axis not so much informative. 2) Some types of noise can be more challenging then others with the same SNR. For example, short peaky sounds, even with very high negative SNR, mask only short parts of speech, allowing to restore them from context. Also, low-frequency sounds should distract less than sounds with the speech frequency with the same SNR, etc. The fact that the F1 classification score based on the Whisper encoder outputs for group A is better can be explained by the fact that such noises are more difficult to filter out than monotonous noises. That is, monotonous noise can be filtered out and does not get into the last layer of the encoder by itself, but it is also difficult to isolate the phonemes of the model if this noise is strong, which is why the model's stability to such noise is poor. That is, it is possible that Whisper does not filter out noises from group A not because they are useful in some way (as the authors assume), but because it simply cannot. Although, on the other hand, some noises can of course be useful because they allow us to understand the context in which the speech is made, and therefore the vocabulary used. For example, the noise of a fight can mean the use of vocabulary from computer games, etc.
	
McDermott, E. (2018). A Deep Generative Acoustic Model for Compositional Automatic Speech Recognition. Retrieved from https://www.semanticscholar.org/paper/A-Deep-Generative-Acoustic-Model-for-Compositional-McDermott/0233d263c7798d550249d94f1a3f434864baa44b

    - Section 1.1 describes the classical compositional ASR approach, when we model p(audio|text), and then we can combine it with p(text) from LM, using Bayes rule. Gaussian Mixture Models (GMMs) used jointly with 1st-order Hidden Markov Models (HMMs) were well-suited to this modular approach, as they directly provide p(audio|text), see eq. 2-4 (W is text, X is audio, S_w is an alighment of text to audio; IMO the formula 2 is strange because of frame independence assumption). Other probabilistic modules such as a pronunciation dictionary can be introduced into the overall chain, again combining with the other components according to Bayes’ rule.
	  - Section 1.2 describes the adaptation of discriminative DNNs into the above scheme. The popular "hybrid" approach converts p(text|audio) to a scaled p(audio|text) using eq. 5-6 (see "Connectionist speech recognition: a hybrid approach"). The overall construction of a sequence-level training objective, converting local frame-level scores from a discriminative model into "generative" scaled likelihoods, only to then plug those into a discriminative sequence training criterion, may seem like a strange hybrid indeed. Nonetheless, this has been a remarkably effective approach, that still constitutes the SOTA (2018).
	  - Section 1.3 describes end-to-end discriminative sequence-level models, such as LAS. However, combination with LMs is not theoretically justified (sec. 1.4).
	  - ... TODO

## Code generation

@incollection{Liu2023Jul,
	author = {Liu, Kaibo and Han, Yudong and Zhang, Jie M. and Chen, Zhenpeng and Sarro, Federica and Harman, Mark and Huang, Gang and Ma, Yun},
	title = {{Who Judges the Judge: An Empirical Study on Online Judge Tests}},
	booktitle = {{TrickyBugs}},
	pages = {334--346},
	year = {2023},
	month = jul,
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	doi = {10.1145/3597926.3598060}
}
  - NOTE: This work was extended in "TrickyBugs: A Dataset of Corner-case Bugs in Plausible Programs", see for more detailed review.
  - Our goal is to assess the reliability of test suites in AtCoder platform. We collect code solutions to coding problems from AtCoder platform, that pass a test suite (a set of input/output pairs).
  - We use widely studied test assessment metrics:
  - 1) Line and branch coverage. These metrics measure the percentage of executed lines/branches of the source code against the total lines/branches of code when running the test suite. Bugs in the uncovered code can never be detected by the test suite.
  - 2) Mutation score. We make minor changes to the source code by using different mutation operators. The modified programs are called mutants. A mutant is called an equivalent mutant if its semantic is equivalent to the original program, despite being syntactically different (IMO, probably within the input constraints). If the test suite is not able to differentiate a mutant and its original program, we call this mutant a survived mutant. Otherwise, we call it a killed mutant. The mutation score is the percentage of killed mutants against the total number of non-equivalent mutants. In practice, mutation score is often calculated as the percentage of killed mutants against the whole set of mutants. Since uncovered mutants cannot be killed, we use both original mutation score and covered mutation score. For survived mutants that are not killed by the test suites, we check whether they are equivalent mutants by manual analysis.
  - For coverage, for Python solutions, we use Coverage.py; for C++ solutions, we use the source-based code coverage feature of clang; for Java solutions, we use JaCoCo. To conduct mutation testing, we use mutmut for Python, Mull for C++, PITest for Java.
  - The coverage for Java solutions are relatively low: 78.2% of Java solutions are fully line covered, and 74.2% of Java solutions are fully branch covered. By manual analysis we discover three main types of uncovered code (listing 1): (i) missing branch that yields a false positive solution (which pass tests but is buggy); (ii) the uncovered code from user templates which are dead code (especially in Java code); (iii) debugging code that does not affect the behaviour of the functional code. This pose concerns regarding the effectiveness of coverage criteria in test suite assessment.
  - Only around 60% of the Python solutions, 40% of the Java solutions, and 35% of the C++ solutions have a full mutation score. So, achieving full mutation coverage is more difficult than achieving full statement or branch coverage. We manually check 381 Python survived mutants in covered code. The majority (95%) of the survived mutants are either fully equivalent (30%) or equivalent under input constraints (65%). Inequivalent mutants, which survived due to inadequate test suites, account for less than 5%.
  - To identify false positive solutions, we randomly generate additional test inputs. We then manually inspect and categorize false positive solutions in table 7. Most of them (73%) are missing corner cases. Different false positive solutions of the same problem often miss the same corner case.
  - Among the false positive solutions, 89% have full line coverage, 79% have full branch coverage, and 38% have full mutation scores. So, a large ratio of bugs will be ignored if developers rely only on the coverage and mutation score.

@incollection{Liu2024Apr,
	author = {Liu, Kaibo and Han, Yudong and Liu, Yiyang and Chen, Zhenpeng and Zhang, Jie M. and Sarro, Federica and Huang, Gang and Ma, Yun},
	title = {{TrickyBugs: A Dataset of Corner-case Bugs in Plausible Programs}},
	booktitle = {{ACM Conferences}},
	pages = {113--117},
	year = {2024},
	month = apr,
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	doi = {10.1145/3643991.3644870}
}
  - In existing bug datasets like Defects4J, CodeFlaws, QuxixBugs most of the bugs are identified by a simple and ordinary test case and do not necessarily represent corner cases.
  - We refer to a program that passes existing tests as a plausible program, which may either be correct or buggy. Bugs in plausible programs are typically harder to detect and are often logical corner cases.
  - Here we extend a TrickyBugs dataset proposed in our prior work "Who Judges the Judge: An Empirical Study on Online Judge Tests". There it was noted that traditional test adequacy metrics such as line/branch coverage and mutation score have limitations when applied to such cases.
  - TrickyBugs is an evaluation dataset for the following task: how to fix and locate faults, given only source code, program specification and passed test cases? It also can be utilized to study how to better measure and improve test adequacy.
  - We collect our data from AtCoder, where users are requested to submit their code solutions for different coding tasks, and the backend test cases (input/output pairs) will judge the submitted programs. We only retain the passing programs. At this step, we get approximately 230,000 human-written programs in C++, 140,000 programs in Java, and 169,000 programs in Python from 939 coding tasks. Then we filter out all coding tasks with multiple correct outputs for one input. We also collect the original test cases and difficulty of the coding tasks from an AtCoder’s official post and a third-party website that evaluates the difficulty of coding tasks based on AtCoder’s rating system.
  - The core step for dataset construction is to find bugs. We then randomly generate 100 additional test inputs for each coding task according to the input constraints. Then we compare the outputs of two or more different implementations. If there is any discrepancy among the outputs, we find bugs. The output that dominates the others in terms of proportion (a dominance ratio) is considered a correct test output. We also perform manuall check if dominance ratio < 0.95. The generated input/output pair is consitered an additional test case.
  - To broaden the applicability of the dataset, we also employ the gpt-3.5-turbo model to assist us in fixing these buggy plausible programs, and then verify the corrections manually.
  - In total, TrickyBugs dataset contains 1,405 buggy programs in C++, 792 in Java, and 846 in Python from 324 coding tasks, and additionally 1,361 fixed programs for 224 coding tasks. In table 1, one can see the average lines of code (Average LOC), average difficulty of the coding tasks (Average Diff.) etc.
  - Are 100 generated test inputs enough? According to fig. 2, the number of discovered buggy plausible programs reaches a plateau when there are around 95 inputs. (IMO the plateau of 5 samples is not enough; there are still bugs to discover; however, the case when we discover more and more bugs with a large amount of random input samples is, probably, typical for a small number of tasks)
  
## Fundametals (various)

@article{Dao2022May,
	author = {Dao, Tri and Fu, Daniel Y. and Ermon, Stefano and Rudra, Atri and R{\ifmmode\acute{e}\else\'{e}\fi}, Christopher},
	title = {{FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness}},
	journal = {arXiv},
	year = {2022},
	month = may,
	eprint = {2205.14135},
	doi = {10.48550/arXiv.2205.14135}
}
  - Many approximate attention methods (sparse-approximation, low-rank approximation, their combinations) focus on FLOP reduction and tend to ignore overheads from memory access (IO).
  - However, on modern GPUs, compute speed has out-paced memory speed, and most operations in Transformers are bottlenecked by memory accesses.
  - We propose FlashAttention (see Algorithm 1), an exact attention implementation with far fewer memory accesses. Our main goal is to avoid reading and writing the attention matrix to and from HBM (relatively slow GPU high bandwidth memory, fig. 1, left). This requires:
  - 1) Computing the softmax reduction without access to the whole input. We restructure the attention computation to split the input into blocks and make several passes over input blocks, thus incrementally performing the softmax reduction (also known as tiling).
  - 2) Not storing the large intermediate attention matrix for the backward pass. We store the softmax normalization factor from the forward pass to quickly recompute attention on-chip in the backward pass.
  - Even with the increased FLOPs due to recomputation, our algorithm both runs faster (up to 7.6x on GPT-2) and uses less memory — linear in sequence length — than standard attention, thanks to the massively reduced amount of HBM access.
  
@article{Dao2023Jul,
	author = {Dao, Tri},
	title = {{FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning}},
	journal = {arXiv},
	year = {2023},
	month = jul,
	eprint = {2307.08691},
	doi = {10.48550/arXiv.2307.08691}
}
  - With FlashAttention, the forward pass only reaches 30-50% of the theoretical maximum FLOPs/s of the device, while the backward pass reaching only 25-35% of maximum throughput on A100 GPU. In contrast, optimized GEMM (matrix-multiply) can reach up to 80-90% of the theoretical maximum device throughput.
  - While the non-matmul FLOPs only account for a small fraction of the total FLOPs (in FlashAttention), they take longer to perform.
  - We propose FlashAttention-2, an exact attention implementation: 1) we reduce number of non-matmul FLOPs, 2) we parallelize both the forward pass and backward pass along the sequence length dimension, in addition to the batch and number of heads dimension, 3) further reduce communication and shared memory reads/writes.
  - FlashAttention-2 achieves around 2x speedup over FlashAttention, reaching up to 73% of the theoretical max throughput in the forward pass, and up to 63% of the theoretical max throughput in the backward pass (with or without causal mask, for different head dimensions).
  
@article{Shah2024Jul,
	author = {Shah, Jay and Bikshandi, Ganesh and Zhang, Ying and Thakkar, Vijay and Ramani, Pradeep and Dao, Tri},
	title = {{FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision}},
	journal = {arXiv},
	year = {2024},
	month = jul,
	eprint = {2407.08608},
	doi = {10.48550/arXiv.2407.08608}
}
  - We observe that FlashAttention-2 nonetheless achieves poor utilization on newer GPUs relative to optimized matrix multiplication (GEMM) kernels, such as 35% vs. 80-90% on the Hopper H100 GPU. The technical challenge is to redesign FlashAttention-2 to make use of new hardware features.
  - We propose FlashAttention-3, which synthesizes three new ideas:
  - 1) Asynchronous execution of data movement by splitting producers and consumers of data into separate warps.
  - 2) We rework the FlashAttention-2 algorithm to circumvent certain sequential dependencies between softmax and the GEMMs.
  - 3) We make use of the FP8 Tensor Cores for GEMM. We use the techniques of block quantization and incoherent processing to mitigate the loss of accuracy that results from moving to FP8 precision. 

@article{Gomez2017Jul,
	author = {Gomez, Aidan N. and Ren, Mengye and Urtasun, Raquel and Grosse, Roger B.},
	title = {{The Reversible Residual Network: Backpropagation Without Storing Activations}},
	journal = {arXiv},
	year = {2017},
	month = jul,
	eprint = {1707.04585},
	doi = {10.48550/arXiv.1707.04585}
}
  - As networks grow wider and deeper, storing the activations imposes an increasing memory burden.
  - We present Reversible Residual Networks (RevNets), a variant of ResNets where each layer’s activations can be computed from the next layer’s activations. This enables us to perform backpropagation without storing the activations in memory. One can perform backprop on a sequence of reversible blocks if one is given simply the activations and their derivatives for the top layer in the sequence. In general, a practical architecture would likely also include non-reversible layers, such as subsampling layers.
  - Our proposed method is inspired by nonlinear independent components estimation (NICE) based on learning a non-linear bijective transformation between the data space and a latent space.
  - We must partition the units in each layer into two groups, denoted x1 and x2; we assume this is done by partitioning the channels, since we found this to work the best in our experiments. Each reversible block takes inputs (x1, x2) and produces outputs (y1, y2) as shown in fig. 2a. Each layer’s activations can be reconstructed from the next layer’s activations (fig. 2b).
  - Note that unlike residual blocks, reversible blocks must have a stride of 1 because otherwise the layer discards information, and therefore cannot be reversible.
  - Surprisingly, constraining the architecture to be reversible incurs no noticeable loss in performance (on CIFAR-10, CIFAR-100, and ImageNet) with only a modest increase in the training time.

@article{Kitaev2020Jan,
	author = {Kitaev, Nikita and Kaiser, {\L}ukasz and Levskaya, Anselm},
	title = {{Reformer: The Efficient Transformer}},
	journal = {arXiv},
	year = {2020},
	month = jan,
	eprint = {2001.04451},
	doi = {10.48550/arXiv.2001.04451}
}
  - We introduce the Reformer model, the Transformer modification. In Reformer, the memory we use for activations in the whole network is independent of the number of layers.
  - 1) We apply the RevNet idea to the Transformer (eq. 9). Now each layer has two input vector sets (x1, x2) and two output vector sets (y1, y2), as done in RevNet. We don't need to store outputs from all layers for backprop, since we can recompute them top-down. We show that it performs the same as the normal Transformer when using the same number of parameters; we achieve this by having both x1 and x2 have size d_model.
  - 2) In normal Transformer, during forward pass we calculate the whole intermediate FFN activations that occupy a lot of memory, since FFN hidden dimension is ofter high. To avoid this, we split the computation into chunks. For models with large vocabulary, we also chunk the log-probabilities at the output and calculate the loss for sections of the sequence at a time to save memory.
  - 3) A lot of layers require a lot of parameters. We can swap them to and from CPU memory when this layer is not computing. In a standard Transformer this would be inefficient because memory transfer to CPU is slow. The batch size multiplied by length in Reformer is much larger though (since we use less memory for activations and so can allow for larger batch size) and therefore the amount of compute done with the parameters amortizes the cost of their transfer.
  - 4) We propose to approximate attention computation based on locality-sensitive hashing (LSH). It replaces the O(L^2) factor in attention layers with O(L log L). We use the same Q and K matrices (shared-QK attention). LSH can quickly find approximate nearest neighbors in high-dimensional spaces. A hashing scheme that assigns each vector X to a hash h(X) is called locality-sensitive if nearby vectors get the same hash with high probability and distant ones do not. After doing LSH, we obtain several hash buckets, and we allow attention only within each hash bucket (fig. 2). With hashing, there is always a small probability that similar items nevertheless fall in different buckets. This probability can be reduced by doing multiple rounds of hashing with distinct hash functions. 
  - We also describe how to implement masking in LSH attention. Usually causal mask do allow a position to attend to itself. Such behavior is undesirable in a shared-QK formulation because the dot-product of a query vector with itself will almost always be greater than the dot product of a query vector with a vector at another position. We therefore modify the masking to forbid a token from attending to itself, except the first token in a sequence.
  
@article{Roy2020Mar,
	author = {Roy, Aurko and Saffar, Mohammad and Vaswani, Ashish and Grangier, David},
	title = {{Efficient Content-Based Sparse Attention with Routing Transformers}},
	journal = {arXiv},
	year = {2020},
	month = mar,
	eprint = {2003.05997},
	doi = {10.48550/arXiv.2003.05997}
}
  - Fixing the sparsity pattern of self-attention (such as local or strided attention) can limit its ability to pool in information from large contexts. "Adaptively sparse transformers", on the other side, does require instantiating a full dense attention matrix prior to sparsification. How to combine both approaches?
  - We propose Routing Transformer which clusters both keys K and queries Q (both normalized by Layer Normalization with the scale and bias terms disabled) using k-means clustering. Then only queries and keys from the same cluster are considered for attention.
  - We apply mini-batch k-means to train the cluster centroids. During training, we update each cluster centroid by an EMA of all the keys and queries assigned to it. We also exclude padding tokens from affecting the centroids.
  - In order to infer balanced routing patterns, for every centroid we sort tokens by distance and cluster membership is determined by top-k. It guarantees that all clusters have the same size, which is extremely important in terms of computational efficiency.
  
@article{Bukharin2023Nov,
	author = {Bukharin, Alexander and Zhao, Tuo},
	title = {{Data Diversity Matters for Robust Instruction Tuning}},
	journal = {arXiv},
	year = {2023},
	month = nov,
	eprint = {2311.14736},
	doi = {10.48550/arXiv.2311.14736}
}
  - For LM instruction tuning, recent work have identified that the instruction responses should be high quality and the instructions should cover a wide range of tasks (i.e. be diverse). How we can select high quality and diverse dataset without manual curation from human experts?
  - We propose a new algorithm, QDIT, to measure and optimize the diversity and quality of instruction tuning datasets.
  - The facility location function measures how well represented each data point in the full dataset is by the data points in the selected subset. With this diversity function and quality functions from prior works, we then define a dataset’s quality-diversity score as a simple linear combination of dataset quality and diversity.
  - QDIT mploys a greedy strategy, where the data point that will improve the joint quality-diversity score the most is selected at each time step. This easily scales to datasets with millions of instruction.
  - To demonstrate the connection between facility location function and ataset diversity, we extract the root verb and first direct noun from each instruction in the Alpaca dataset. We then plot the distribution of verb-noun pairs in Figure 1 for random, quality driven, and QDIT data selection. QDIT is able to improve diversity without significantly decreasing data quality.
  - For training, we use a combined dataset of Alpaca 52K, Dolly 15K , and the OIG-small-chip2 dataset (210K). To measure instruction-response quality, we use the provided ChatGPT quality scores for Alpaca and for all other datasets we use the reward model from Raft which is trained on the Anthropic Helpful Harmless dataset.
  - With QDIT, we improve worst case performance while maintaining or improving best case and average performance for robust instruction following.
  
@article{Peters2019May,
	author = {Peters, Ben and Niculae, Vlad and Martins, Andr{\ifmmode\acute{e}\else\'{e}\fi} F. T.},
	title = {{Sparse Sequence-to-Sequence Models}},
	journal = {arXiv},
	year = {2019},
	month = may,
	eprint = {1905.05702},
	doi = {10.48550/arXiv.1905.05702}
}
  - We propose neural sparse seq2seq models that replace the softmax (both in the attention and output) by α-entmax.
  - The recently-introduced α-entmax (see "Learning classifiers with fenchel-young losses: Generalized entropies, margins, and algorithms") is defined in eq. 10: given scores (kind of "logits") z, we need to find such probability vector that maximizes sum of the two terms: (i) dot product between p and z and (ii) the Tsallis entropy of p. This vector will be the output. For α = 1 it exactly recovers the softmax mapping. For α > 1 it permits sparse solutions, and for α = 2, it recovers the sparsemax mapping. To compute the value of α-entmax, one must find the threshold τ such that the r.h.s. in 13. 6 sums to one.
  - In this work we show how to calculate the α-entmax Jacobian (and the Hessian), enabling to backprop through it.
  - We derive a novel exact algorithm for the case of 1.5-entmax, achieving processing speed close to softmax on the GPU. For arbitrary α, we investigate a GPU-friendly approximate algorithm.
  - Our sparse attention is a form of inductive bias that increases focus on relevant source words and makes alignments more interpretable.
  - Our sparse output probabilities, together with auto-regressive models, can lead to probability distributions that are nonzero only for a finite subset of all possible strings. In certain cases, a short list of plausible outputs can be enumerated without ever exhausting the beam, rendering beam search exact.
  
@article{Correia2019Aug,
	author = {Correia, Gon{\ifmmode\mbox{\c{c}}\else\c{c}\fi}alo M. and Niculae, Vlad and Martins, Andr{\ifmmode\acute{e}\else\'{e}\fi} F. T.},
	title = {{Adaptively Sparse Transformers}},
	journal = {arXiv},
	year = {2019},
	month = aug,
	eprint = {1909.00015},
	doi = {10.48550/arXiv.1909.00015}
}
  - We propose Adaptively Sparse Transformers wherein we simply replace softmax with α-entmax in the attention heads (see "Sparse Sequence-to-Sequence Models").
  - Comparing to "Sparse Sequence-to-Sequence Models", our novelty is that we also show how to calculate the derivative of α-entmax w.r.t. α, to make it learnable (the value α = 1.5 is a sensible starting point). This is motivated by the fact that different attention heads learn different sparsity behaviors.
  - Sparse attention Transformers tend to have slightly higher BLEU in NMT tasks, but their sparsity leads to a better potential for analysis.
  - The learning trajectories of α values as shown in fig. 2. These plots suggest that softmax-like behavior (α = 1) may be preferable while the model is still very uncertain. After around one thousand steps, some heads change direction and become sparser.
  - While the encoder self-attention learn to be sparse (fig. 4), the other two modules exhibit more uniform α distributions. This suggests that perhaps entirely sparse Transformers are suboptimal.
  - In the proposed adaptively sparse Transformer heads can learn to specialize more and with higher confidence (sec. 5.2). Some of the automatically-learned behaviors of our adaptively sparse Transformers – for instance, the near-deterministic positional heads or the subword joining head – may provide new ideas for designing static variations of the Transformer.
  
@article{Sukhbaatar2019May,
	author = {Sukhbaatar, Sainbayar and Grave, Edouard and Bojanowski, Piotr and Joulin, Armand},
	title = {{Adaptive Attention Span in Transformers}},
	journal = {arXiv},
	year = {2019},
	month = may,
	eprint = {1905.07799},
	doi = {10.48550/arXiv.1905.07799}
}
  - We propose Adaptive Attention Span: an alternative to the self-attention layer that learns its optimal context size to reduce computational and memory cost. For each head, we add a masking function to control for the span (fig. 2). This function has a learnable parameters with L1-penalty. The attention weights are then computed on the masked span.
  - As an extension, we consider Dynamic attention span where the span parameter is a function of the current input.
  - We validate our approach on the task of character level language modeling.
  - We observe that this leads to Transformer with small context in the low level layers and very large ones for the last layers. Even with a limit on span sets to 8192, the average span is only 314.
  - IMO, such a model should struggle with long context, since the examples with long and important context are either rare, or even abcent in the training set (the length extrapolation problem), so that the L1 penalty will dominate.
  
@article{Child2019Apr,
	author = {Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
	title = {{Generating Long Sequences with Sparse Transformers}},
	journal = {arXiv},
	year = {2019},
	month = apr,
	eprint = {1904.10509},
	doi = {10.48550/arXiv.1904.10509}
}
  - We stuty learned attention patterns from a 128-layer Transformer on CIFAR-10 trained with full attention (fig. 2). Many early layers in the network learn locally connected patterns, which resemble convolution. In some layers the network learned to split the attention across a row attention and column attention, effectively factorizing the global attention calculation. Several attention layers showed global, data-dependent access pattern. Higher layers exhibited high sparsity, with positions activating rarely and only for specific input patterns.
  - We propose Sparse Transformer with sparse attention patterns that have connectivity between all positions over several steps of attention. This still provides global context to any given position. Factorized self-attention has P separate attention heads. Given context size N, each head is attending to the amount of previous indices proportional to N^(1/P), and we want all input positions to be connected to all future output positions across the P steps of attention. This reduces the total effective computation to O(N^(1 + 1/P)).
  - We explore two factorizations for p = 2:
  - 1) A natural approach to defining a factorized attention pattern in two dimensions is to have one head attend to the previous L locations, and the other head attend to every L-th location, where L is the stride and chosen to be close to sqrt(N), a method we call strided attention (fig. 3b). This formulation is convenient if the data naturally has a structure that aligns with the stride, like images.
  - 2) For data without a periodic structure, we propose a fixed attention pattern, where specific cells summarize previous locations and propagate that information to all future cells (fig. 3c). All context is divided into blocks of length L (call it stide). The first head implements a filly-connected causal attention inside each stride. The next head allows an element to attent to the last C elements of each previous blocks. Concretely, if the stride is 128 and c = 8, then all future positions greater than 128 can attend to positions 120-128, all positions greater than 256 can attend to 248-256, and so forth. A fixed-attention pattern with C = 1 limits the expressivity of the network significantly. We found choosing C in {8, 16, 32} for typical values of L in {128, 256} to perform well. Additionally, we found that when using multiple heads, having them attend to distinct subblocks of length C within the block of size L was preferable to having them attend to the same subblock.
  - We defined two attention patterns, how to combine them in the model? The first approach is to use a single pattern per residual block, and interleave blocks with two different patterns. The second approach is to use the same pattern ijn each block, which is a union of both patterns. A third approach is to use multi-head attention, where attentions with different patterns are calculated in parallel, and the results are concatenated. We typically find this to work well.
  - Transformers are difficult to train with many layers. We propose a restructured residual block and weight initialization to improve training of very deep networks (fig. 4). We also recompute attention weights during the backwards pass to reduce memory usage.
  - For images, the Sparse Transformer demonstrates usage of long-term context and generates globally coherent samples (fig. 1).

@article{Al-Rfou2018Aug,
	author = {Al-Rfou, Rami and Choe, Dokook and Constant, Noah and Guo, Mandy and Jones, Llion},
	title = {{Character-Level Language Modeling with Deeper Self-Attention}},
	journal = {arXiv},
	year = {2018},
	month = aug,
	eprint = {1808.04444},
	doi = {10.48550/arXiv.1808.04444}
}
  - Currently character-level language modeling is dominated by RNN approaches.
  - We train a Transformer for autoregressive character-level language modeling. We add auxilary losses to speed up convergence.
  - 1) We predict next token at each time step (IMO is this not a standard??)
  - 2) We predict from the output of each intermediate layer. Lower layers are weighted to contribute less and less to the loss as training progresses. We drop all intermediate losses after half of the training is done.
  - 3) At each time step we make two (or more) predictions of future characters.
  - Since our network is deep, we hypothesize that the timing information (from positional embeddings) may get lost during the propagation through the layers. To address this, we add a learned positional embedding to the input sequence before each transformer layer (different for each layer). We are able to safely use learnable positional embeddings for our task, as we don’t require the model to generalize to longer contexts thanthose seen during training.


  @article{Choromanski2020Sep,
  author = {Choromanski, Krzysztof and Likhosherstov, Valerii and Dohan, David and Song, Xingyou and Gane, Andreea and Sarlos, Tamas and Hawkins, Peter and Davis, Jared and Mohiuddin, Afroz and Kaiser, Lukasz and Belanger, David and Colwell, Lucy and Weller, Adrian},
  title = {{Rethinking Attention with Performers}},
  journal = {arXiv},
  year = {2020},
  month = sep,
  eprint = {2009.14794},
  doi = {10.48550/arXiv.2009.14794}
}
  - Previous solutions to address the issue of quadratic self-attention complexity do not aim to approximate regular attention, but rather propose simpler and more tractable attention mechanisms (Reformer etc.), or by trading regular with sparse attention using more layers (Sparse Transformers). There is a lack of rigorous guarantees for the representation power produced by such methods.
  - We propose Performers with Fast Attention Via positive Orthogonal Random features (FAVOR+) mechanism, capable of estimation of regular attention, but of only linear space and time complexity and not relying on any priors such as sparsity or low-rankness.
  - We show that the attention matrix exp(QK^T) can be approximated up to any precision in time O(L d^2 log d), where L is the sequence length, and d is the model dimension. This is even faster w.r.t. L than Locality-Sensitive Hashing (LSH) techniques that have O(L d^2 log L) time complexity.
  - We propose to approximate the softmax kernel exp(q_i k_j) with a specific random projections kernel (sec. 2.3). There is however a caveat there. The softmax kernel produces non-negative scores, but the approximation may give potentially negative scores leading to unstable behaviours, especially when kernel scores close to 0 (which is the case for many entries of A corresponding to low relevance tokens) are approximated by estimators with large variance in such regions. Empirically this either completely prevents training or leads to sub-optimal models. We theoretically show that the variance of such an approximation is large as approximated values tend to 0. This is one of the main reasons why the robust random feature map mechanism for approximating regular softmax attention was never proposed. Instead of standard trigonometric random features, we propose a robust mechanism in this paper. The variance of our new unbiased positive random feature map estimator tends to 0 as approximated values tend to 0. To further reduce the variance of the estimator, we entangle different random samples ω1, ..., ωm to be exactly orthogonal (sec. 2.4). This leads to the first exponentially small bounds on large deviations probabilities. Our theoretical results are tightly aligned with experiments. We thus drastically improve accuracy of the approximation of the attention matrix.
  - We implemented our setup on top of pre-existing Transformer training code in Jax. A Performer replaces only the attention component with our method, while all other components are exactly the same as for the regular Transformer.
  - To further improve overall approximation of attention blocks during training, random samples should be periodically redrawn.
  - Even if the approximation of the attention mechanism is tight, small errors can easily propagate throughout multiple Transformer layers (e.g. MLPs, multiple heads). In other words, the model’s Lipschitz constant can easily scale up small attention approximation error.
  - We transferred the original pretrained Transformer’s weights into the Performer, which produces an initial non-zero 0.07 accuracy, but quickly recovers accuracy in a small fraction of the original number of gradient steps. Positive softmax with feature redrawing is necessary to match the Transformer. Ablation studies over many attention kernels show that trigonometric random features lead even to NaN values in training.
  - In principle, FAVOR+ can also be combined with other techniques, such as reversible layers or cluster-based attention.
  - IMO, the core of the method is the following (see also "Transformer Dissection: A Unified Understanding of Transformer's Attention via the Lens of Kernel"). As in eq. 1, regular self-attention can be written in form D^(-1) exp (QK^T / sqrt(d)) V, This form is based on the fact that we can split softmax into exp and L1 norm, and apply the L1 norm after all the operations. The first term D^(-1) is L1 norm, the QK^T are attention logits, and the last term are values. The exp (QK^T) is a matrix operation consisting of kernel operations k(q_i, k_j) = exp(q_i k_j), called the "softmax-kernel". It can be rewritten as k(q_i, k_j) = fi(q_i) fi(k_j), where fi is a projection to an inifinte-dimensional space - for now see "r" as "infinity". To understand this easier we can even drop fi for simplicity and consider a dot product kernel k(q_i, k_j) = q_i k_j. Instead of exp(Q K^T) W we now have only Q K^T W (fig. 1). For a dot product kernel, shapes of Q K^T W are (L,d)(d,L)(L,d) = (L,d). If we first calculate the first multiplication, shapes are (L,L)(L,d). If we first calculate the second multiplication, shapes are (L,d)(d,d) that is much smaller and faster if L >> d. Lets simpify it further and consider only one query and a lot of key-values. Then shapes are (1,d)(d,L)(L,d) = (1,d). Usually we first calculate the first multiplication, i.e. calculate a list of dot products between the query and the keys. But we can first calculate the second multiplication: it represents the fact the the operation Q K^T W is a linear projection of Q with the projection matrix K^T W of size (d,L)(L,d) = (d,d). So, for dot product kernel, for each q we do not need to calculate all the dot products k(q, k_j) over the sequence length j=1..L, but we only need to calculate dot products over d basis vectors. This cancels the L^2 complexity (however, may hurt representation power). In performer, instead of dot product kernel the authors propose to use fi(q_i) fi(k_j), where fi is some function from the function class in eq. 5. In this equation, the second part of the right side is a forming vector from scalars, obtained from random projections. To approximate the softmax kernel, the authors propose their FAVOR+ method.
  
@article{Tsai2019Aug,
	author = {Tsai, Yao-Hung Hubert and Bai, Shaojie and Yamada, Makoto and Morency, Louis-Philippe and Salakhutdinov, Ruslan},
	title = {{Transformer Dissection: A Unified Understanding of Transformer's Attention via the Lens of Kernel}},
	journal = {arXiv},
	year = {2019},
	month = aug,
	eprint = {1908.11775},
	doi = {10.48550/arXiv.1908.11775}
}
  - We present a new formulation for Transformer’s attention via the lens of kernel, which measures how similar two different inputs are.
  - A standard Transformer uses the scaled asymmetric exponential kernel (eq. 3). Asymmetric kernel form can be flexible and even non-valid (i.e., a kernel that is not symmetric and positive semi-definite).
  - As for integrating the positional information, three forms were proposed (eq. 4, 5, 6), and it was shown (in "Transformer-xl" paper) that eq. 5 words better than eq. 6, which in turn works better than eq. 4. We argue the reason is that if viewing features and positional embeddings as two distinct spaces, and thir direct sum may not be optimal, it's better to use a product of two kernels (eq. 5) to capture the similarities for both temporal and non-temporal components.
  - We present a new form of attention with a kernel (eq. 9) that is valid, i.e., symmetric and positive semi-definite (IMO seems like just a version of eq. 5 when Wq and Wk are the same). We see no much performance difference when comparing asymmetric to symmetric kernel, but it saves parameters.
  - We compare different kernel forms for the non-positional features. The linear kernel does not converge for both NMT (De-En) and SP (sequence prediction on WikiText-103). Kernel with infinite feature space (i.e., exponential and RBF kernel) outperforms the kernel with finite feature space (i.e., polynomial kernel). RBF kernel performs the best for NMT and exponential kernel performs the best for SP.
  - The need of the positional embedding (PE) in the attention mechanism is based on the argument that the attention mechanism is permutation equivariant. However, decoder self-attention is not permutation equivariant. Do we require PE in decoder self-attention? We present the results: for NMT, removing PE only in decoder self-attention results in slight performance drop, however, removing PE in the entire model greatly degrades the performance. On the other hand, for sequence prediction, removing PE from our proposed attention variant dramatically degrades the performance.
  - We find that there is no much performance difference by considering or not considering the positional embedding in value function in self-attention.
  
@article{Katharopoulos2020Jun,
	author = {Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, Fran{\ifmmode\mbox{\c{c}}\else\c{c}\fi}ois},
	title = {{Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention}},
	journal = {arXiv},
	year = {2020},
	month = jun,
	eprint = {2006.16236},
	doi = {10.48550/arXiv.2006.16236}
}
  - We propose linear transformer that scales linearly with respect to the context length. It combines the best of both worlds: when it comes to training, the computations can be parallelized; and when it comes to inference, the cost per time and memory for one prediction is constant.
  - Given a kernel sim(k, q) = fi(k)^T fi(q), we can recombine Q, K, V order of computation in attention, eq. 6 (IMO same as in Performers paper). It reduces time and memory complexity to O(N). We can simply store the fi(Kj) Vj^T matrix as an internal state and update it at every time step like a RNN.
  - We propose a feature map fi(x) = elu(x) + 1 (we prefer elu over relu to avoid setting the gradients to 0 when x is negative), because the feature function fi that corresponds to the exponential kernel is infinite dimensional. Finite dimensional polynomial kernel has been shown to work equally well with the exponential or RBF kernel. Also, when dealing with very long sequences (when N > D^2), polynomial kernel's is computationally favorable.
  - This shows that any transformer is a RNN (eq. 16-20), in theory even the ones using softmax attention. (IMO but the dimension of fi(.) is infinite).
  - We also linearize the causal masked attention in the same way. However, during training, storing all the output values (V) require a lot of memory in order to compute the gradients. We derive the gradients as cumulative sums. This allows us to compute both the forward and backward pass of causal linear attention in linear time and constant memory (algorithm 1).
  - On a sequence copying task with causal masking and max length 128, our linear transformer reaches the same loss as softmax. In contrast, Reformer with 4 hashing rounds (denoted as lsh-4) converges to a higher loss due to the noise introduced by hashing.
  - IMO, in the example with sequence copying, on larger sequence lengths a retular transformer should outperform linear transformer, since it does not require to compress all the context into a fixed size representation.
  - We train causally masked linear transformers to predict images pixel by pixel. Linear transformer achieved performance in terms of bits per dimension is on par with softmax attention while being able to generate images more than 1,000 times faster and with constant memory per image from the first to the last pixel.
  - In ASR with CTC loss, when training on the 80 hour WSJ dataset, linear transformer outperforms the RNN baseline and Reformer. A regular transformer achieves better metric, but is significantly slower.

@article{Shen2018Dec,
	author = {Shen, Zhuoran and Zhang, Mingyuan and Zhao, Haiyu and Yi, Shuai and Li, Hongsheng},
	title = {{Efficient Attention: Attention with Linear Complexities}},
	journal = {arXiv},
	year = {2018},
	month = dec,
	eprint = {1812.01243},
	doi = {10.48550/arXiv.1812.01243}
}
  - Dot-product attention involves two consecutive matrix multiplications: (Q K^T) V. Switching the order of multiplication to Q(K^T V) results in a substantially more efficient mechanism, which this paper names "efficient attention".
  - However, dot-product attention is only approximately equivalent to softmax normalization. We show that when the equivalence is approximate, it does not impact accuracies on four distinct tasks, object detection, instance segmentation, and stereo depth estimation.
  - This work is concurrent to "Transformers are RNNs".
  
@article{Wang2020Jun,
	author = {Wang, Sinong and Li, Belinda Z. and Khabsa, Madian and Fang, Han and Ma, Hao},
	title = {{Linformer: Self-Attention with Linear Complexity}},
	journal = {arXiv},
	year = {2020},
	month = jun,
	eprint = {2006.04768},
	doi = {10.48550/arXiv.2006.04768}
}
  - How to avoid a quadratic operation in self-attention?
  - It was shown (in the "BlockBERT" paper) that when each token attend to only a subset of tokens, this suffers from performance drop with limited efficiency gains, i.e., a 2% drop with only 20% speed up. As for Reformer, its efficiency gains only appear on sequences with length > 2048. Furthermore, the Reformer’s multi-round hashing approach actually increases the number of sequential operations, which further undermines their final efficiency gains.
  - We show both theoretically (theorem 1) and empirically that the context mapping matrix P = softmax(QK^T) formed by self-attention can be approximated by a low-rank matrix. Empirically (fig. 1), in RoBERTa on  Wiki103 and IMDB, SVD decomposition shows a clear long-tail spectrum distribution across each layer and head, and especially for higher layers (see fig. 1c). This implies that most of the information of matrix P can be recovered from the first few largest singular values.
  - Based on this, we propose a low-rank attention factorization. We project K and V matrices from (n, d) shape into (k, d) shape, when k << n, and then compute a regular attention. We theoretically estimate a possible error in ths case (theorem 2). For this we use two fixed-size matrices, and this requires a fixed n (max context size).
  - Experiments show that at k = 128 for n = 512 and k = 256 for n = 1024, Linformer’s performance is already nearly on par with the original Transformer. Also,we can use a single projection matrix E across all layers, for all heads, and for both key and value, without sacrificing performance.
  - Further, we hold k = 256 for n ∈ {512, 1024, 2048, 4096} (this requires 4 different training runs, as the projection matrices have different shape). As sequence length increases, even though our projected dimension is fixed, the final perplexities after convergence remain about the same.
  - This supports our assertion that the Linformer is linear-time. The performance of Linformer model is mainly determined by the projected dimension k instead of the ratio n/k.
  - One can also choose different kinds of low-dimensional projection methods instead of a simple linear projection. For example, one can choose mean/max pooling, or convolution where the kernel and stride is set to n/k. (IMO the latter resembles the Sparse Transformer).
  - IMO, while the results are interesting and useful, mathematically the O(n) statement is incorrect, becase O-notation describes the asymptotic behaviour in infinity, which means operations cannot require a limited context size. However, in practice we are not interested in the asymptotic becavour in infinity, instead we are interested in the efficiency for some large but finite n. So, the results can be seen as a statement that even for very long sequences we need only, say, k = 128, when a context size can be limited to, say, 10^6. However, the projection from 10^6 to k=128 have a lot of trainable weights that can be undertrained, if long sequences are rare. As the authors suggest, some regularization may be needed, for example mean/max pooling or convolutions instead of linear projections.
  
@article{Beltagy2020Apr,
	author = {Beltagy, Iz and Peters, Matthew E. and Cohan, Arman},
	title = {{Longformer: The Long-Document Transformer}},
	journal = {arXiv},
	year = {2020},
	month = apr,
	eprint = {2004.05150},
	doi = {10.48550/arXiv.2004.05150}
}
  - We propose a Longformer, in which the attention mechanism is a combination of a local and global attention. This falls within the general approach that defines some form of sparse attention pattern (like in Sparse Transformer).
  - For local-context self-attention we use a fixed-size sliding window (fig. 2b) which scales linearly with input sequence length. In a transformer with L layers, the receptive field is L times larger than the window size, given a fized window size for all layers.
  - The sliding window also can be "dilated" (fig. 2c). We found settings with different dilation configurations per head improves performance.
  - We add "global attention" on few pre-selected input locations (fig. 2d). It is symmetric: a token with a global attention attends to all tokens across the sequence, and all tokens in the sequence attend to it. The pattern is task specific. For classification, global attention is used for the CLS token while in QA global attention is provided on all question tokens (the number of such tokens is small relative to n and independent of n). The complexity of the combined local and global attention is still O(n).
  - We use different Q, K, V projections for local and global attention.
  - The dilated sliding window attention is not supported in existing DL libraries. We develop a custom CUDA kernel for it.
  - For autregressive language modeling we use small window sizes for the lower layers and increase window sizes as we move to higher layers. (IMO seems like they do not use a global attention pattern) During traning we increase the attention window size and sequence length across multiple training phases. This makes training fast, while keeping the slow part (longest sequences and window sizes) to the end. Longformer outperforms the comparable TransformerXL model, matches the performance of the comparable Sparse Transformer.
  - For masked language modeling we continue pretraining from the RoBERTa on a corpus of long documents, using a Longformer’s attention mechanism (our attention pattern can be plugged into any pretrained transformer model). We add extra position embeddings to support up to position 4,096.
  - For long document tasks (including QA, coreference resolution and classification) Longformer consistently outperforms the RoBERTa baseline that breaks the context into the longest possible segment, passes each individually through RoBERTa, and concatenates the activations for further processing. For QA tasks, we also concatenate the question to each segment. For HotpotQA Longformer places second on the published leaderboard, when the top model uses GNN of entities, which seem to encode an important inductive bias for the task.
  - We also propose Longformer-Encoder-Decoder (LED), using the Longformer local+global attention pattern in the encoder, and full self-attention in the decoder. We initialize LED parameters from the BART and extend position embedding to 16K tokens. On the arXiv summarization task LED achieves SOTA results, slightly outperforming BigBird.
  - This is a concurrent work with ETC, GMAT and BigBird.
  - IMO, it is important that for decoder-only Transformer choosing global tokens was not proposed, since it is not trivial how to.
  
@article{Zaheer2020Jul,
	author = {Zaheer, Manzil and Guruganesh, Guru and Dubey, Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and Ahmed, Amr},
	title = {{Big Bird: Transformers for Longer Sequences}},
	journal = {arXiv},
	year = {2020},
	month = jul,
	eprint = {2007.14062},
	doi = {10.48550/arXiv.2007.14062}
}
  - We propose a sparse attention where each query attends over R random number of keys. Thus we obtain a random graph where the shortest path between any two nodes is logarithmic in the number of nodes.
  - It was shown that in BERT's self-attention the neighboring inner-products are extremely important. Motivated by this, we combine random attention with a sliding window attention.
  - We found that random blocks and local window were insufficient to reach the performance of BERT. To improve performance we also make some tokens (such as CLS) global: tokens that attend to all tokens in the sequence and to whom all tokens attend to (fig. 1d).
  - We show that Transformer encoder with sparse attention is a Universal Approximator of sequence to sequence functions (this was also studied in a concurrent work "O(n) connections are expressive enough: Universal approximability of sparse transformers"). We further show that sparse encoder-decoder transformers are Turing Complete.
  - However, we show a natural task where any sufficiently sparse mechanism will require polynomially more layers: given an input set of unit vectors V = (v_1, ..., v_n), i-th output vector should be a vector from V that is the most distant from v_i.
  - This is a concurrent work with GMAT and Longformer. See also "ETC: Encoding Long and Structured Inputs in Transformers" from nearly the same authors.

@article{Ainslie2020Apr,
	author = {Ainslie, Joshua and Ontanon, Santiago and Alberti, Chris and Cvicek, Vaclav and Fisher, Zachary and Pham, Philip and Ravula, Anirudh and Sanghai, Sumit and Wang, Qifan and Yang, Li},
	title = {{ETC: Encoding Long and Structured Inputs in Transformers}},
	journal = {arXiv},
	year = {2020},
	month = apr,
	eprint = {2004.08483},
	doi = {10.48550/arXiv.2004.08483}
}
  - We present the Extended Transformer Construction (ETC) with key modifications to tackle long and structured inputs: relative position encoding, global-local attention, and a CPC pre-training task.
  - ETC receives two separate input sequences: the "long input" contains the input a standard Transformer would receive, while the "global input" contains a much smaller number of auxiliary tokens. Attention is then split into four separate pieces: global-to-global, global-to-long, long-to-global, and long-to-long. The latter is windowed, others are full (unrestricted). So, long input tokens can transfer information to each other through global input tokens. If the window radius is 1, and we have a single global token, we recover exactly the Star Transformer.
  - To handle long inputs in ETC we place the entire sequence of input tokens in the long input, and then assuming some sort of division into segments (e.g., sentences), place one auxiliary token in the global input per segment in the long input. We then use one relative position label to link the global segment tokens with the word piece tokens that belong to them, and a different label for those that do not. Seeing a Transformer as a graph neural network, the division between long and global input induces a natural structure with a 2-level hierarchy (we could in principle construct a 3-level hierarchy, or beyond).
  - For structured inputs (beyond sequential order) we can extend the vocabulary of relative position labels to label some edges such as is-a, part-of, or others.
  - In g2l, asymmetric hard masking in one direction can bring performance gains in some datasets (in fig. 3a different colors indicate different relative position labels).
  - We employ Contrastive Predictive Coding (CPC) pre-training task, together with MLM with full word masking. The goal of CPC is to predict subsequent inputs in latent space. For ETC, given an input sequence containing n sentences, we mask all the tokens corresponding to a subset of sentences, but leave the sentence summary tokens in the global input. We then train the model to minimize the difference between the hidden representation of the global sentence summary tokens for the masked sentences with respect to that of a global summary token that can see the unmasked sentence and nothing else.
  - The key differences from Longformer are more possibilities to encode structured inputs in a similar way as graph neural networks do, and global tokens in Longformer are never pre-trained with anything like our CPC loss, and thus their use must be learned during fine-tuning.
  - In out concurrent work "Big Bird: Transformers for Longer Sequences" we consider a slightly different model a random attention and study its expressive power.
  - This is a concurrent work with GMAT and Longformer.
  
@article{Gupta2020Jun,
	author = {Gupta, Ankit and Berant, Jonathan},
	title = {{GMAT: Global Memory Augmentation for Transformers}},
	journal = {arXiv},
	year = {2020},
	month = jun,
	eprint = {2006.03274},
	doi = {10.48550/arXiv.2006.03274}
}
  - Several previous works (BlockBERT, ReFormer) proposed a chunk-based attention sparsification. However, a position can require many layers to accumulate information from the entire input. We show that ReFormer struggles on our "majority tagging" combinatorial task that requires information from the entire sequence.
  - We propose Global Memory Augmentation for Transformers (GMAT). We prefix every input sequence with a list of M memory tokens. The sequence is processed using any sparse variant of attention, but attends to the M memory tokens using dense attention, and vice versa.
  - As a concrete sparcity pattern, we use a chunked self-attention, where blocks interact with each other only via the global memory. More complex sparsification schemes are possible. We use two positional embeddings: one for a chunk index and one for a token index inside a chunk. Memory tokens have a fixed position, and thus positional embeddings are used only for the main sequence.
  - After encoding an input sequence with N1 GMAT layers, we can discard the vectors corresponding to the main sequence, and keep only the global memory vectors, which are now a compressed representation of the entire input. We then use N2 more layers applied only on the M memory vectors resulting in richer representations. We then concatenate the memory vectors with positional embeddings and use N3 more layers to restore the original sequence, thus decompressing the information packed in the memory.
  - Using the decompressed representations leads to only a small performance degradation on MLM and reading comprehension.
  - This is a concurrent work with ETC, Big Bird and Longformer. Comparing to Longformer, the contribution of the global memory component is not evaluated, and global memory is designed on a task-by-task basis. Moreover, in Longformer attention scores for the memory and the main sequence are computed using separate sets of projection matrices, thereby introducing new parameters. (IMO it is not stated if this improves performance).
  
@article{Qiu2019Nov,
	author = {Qiu, Jiezhong and Ma, Hao and Levy, Omer and Yih, Scott Wen-tau and Wang, Sinong and Tang, Jie},
	title = {{Blockwise Self-Attention for Long Document Understanding}},
	journal = {arXiv},
	year = {2019},
	month = nov,
	eprint = {1911.02972},
	doi = {10.48550/arXiv.1911.02972}
}
  - We propose BlockBERT with sparse block attention: we split the input sequence into n blocks, make attention mask M consisting of n blocks (all tokens in each block attend to each other), and then make some permutation of the blocks inside the matrix (fig. 2). Permutations allow tokens within the same block attending to tokens in another block.
  - Different heads use different permutations, which are generated by shifting one position. With 12 attention heads and 2 blocks, we can assign 10 heads to permutation (1, 2) and the other 2 heads to permutation (2, 1). For 3 blocks, we assigns 8, 2 and 2 heads to permutation (1, 2, 3), (2, 3, 1), and (3, 1, 2), respectively.
  - Long sequence pre-training benefits long sequence fine-tuning.
  
@article{Lewis2019Oct,
	author = {Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
	title = {{BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension}},
	journal = {arXiv},
	year = {2019},
	month = oct,
	eprint = {1910.13461},
	doi = {10.48550/arXiv.1910.13461}
}
  - We present BART, a seq2seq denoising autoencoder. Text is corrupted with an arbitrary noising function, and a seq2seq model is learned to reconstruct the original text.
  - BART allows us to apply any type of document corruption, including changing its length. In the extreme case, where all information about the source is lost, BART is equivalent to a language model.
  - We compare several transformations, training for 1M steps on a combination of books and Wikipedia data (table 1). Performance of pre-training methods varies significantly across tasks. Token masking (or deletion, or self-attention masking) is crucial: pre-training objectives based on rotating documents or permuting sentences perform poorly in isolation. Bidirectional encoders are crucial for SQuAD (comparing to autoregressive LM pre-training). However, pure language models perform best on ELI5 (Long Form QA), suggesting that BART is less effective when the output is only loosely constrained by the input. On other tasks BART models using text-infilling perform well on all tasks. Sentence permutation only shows significant additive gains on the CNN/DM summarization dataset, however we hypothesise that larger pre-trained models may be better able to learn from this task.
  - We then trained BART using the same scale as the RoBERTa model on 160Gb of news, books, stories, and web text. We use a combination of text infilling and sentence permutation. We mask 30% of tokens in each document, and permute all sentences.
  - Overall, BART performs similarly to RoBERTa on discriminative SQuAD and GLUE tasks. This suggests that BART’s improvements on generation tasks do not come at the expense of classification performance.
  - We also experiment with several text generation tasks. BART is fine-tuned as a standard seq2seq model from the input to the output text. BART outperforms all existing work on CNN/DailyMail extractive summarization, XSum abstractive summarization, dialogue response generation on CONVAI2, ELI5 abstractive QA.
  - Fine-tuning BART recipes:
  - 1) For sequence classification tasks, the same input is fed into the encoder and decoder, and the final hidden state of the final decoder token is fed into new multi-class linear classifier. (IMO this is unusual and interesting approach) 
  - 2) For token classification tasks we feed the complete document into the encoder and decoder, and use the top hidden state of the decoder as a representation for each word.
  - 3) For sequence generation tasks the encoder input is the input sequence, and the decoder generates outputs autoregressively.
  - 4) For NMT, we replace BART’s encoder embedding layer with a new randomly initialized encoder. The model is trained end-to-end, which trains the new encoder to map foreign words into an input that BART can de-noise to English. The new encoder can use a separate vocabulary from the original BART model. n the first step, we freeze most of BART parameters and only update the randomly initialized source encoder, the BART positional embeddings, and the self-attention input projection matrix of BART’s encoder first layer. In the second step, we train all model parameters for a small number of iterations.
  
@article{Ye2019Nov,
	author = {Ye, Zihao and Guo, Qipeng and Gan, Quan and Qiu, Xipeng and Zhang, Zheng},
	title = {{BP-Transformer: Modelling Long-Range Context via Binary Partitioning}},
	journal = {arXiv},
	year = {2019},
	month = nov,
	eprint = {1911.04070},
	doi = {10.48550/arXiv.1911.04070}
}
  - We propose BP-Transformer (BPT for short) which introduces coarse-to-fine connections to approximate the reasonable inductive bias of language.
  - We partition a sequence into multi-granular spans via binary partitioning (BP). Each partition can be regarded as a node in GNN, and we divide the nodes into two types, token (leaf nodes) and span (non-leaf nodes). Given a span node, we additionally add a directed edge from each of its contained token nodes (fig. 3). These edges shorten the path between a span node and its corresponding token nodes. For a leaf node, we add the incoming edges from the different granularity. We use a hyper-parameter k to determine the connection density of the graph. We add k edges per level (see the longest edges in fig. 3).
  - We synchronously update representations of all nodes via Graph Self-Attention (GSA). Each node attends to its predecessors. The predecessors of a token node is the multi-scale spans it attending to, while the predecessors of a span node are all its contained token nodes.
  - We generalize the notion of relative position encoding from sequences to trees.
  - BPT improves the time/space complexity of Transformer models from O(d n^2) to O(d k n log n/k) in theory, such speedup cannot be achieved by tensor-based attention operators. To address this problem, we designed a set of CUDA kernels for sparse attentions.
  - On document level tasks, we achieved SOTA on language modeling, NMT and text classification.
  - IMO, seems like their partitioning method does not respect sentence boundaries, at least have not found anything about this in the work.
  
@article{Velickovic2017Oct,
	author = {Veli{\ifmmode\check{c}\else\v{c}\fi}kovi{\ifmmode\acute{c}\else\'{c}\fi}, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Li{\ifmmode\grave{o}\else\`{o}\fi}, Pietro and Bengio, Yoshua},
	title = {{Graph Attention Networks}},
	journal = {arXiv},
	year = {2017},
	month = oct,
	eprint = {1710.10903},
	doi = {10.48550/arXiv.1710.10903}
}
  - Many interesting tasks can be represented in the form of graphs: 3D meshes, social networks, telecommunication networks, biological networks or brain connectomes.
  - We introduce an attention-based architecture to perform node classification of graph-structured data. The idea is to compute the hidden representations of each node in the graph, by attending over its neighbors, following a multi-head self-attention strategy.
  - Our work can also be reformulated as a particular instance of MoNet.
  - We achieve or match SOTA on on four challenging graph benchmarks.
  - IMO the related work sections (1 and 2.2) are very informative.
  
@article{Guo2019Feb,
	author = {Guo, Qipeng and Qiu, Xipeng and Liu, Pengfei and Shao, Yunfan and Xue, Xiangyang and Zhang, Zheng},
	title = {{Star-Transformer}},
	journal = {arXiv},
	year = {2019},
	month = feb,
	eprint = {1902.09113},
	doi = {10.48550/arXiv.1902.09113}
}
  - We proposed a lightweight StarTransformer with a star-shaped attention pattern with one additional "global" token (fig. 1b) with O(n)computation cost w.r.t. input sequence length.
  - Local compositionality is a robust inductive bias for modeling the text sequence. However, the Transformer learns this bias from scratch. In contrast, Star-Transformer works for modestly sized datasets and does not rely on heavy pre-training.
  - We compare the standard Transformer with other models on one toy dataset and 21 real datasets, excluding the factor of unsupervised pre-training. Star-Transformer outperforms the standard Transformer and achieves comparable results with SOTA models.
  - Remains to be tested is whether one shared relay node is capable of capturing the long-range dependencies (IMO, the GMAT model is a denser vesion of this, with more shared nodes).

@article{Daras2020Oct,
	author = {Daras, Giannis and Kitaev, Nikita and Odena, Augustus and Dimakis, Alexandros G.},
	title = {{SMYRF: Efficient Attention using Asymmetric Clustering}},
	journal = {arXiv},
	year = {2020},
	month = oct,
	eprint = {2010.05315},
	doi = {10.48550/arXiv.2010.05315}
}
  - Despite the progress in sparse attention methods, new SOTA models still use the original dense attention layers because fast-attention mechanisms degrade the performance, while methods such as Longformer or Sparse Transformer require highly specialized GPU-kernels and which prevents usage in several hardware settings (e.g. TPUs), and also methods like Star Transformer prevents the use of causal masking.
  - We observe that in most real networks, the attention weights are sparse. For example in a pre-trained BigGAN on ImageNet, on average 98% of keys get weight less than 0.01 in softmax.
  - The matrix Q K^T is going to be of rank at most the dimension of the query and key vectors, therefore it is low rank (if seqnence length is larger than embedding dimension). However, it may become full rank after softmax. Our finding is that this does not happen in practice. Real attention matrices of pretrained models have a sharp decay in their singular values. In a pre-trained BigGAN model and most heads of BERT model, decay of singular values is exponential, which means that the matrix after softmax is effectively low rank. (IMO this is the same observation as in Linformer)
  - We formulate the assignment of keys and queries into clusters as an optimization problem. We want to compute attention only within each cluster. For fast execution on TPUs/GPUs we require that clusters are balanced: i.e. all clusters contain the same number of keys and queries. We are searching for the cluster assignment that approximates the dense attention matrix as well as possible, in Frobenius norm. We call this problem Attention Biclustering and show that it is NP-hard.
  - We need an efficient way to find, for any given query vector the set of keys with which it has big inner products. This problem, called Maximum Inner Product Search (MIPS), can be efficiently solved by transforming query and key vectors to convert it to a Nearest Neighbor Search as proposed in the pioneering Asymmetric LSH (Locality Sensitive Hashing) work.
  - We propose SMYRF algorithm for approximating self-attention, consisting of the following stages:
  - 1) We use a novel assymetric functions (eq. 4) to map all queries and keys to a (d+2)-dimensional ball where the Euclidean distance of the transformed vectors decreases linearly with the inner product of the original vectors.
  - 2) We then use a Locality Sensitive Hashing (LSH) function to map transformed vectors in real numbers, so that that vectors that are close in Euclidean distance correspond to numbers that are close on the real line.
  - 3) We sort vectors based on their LSH value and group them by adapting the thresholds to ensure balanced clusters.
  - 4) We perform dense attention within each cluster.
  - SMYRF is an excellent drop-in replacement for pre-trained dense attention it show significant memory benefits for relatively small performance drop, with no training at all. For sequence length 2048, SMYRF-BERT offers ≈ 20% speedup, while for 4096 speedup increases to ≈ 50%. So, SMYRF enables a new tradeoff in the design space.
  - We also can finetune pre-trained models with SMYRF. Finetuned SMYRF models, with 50% memory reduction, can outperform dense attention. Even with more aggressive memory-shrinking, up to 97%, SMYRF maintains a relatively good performance.
  - We also include experiments for networks trained from scratch: a non-pretrained model learns with randomly initialized, SMYRF layers. Initially, the random weights produce less sparsity. However, the model quickly learns to create sparse attention maps and learning under our framework is possible. Interestingly, SMYRF-BigGAN outperforms its dense counterpart FID by 3.95%.
  - The asymmetrical transformations of SMYRF largely outperform all the other LSH schemes, including Reformer. This is expected since by design SMYRF tries to form clusters that maximize the inner products between queries and keys, while E2LSH and Reformer try to minimize euclidean distance and angular distance respectively, which is not the best objective when dealing with queries and keys with different vector representations and arbitrary norms.

@article{Chen2021Oct,
	author = {Chen, Beidi and Dao, Tri and Winsor, Eric and Song, Zhao and Rudra, Atri and R{\ifmmode\acute{e}\else\'{e}\fi}, Christopher},
	title = {{Scatterbrain: Unifying Sparse and Low-rank Attention Approximation}},
	journal = {arXiv},
	year = {2021},
	month = oct,
	eprint = {2110.15343},
	doi = {10.48550/arXiv.2110.15343}
}
  - According to Long range arena and our experiments, low-rank-based attention might be less effective on hierarchically structured data or language modeling tasks, while sparse-based variants do not perform well on classification tasks.
  - In fig. 1 (more detailed in fig. 7) we present the approximation error of the attention matrices from a Transformer trained on (i) IMDb reviews classification, (ii) WikiText103, and (iii) from BigGAN-ImageNet. Sparse and low-rank approximation are complementary: sparse excels when the softmax has low entropy, and low-rank excels when the softmax has high entropy. Their errors are negatively correlated. An ideal combination of sparse and low-rank, obtained with robust PCA (denoted as orange), achieves lower error than both. (IMO these results are obvious: the more diffuse is attention, the less efficient is sparse approximation)
  - We describe a generative model of how the sparse + low-rank structure in attention matrices could arise when the elements of the input sequence form clusters. The model is paremetrized by the intra-cluster distance (fig. 2 shows different values) and the inverse softmax temperature β, so that unnormalized attention matrix M = exp(β Q Q^T). If β is small, the softmax distribution is diffuse, and we can approximate it with a low-rank matrix. In the middle regime of β, we need the sparse part to cover the intra-cluster attention and the low-rank part to approximate the inter-cluster attention.
  - How to decompose the attention matrices into sparse and low-rank components? The sparse + low-rank matrix structure has been well studied in statistics and signal processing since the late 2000s. Classical Robust PCA presents a polynomial algorithm to approximate such a structure. However, Robust PCA is orders of magnitude too slow and requires materializing the full attention, which defeats the main purpose of reducing compute and memory requirements. On the other side, straightforward addition of sparse (say, from Reformer) and low-rank (say, from Performer) attention will be inaccurate due to double counting.
  - To this end, we present a Scatterbrain algorithm. We first construct a low-rank approximation X, then construct a sparse matrix S such that S + X matches A on the support of S, then combine the results (sec. 4.2). So, our approximation is exact for entries on the support of S (which are likely to be large). On other entries our approximation matches the low-rank part.
  - Scatterbrain can use different kinds of low-rank (from Performer, Linear attention, or global tokens as in BigBird) and sparse (from Reformer, Longformer's local attention, or random block-sparse attention) approximation. As long as the low-rank component is unbiased (e.g., Performer), Scatterbrain retains the unbiasedness but with strictly lower variance.
  - The approximation by Scatterbrain is close to the Robust PCA oracle and up to 2.1X lower approximation error than SMYRF and Performer, while requiring up to 12X smaller memory than full attention. Even without training, Scatterbrain can reduce the attention memory of Vision Transformer by 98% at the cost of only 0.8% drop of accuracy.
  - When trained end-to-end, Scatterbrain outperforms baselines (sparse or low-rank attention) on a wide variety of benchmark tasks, including language modeling and classification. Scatterbrain achieves up to 5 points higher average accuracy on the Long-range Arena compared to Performer and Reformer. For language modeling tasks, sparse+low-rank has the smallest approximation error in most of the cases, and sparse has the largest error. It confirms the observation (from the Long range arena paper) that kernel or low-rank based approximations are less effective for hierarchical structured data.
  - As Scatterbrain has sparse attention as a component, it is not yet as hardware friendly (on GPUs and TPUs) as the low-rank component. This is the same limitation suffered by other sparse attention methods.
  
@article{Dehghani2018Jul,
	author = {Dehghani, Mostafa and Gouws, Stephan and Vinyals, Oriol and Uszkoreit, Jakob and Kaiser, {\L}ukasz},
	title = {{Universal Transformers}},
	journal = {arXiv},
	year = {2018},
	month = jul,
	eprint = {1807.03819},
	doi = {10.48550/arXiv.1807.03819}
}
  - In sequence processing systems, certain symbols are usually more ambiguous than others. It is therefore reasonable to allocate more processing resources to these more ambiguous symbols.
  - We introduce the Universal Transformer (UT), a generalization of the Transformer model with the added recurrence.
  - The UT contains encoder and decoder (fig. 2). In each recurrent time-step, the representation of every position is concurrently (in parallel) revised in two sub-steps. First, using a self-attention mechanism to exchange information across all positions in the sequence. Then, by applying a transition function (either a separable convolution or a FCN, depending on the task) independently at each position (they repeat the same self-attention + transition layer multiple times, fig. 1). We also add a dynamic per-position halting mechanism based on Adaptive Computation Time (ACT), when a model outputs a scalar halting probability at each step. This allows the model to choose the required number of refinement steps for each symbol dynamically (a per-symbol variable depth). Once the per-symbol recurrent block halts, its state is simply copied to the next step until all blocks halt, or we reach a maximum number of steps. The final output of the encoder is then the final layer of representations produced in this way.
  - IMO, a lot of things are not clear, need to read in more details, stupid tf.while_loop listings instead of explanations. What is the difference between state and previous state in the args of function from listing 2? Does the token keep updating with self-attention after halting, or not? Is UT without halting actually a regular transformer when a single layer is applied multiple times?
  
@article{Peng2021Mar,
	author = {Peng, Hao and Pappas, Nikolaos and Yogatama, Dani and Schwartz, Roy and Smith, Noah A. and Kong, Lingpeng},
	title = {{Random Feature Attention}},
	journal = {arXiv},
	year = {2021},
	month = mar,
	eprint = {2103.02143},
	doi = {10.48550/arXiv.2103.02143}
}
  - Many existing approaches to improve transformer asymptotic complexity are less well-suited for moderate length sequences: their additional computation steps can overshadow the time and memory they save.
  - We built on a previous works ("Random features for large-scale kernel machines", 2007; "Orthogonal random features", 2016), where it was shown that a certain stochastic function approximates a gaussian kernel (eq. 2, 3) which is connected to a softmax kernel with eq. 4.
  - We propose random feature attention (RFA) (see eq. 5) where fi is based on a random linear layer with sin and cos activations (eq. 2). While the linear layer is random, we multiply it by a learnable vector σ (eq. 8). Eq. 5 assumes that q and k are l2-normalized, but we find it has little impact on the performance when σ is set properly or learned from data.
  - Since attention is kernelized, it is now linear in sequence length. RFA can be used as a drop-in-replacement for softmax-attention and has a causal variant that allows for a recurrent computation (sec. 3.1). (IMO this looks the same as eq. 10, 11 in "Transformers are RNNs", and other kernel-based attention approximations).
  - The canonical softmax attention does not have any explicit modeling of distance or locality, so transformers heavily rely on positional encodings. In RFA, we can augment the recurrent equations with a learnable gating mechanism (eq. 7). By multiplying the learned scalar gates against the hidden state, history is exponentially decayed, favoring more recent context. This is a benefit of RFA: it would be otherwise more difficult to build similar techniques into the softmax attention, where there is no clear sense of "recurrence". There are interesting connections between gated RFA and fast weights which we leave to future work.
  - If we use ReLU activation instead of sin and cos (sec. 3.3), it approximates the order-1 arc-cosine kernel (instead of softmax kernel). In practice this achieves similar performance. This supplements the exploration of alternatives to softmax in attention.
  - If we use elu(x) + 1, as in "Transformers are RNNs", it significantly underperforms both the baseline and RFA, showing the importance of a properly-chosen feature map. Random feature approximation of attention is also explored by a concurrent work "Performers".
  - On language modeling, machine translation, and long text classification benchmarks, RFA achieves comparable performance to softmax attention. However, when we train with one attention but evaluate with the other, the performance is hardly better than randomly-initialized untrained models. Yet, an RFA model initialized from a pretrained softmax transformer achieves decent training loss after a moderate amount of finetuning steps.
  - RFA does not achieve any speedup when working with 512-length context, but achieves a 5.3x speedup with 4,000-length context.

@article{Qin2022Feb,
	author = {Qin, Zhen and Sun, Weixuan and Deng, Hui and Li, Dongxu and Wei, Yunshen and Lv, Baohong and Yan, Junjie and Kong, Lingpeng and Zhong, Yiran},
	title = {{cosFormer: Rethinking Softmax in Attention}},
	journal = {arXiv},
	year = {2022},
	month = feb,
	eprint = {2202.08791},
	doi = {10.48550/arXiv.2202.08791}
}
  - The Performer, RFA and Reformer show less satisfactory performance on the GLUE benchmark when compared with the vanilla architecture. Also Linformer (why?) and BigBird are not applicable to casual attentions. See also fig. 1 for the Long-Range Arena benchmark.
  - The key to the linear attentions is to find a decomposable similarity function S(q, k) = fi(Q) fi(K)^T that generalizes well to different tasks. Most existing linear transformers (RFA, Performer) are trying to find an unbiased estimation of the softmax attention. Another group of works (the linear transformer from "Transformers are RNNs") attempt to directly replace the softmax with a linear operation.
  - We compare 3 linear attentions with softmax attention (table 1). The superior results of ReLU over φI and LeakyReLU demonstrate the benefit of retaining non-negative values. Our conjecture is that it helps to avoid aggregating irrelevant contextual information. Also, since softmax is better than ReLU, we think that softmax normalization amplifies the correlated pairs, which might be useful to identify useful patterns. (IMO probably they mean amplification of large values with exponent) We also empirically find that the non-linear re-weighting mechanism in softmax attention can punish far-away connections and enforce locality in some cases. (IMO maybe because of the positional encoding dot products between near vectors become larger, and exponent further amplifies this)
  - We propose CosFormer with a linear attention. It discards entirely the softmax normalization while still features the non-negativity and re-weighting mechanism.
  - 1) We process K and V with ReLU to ensure a full positive attention matrix (since dot product of non-negative vectors is also non-negative). We then employ a row-wise normalization of the attention matrix.
  - 2) To introduce recency bias to the attention matrix we propose a cos-based re-weighting mechanism (eq. 10). That is, we multiply each attention weight by a scaled cos(i-j). It can be rewritten in another form (eq. 12) that still has a recurrent formulation.
  - In autoregressive language modeling, cosFormer outperforms a standard transformer with a clear margin in linear computation complexity and significantly outperforms RFA and a linear transformer (IMOit may remain true in a large scale, since a model may become able to use a distant context, when the cos scaling will hurt)
  - For bidirectional language modeling CosFormer converges faster than vanilla transformer with a comparable or smaller loss values, despite it only consumes linear space and time computation complexity. In addition, by ablations we show that re-weighting helps.
  - In fine-tuning CosFormer outperforms the RoBERTa baseline on 3/5 datasets, and achieves secondary place on the remaining 2/5 datasets. Longformer achieves better results on MNLI than CosFormer, but it is slower and requires more memory overhead. Other competing methods based on kernel functions have substantial performance gaps compared with our model.
  - On Long-range-arena, CosFormer achieves competitive results across all the tasks and achieves the best overall score, being one of the only two models that surpass vanilla transformer architecture (fig. 1). For the Pathfinder task, since the distance between the two points can be very far from each other, our introduced locality bias would have negative impact comparing to SOTA methods, despite that the performance gap between our method and the vanilla transformer is small.
  - IMO, sections 2.1, 2.2, 4 and fig. 2 are a good introduction to linear transformers.
  
@article{Zaremba2014Oct,
	author = {Zaremba, Wojciech and Sutskever, Ilya},
	title = {{Learning to Execute}},
	journal = {arXiv},
	year = {2014},
	month = oct,
	eprint = {1410.4615},
	doi = {10.48550/arXiv.1410.4615}
}
  - We show that seq2seq LSTM can accurately evaluate short simple Python programs by reading the program character-by-character and computing the output (fig. 1).
  - We consider programs that can be evaluated in linear time and constant memory. We allow the following operations: addition, subtraction, multiplication, variable assignments, if-statements, and for-loops, but we forbid double loops. We also limit multiplication since generic integer multiplication requires superlinear time. Every program ends with a single "print" statement whose output is an integer.
  - The characters are initially meaningless from the model’s perspective, and scrambling them has no effect on the model’s ability to solve this problem (fig. 2). It helps illustrate the difficulty faced by our neural network.
  - Initially, we found it difficult to train LSTMs to accurately evaluate programs. We think that the LSTM would learn faster if we first taught it about the individual operators and how to combine them by gradually increasing the "difficulty level" of the examples.
  - We apply a curriculum learning strategy of Bengio et al. 2009 when we gradually increase compexity (integer size and structural compexity). However, it gives even worse performance than baseline.
  - We design a novel curriculum procedure which outperforms both conventional training (no curriculum learning). In this procedure we use a balanced mixture of easy and difficult examples. This strategy always exposes the network at least to some difficult examples, in contrast to the naive curriculum strategy.
  - We also consider the addition task (just to add two integers) and the memorization task (replicating a random sequence of numbers of lengths ranging from 5 to 65). In these tasks our curriculum learning strategy also helps. Also, reversing and doubling the input sequence improves the performance.
  - The results suggest that a proper curriculum learning strategy is critical for achieving good performance on very hard problems. We hypothesize that the naive curriculum learning strategy fails because in memorization task the best way to accurately memorize a small amount of numbers could be to spread them over the entire hidden state / memory cell. This implies that the harder examples would require a restructuring of its memory patterns. Our strategy prevents the network from utilizing all the memory on the easy examples, thus eliminating the need to restructure its memory patterns.
  - We do not know how heavily our model relies on memorization and how far the learned algorithm is from the actual, correct algorithm. This could be tested by creating a big discrepancy between the training and test data, but in this work, the training and the test distributions are the same.
  
@article{Weston2014Oct,
	author = {Weston, Jason and Chopra, Sumit and Bordes, Antoine},
	title = {{Memory Networks}},
	journal = {arXiv},
	year = {2014},
	month = oct,
	eprint = {1410.3916},
	doi = {10.48550/arXiv.1410.3916}
}
  - RNN memory is typically too small, and is not compartmentalized enough to accurately remember facts from the past (knowledge is compressed into dense vectors). RNNs are known to have difficulty in performing memorization, for example the simple copying task (see "Learning to Execute" paper).
  - A memory network consists of a memory m (an array of objects, such as vectors or strings, indexed by m_i) and four (potentially learned) components. These components are general and can potentially use any existing ideas from the machine learning literature.
  - 1) I: input -> input features
  - 2) G: input features, memory cells -> updated memory cells
  - 3) O: input features + whole memory -> output features
  - 4) R: output features -> response format desired
  - The simplest form of G is to store I(x) in a "slot" in the memory: For efficiency at scale, G and O can operate on only a retrieved subset of  memory (on memories that are on the right topic). The O component is typically responsible for calculating what are the relevant memories to perform a good response. Our hypothesis is that without conditioning on memories, RNN will perform poorly.
  - In our basic architecture, the I module takes an input text, and G just keep adding new memory slots. The O module produces output features by finding k=2 supporting memories given x, by taking argmax of some similarity function. We train in a fully supervised setting, when the supporting sentences are also labeled in the training data (but not in the test data).
  - There are other recent related works: "Neural machine translation by jointly learning to align and translate" and "Generating sequences with recurrent neural networks". One can view these as particular variants of memory networks where in that case the memory only extends back a single sentence or character sequence. (IMO, this means that the authors focus on case when the memory is very large and is extracted from the train set, not from the context)

@article{Sukhbaatar2015,
	author = {Sukhbaatar, Sainbayar and Szlam, Arthur and Weston, Jason and Fergus, Rob},
	title = {{End-To-End Memory Networks}},
	journal = {Advances in Neural Information Processing Systems},
	volume = {28},
	year = {2015},
	url = {https://proceedings.neurips.cc/paper_files/paper/2015/hash/8fb21ee7a2207526da55a679f0332de2-Abstract.html}
}
  - Our previously proposed Memory Networks require supervision at each layer of the network. We propose the end-to-end version. It can also be seen as a version of RNNsearch from "Neural machine translation by jointly learning to align and translate" with multiple computational steps per output symbol.
  - A single layer of our model accepts several memory vectors M_i, the corresponding output vectors C_i (IMO looks like keys and values), and a query vector U. We then compute similarity P_i = softmax(U^T M_i) and calculate the weighted average with a skip connection O = Q + sum P_i C_i. Other recently proposed forms of memory/attention also take this approach.
  - For multple layers, we explore two types of weight tying within the model: (i) the output embedding for one layer is the input embedding for the one above, and (ii) the input and output embeddings are the same across different layers. Note that if we use the layer-wise weight tying scheme, our model can be cast as a traditional RNN where we divide the outputs of the RNN into internal and external outputs.
  - IMO, while the authors say that this model is recurrent (the last paragraph of sec. 2.2), this is only recurrent in the same sence as ALBERT is recurrent, since the number of steps is fixed.

@article{Rae2016,
	author = {Rae, Jack and Hunt, Jonathan J. and Danihelka, Ivo and Harley, Timothy and Senior, Andrew W. and Wayne, Gregory and Graves, Alex and Lillicrap, Timothy},
	title = {{Scaling Memory-Augmented Neural Networks with Sparse Reads and Writes}},
	journal = {Advances in Neural Information Processing Systems},
	volume = {29},
	year = {2016},
	url = {https://proceedings.neurips.cc/paper/2016/hash/3fab5890d8113d0b5a4178201dc842ad-Abstract.html}
}
  - In LSTM the number of parameters grows proportionally to the square of the size of the memory, making them unsuitable for problems requiring large amounts of long-term memory. Recently memory augmented neural networks (we will denote as MANNs) such as Neural Turing Machines and Memory Networks decoupled the memory capacity from the number of model parameters. Nonetheless, MANNs have had limited success in real world application. (IMO, in Memory Networks the memory actually IS the model parameters, since the memory is extracted from the whole train set)
  - We present SAM (sparse access memory). Comparing to Memory Networks, all writes to and reads from external memory are constrained to a sparse subset of the memory words using approximate nearest neighbor. Secondly, we introduce a sparse memory management scheme that tracks memory usage and finds unused blocks of memory for recording new information.
  - From "Large Memory Layers with Product Keys": their (SAM's) approach relies on an external indexing structure, which is approximate and needs to be relearned regularly while training the neural network to avoid a catastrophic drift.
  
@article{Lample2019Jul,
	author = {Lample, Guillaume and Sablayrolles, Alexandre and Ranzato, Marc'Aurelio and Denoyer, Ludovic and J{\ifmmode\acute{e}\else\'{e}\fi}gou, Herv{\ifmmode\acute{e}\else\'{e}\fi}},
	title = {{Large Memory Layers with Product Keys}},
	journal = {arXiv},
	year = {2019},
	month = jul,
	eprint = {1907.05242},
	doi = {10.48550/arXiv.1907.05242}
}
  - We propose a key-value memory layer  that can scale to very large sizes while keeping exact search on the key space. This is similar to self-attention, but the keys and values do not correspond to input tokens but are free embedding vectors, and the number of values (memory size) is very large. We propose to replace FFN in Transformer with our layer.
  - We do not build an approximate index, but rather we exploit the idea to represent a large set of key vectors by a drastically smaller number of vectors. We define keys as the concatenation of two sub-keys, in the spirit of product quantization.
  - 1) The query network maps the input x to a query q(x).
  - 2) Let we have two vector codebooks C and C' (two lists of keys) of dimension d_q/2. We consider their cartesian product C x C' by concatenating every element from C with every element from C', obtaining |C|x|C'| "long keys" of dimension d_q (usually |C| == |C'|). Each "long key" is associated with a learnable value vector. We exploit this structure to compute the closest keys efficiently. First, we split the query q(x) into two sub-queries q1 and q2. Then we find top-k vectors I from C closest to q1 (by the inner product), and top-k vectors I' from C' closest to q2. After selecting top-k, we perform a softmax normalization of the selected k vectors. Now we have found k^2 keys from C x C' (fig. 2). It is guaranteed that top-k "long" keys from C x C' closest to the "long" query q(x) are inside our selected subset of k^2 keys, while performing only O(sqrt(N)) operations, where N is the number of "long leys" and values. Finally, we average the selected values with their scores.
  - We make the model more expressive with a multi-head mechanism, where each head independently computes a query used to select k keys from the memory. In practice, for the same input we observe very little overlap between the keys selected by two different heads. This increases key usage and generally improves performance.
  - All the parameters of the memory are trainable, yet only selected memory slots are updated for each input. The sparse selection and parameter update make both training and inference very efficient.
  - Key-value memory layer provides important gains on large-scale language modeling, reaching with 12 layers the performance of a 24-layer BERT-large model with half the running time.
  - We find that adding a batch normalization layer on the top of the query network helps increasing key coverage during training.

@article{Shazeer2017Jan,
	author = {Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
	title = {{Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer}},
	journal = {arXiv},
	year = {2017},
	month = jan,
	eprint = {1701.06538},
	doi = {10.48550/arXiv.1701.06538}
}
  - Mixture-of-experts approach was introduced more than two decades ago. To increase the network capacity, various forms of conditional computation have been proposed, when large parts of a network are active or inactive on a per-example basis. The gating decisions may be binary or sparse and continuous, stochastic or deterministic. However, no work to date has yet demonstrated massive improvements in model capacity, training time, or model quality, since GPUs are much faster at arithmetic than at branching etc. (several reasons are listed but IMO not clearly understandable)
  - We propose a Sparsely-Gated Mixture-of-Experts Layer (MoE). It consists of a set of n "expert networks" E_i, ..., E_n (usually FFN), and a "gating network" G whose output is a sparse n-dimensional vector. The output y of the MoE module can be written as sum_i G(x)_i E_i(x). We save computation based on the sparsity of G(x). We use G in form of a linear layer with softmax, equipped by learnable gaussian noise and selecting only top-k (eq. 3).
  - From "Switch transformers" paper: the MoE authors conjectured that routing to k > 1 experts was necessary in order to have non-trivial gradients to the routing functions. The authors intuited that learning to route would not work without the ability to compare at least two experts.
  - To perform load balancing we define two additional losses. Let I(X) = sum_x G(x) be an vector of inportance values for all experts, calculated on a dataset. Firstly, we penalize the square of the coefficient of variation of this vector. This ensures equal importance. However, experts may still receive very different numbers of examples. For example, one expert may receive a few examples with large weights, and another may receive many examples with small weights. To solve this problem, we introduce a second loss function. Since gaussian noise in G introduces stochastity, we can calculate the probability of each element of G(x) to be non-zero, given x (eq. 8-9). An additional loss based on this value ensures balanced loads.
  - If the number of experts is very large, we could use hierarchical MoE, a MoE of MoE-s.
  - We apply a MoE between stacked LSTM layers (fig. 1). The MoE is called once for each position in the text, selecting a potentially different combination of experts at each position.
  - We train on 1-Billion-Word Language-Modeling Benchmark with a vocabulary of 793,471 words. We trained models with 4, 32, and 256 experts, and models with hierarchical MoEs containing 256, 1024, and 4096 experts (all with roughly equal computational costs). The largest of the models (4096 experts) achieved an impressive 24% lower perplexity on the test set.
  - We also train on a 100 billion word corpus. Even at 65536 experts (99.994% layer sparsity), computational efficiency for the model stays fine. After 131072 experts (137 billion parameters) the model performance starts to degrade (fig. 3), possibly a result of too much sparsity.
  - We also experiment with NMT and obtain gains over the strong baselines.

@article{Fedus2021Jan,
	author = {Fedus, William and Zoph, Barret and Shazeer, Noam},
	title = {{Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity}},
	journal = {arXiv},
	year = {2021},
	month = jan,
	eprint = {2101.03961},
	doi = {10.48550/arXiv.2101.03961}
}
  - Widespread adoption of MoE is hindered by complexity, communication costs, and training instabilities.
  - In this work a guiding design principle is to maximize the parameter count. "Scaling laws for neural language models" uncovered powerlaw scaling with model size, data set size and computational budget, and we investigate a fourth axis: increase the parameter count while keeping the FLOPs per example constant. Is it a separately important axis on which to scale?
  - We propose The Switch Transformer, which simplifies and improves over Mixture of Experts. We replace the dense FFN layer with a sparse Switch FFN layer. Contrary to MoE, we route to only a single expert: this k=1 routing strategy is later referred to as a Switch layer (the gate value still permits differentiability of the router).
  - Experts are splitted across devices. Each token is routed to the expert with the highest router probability, but each expert has a fixed batch size of (total tokens / num experts) × capacity factor (we design our model with TPUs in mind, which require statically declared sizes). If the tokens are unevenly dispatched then certain experts will overflow (red lines in fig. 3), resulting in these tokens not being processed by this layer, and the token representation is passed directly to the next layer through the residual connection. Higher expert capacity helps to mitigate token overflow, and empirically we find ensuring lower rates of dropped tokens are important for the scaling of sparse expert-models.
  - We simplify a load balancing comparing to MoE. The loss is the dot-product between (i) the fractions of tokens dispatched to experts and (ii) the fractions of the router probability allocated for experts, averaged by a batch (eq. 4-6). We desire both vectors to have values of 1/N, and the described loss is minimized under a uniform distribution. It can be differentiated as the probabilities are differentiable.
  - We consider several approaches to encourage exploration across experts. Multiplicative jitter noise on the incoming representation is slightly better than argmax baseline, while input dropout or sampling from the softmax distribution is worse than baseline (table 11).
  - Switch transformer outperform dense models on the speed-accuracy Pareto curve. When training T5 architecture on C4 corpus with MLM task, across different models sizes, it outperforms dense models per step and on wall clock time.
  - To compare Switch Transformer and MoE, the first has a smaller computational footprint (because of k=1), and for a fixed amount of computation and wall-clock time, it achieves better result. If we increase its size to match the training speed of the MoE Transformer, it outperforms MoE on a per step basis as well.
  - Sparse expert models may introduce instability. For this reason, in Gshard float32 was used when training MoEs. We only cast the router input to float32 precision, and no expensive float32 tensors are broadcast through all-to-all communication operation.
  - We reduce the default Transformer initialization scale s = 1.0 by a factor of 10. This both improves quality and reduces the likelihood of destabilized training in our experiments.
  - Switch Transformers have a lot of parameters, which can lead to severe overfitting on smaller downstream tasks. Simply increasing the dropout across all layers leads to worse performance. We propose expert dropout: setting a smaller dropout rate (0.1) at non-expert layers and a much larger dropout rate (0.4) at expert layers leads to performance improvements on four smaller downstream tasks.
  - Scaling properties are the following. When keeping the FLOPS per token fixed, having more parameters (experts) speeds up training on a step basis (fig. 4), as well as on a wall-clock basis (fig. 5). Interestingly, sparse T5-base trains even faster than a larger dense T5-large (fig. 6). We confirmed that parameters, independent of the total FLOPs used, are a useful axis to scale neural LMs. The Switch Transformer is beneficial even with only a few computational cores.
  - As in "Scaling laws for neural language models", we find that larger models are also more sample efficient — learning more quickly for a fixed number of observed tokens.
  - Switch Transformer is also an effective architecture at small scales: training Switch Transformers with 2, 4, or 8 experts results in solid improvements over T5-Base dense baselines (fig. 12).
  - In fine-tuning, we observe significant improvements spanning both reasoning and knowledge-heavy tasks. This validates our architecture, not just as one that pre-trains well, but can translate quality improvements to downstream tasks via fine-tuning.
  - Deploying massive neural networks is inconvenient, so we study distilling large sparse models into small dense models. Initializing T5-Base with the non-expert weights from Switch-Base and using a loss from a mixture of teacher and ground-truth labels obtains the best performance, preserving 30% of the quality gains of the large sparse teacher (table 6).
  - We also distil Switch-Base fine-tuned on the SuperGLUE task, into a FLOP matched dense 223M T5-Base. Similarly, we are able to preserve 30% of the gains of the sparse model. Potentially we may examine the specific experts being used for fine-tuning tasks and extracting them to achieve better model compression.
  - We discuss data-, model-, and expert-parallelism in sec. 5. We design 395 billion parameters, 64 experts, 6.3T FLOPS model (Switch XXL) and 1.6 trillion parameters, 2048 experts, 890B FLOPS model (Switch-C). Switch-C model exhibits no training instability at all, but Switch XXL with nearly 10x larger FLOPs per sequence, is sometimes unstable.
  - Fig. 13 presents the correlation of the C4 pre-training loss and SuperGLUE (model’s reasoning) and TriviaQA (factual knowledge) score. We found different scaling relationships for dense and switch models: switch models seem to better translate pretraining loss gains into factual knowledge fine-tuning score, but further statistics would be necessary to confirm these observations.
  - Besides FFN, in Appendix A, we report quality improvement adding switch experts inside self-attention layers, where our layer replaces the weight matrices which produce Q, K, V (fig. 10). Though we find improvements, we also found these layers to be more unstable when using bfloat16 precision.

@misc{BibEntry2022Feb,
	title = {{The Bitter Lesson}},
	year = {2022},
	month = feb,
	note = {[Online; accessed 16. Dec. 2024]},
	url = {https://www.cs.utexas.edu/~eunsol/courses/data/bitter_lesson.pdf}
}
  - The biggest lesson from 70 years of AI research is that general methods that leverage computation are ultimately the most effective, and by a large margin.
  - Researchers seek to leverage their human knowledge of the domain. This always helps in the short term, and is personally satisfying to the researcher. But in the long run it plateaus and even inhibits further progress. Breakthrough progress eventually arrives by an opposing approach based on scaling computation by search and learning - the two methods that seem to scale arbitrarily. The human-knowledge approach tends to complicate methods in ways that make them less suited to taking advantage of general methods leveraging computation.
  - In game Go engine, initial efforts went into avoiding search by taking advantage of human knowledge, or of the special features of the game, but all those efforts proved irrelevant, or worse, once search was applied effectively at scale. We have to learn the bitter lesson that building in how we think we think (?) does not work in the long run.
  - In speech recognition, there was an early competition, sponsored by DARPA, in the 1970s. The statistical methods won out over the human-knowledge-based methods. Deep learning methods rely even less on human knowledge, and use even more computation, together with learning on huge training sets. Putting human knowledge in their systems proved ultimately counterproductive, and a colossal waste of researcher's time. The same in computer vision.
  - The second general point to be learned from the bitter lesson is that the actual contents of minds are tremendously, irredeemably complex; we should stop trying to find simple ways to think about the contents of minds, such as simple ways to think about space, objects, multiple agents, or symmetries. All these are part of the arbitrary, intrinsically-complex, outside world.
  - We should build in only the meta-methods that can find and capture arbitrary complexity. They can find good approximations, but the search for them should be by our methods, not by us. We want AI agents that can discover like we can, not which contain what we have discovered. Building in our discoveries only makes it harder to see how the discovering process can be done.

@article{Kaplan2020Jan,
	author = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
	title = {{Scaling Laws for Neural Language Models}},
	journal = {arXiv},
	year = {2020},
	month = jan,
	eprint = {2001.08361},
	doi = {10.48550/arXiv.2001.08361}
}
  - To study scaling laws we train decoder-only Transformer on WebText2. Context length is 1024 and batch size us 2^19 for most runs, but we also vary it. We also train LSTM models and Universal Transformers for comparison. We used Adam with learning rate schedule with a 3000 step linear warmup followed by a cosine decay to zero.
  - Transformer LM performance (on the cross-entropy loss) depends most strongly on scale: the number of model parameters N (excluding embeddings, otherwise the trend is somewhat obscured), the size of the dataset D, the amount of compute C used for training. Empirical performance has a power-law relationship with each individual factor when not bottlenecked by the other two (fig. 1, 11).
  - After an initial transient period, learning curves for all model sizes follow predictable power-laws whose parameters are roughly independent of the model size. This reflects the interplay of optimizer dynamics and the loss landscape. Since the fits are best late in training, when the loss may be approximately quadratic, the powerlaw should provide information about the spectrum of the Hessian of the loss. Its universality suggests that the Hessian eigenvalue density is roughly independent of model size.
  - Larger models are more sample efficient: they require fewer samples and optimization steps to reach the same performance (fig. 2). The early-stopped test loss varies predictably with the dataset size and model size (fig. 4a). However, convergence is inefficient: we attain optimal performance by training very large models and stopping significantly short of convergence (fig. 2b).
  - The critical batch size, which determines the speed/efficiency tradeoff for data parallelism, also roughly obeys a power law in L, when the loss L is a function of N, D, C. The results in fig. 1 involved training at a fixed batch size B, whereas we could train more efficiently by training at the critical batch size.
  - For optimal performance all three factors N, D, C must be scaled up in tandem. As more compute becomes available, most of the increase should go towards increased model size (fig. 3). Optimal model size increases by 5x for each 10x increase in compute (fig. 14a). The optimal number of data grows relatively modestly by only 2x. The optimal number of optimization steps grows very slowly, if at all (fig. 14b), meaning that most of the growth in data examples processed can be used for increased batch sizes. Models between 0.6x and 2.2x the optimal size can be trained with a 20% larger compute budget (fig. 12a).
  - Universality of overfitting: performance improves predictably as long as we scale up N and D in tandem, but enters a regime of diminishing returns if either N or D is held fixed while the other increases. Every time we increase the model size 8x, we need to increase the data by roughly 5x to avoid a penalty.
  - When testing on additional text data distributions (Books, Wikipedia etc.), loss depends almost exclusively on the in-distribution validation loss (fig. 8), and does not depend on the duration of training, proximity to convergence or model depth.
  - Results at convergence were largely independent of learning rate schedule.
  - Performance depends very mildly on model shape such as depth vs. width (fig. 5). The loss varies only a few percent. Deep residual models can function as ensembles of shallower models (see "Residual networks behave like ensembles of relatively shallow networks"), which could potentially explain this finding.
  - When varying non-embedding parameter count, Transformers asymptotically outperform LSTMs. When measuring per-token test loss, LSTMs perform as well as Transformers for tokens appearing early in the context, but cannot match the Transformer performance for later tokens (fig. 7).
  - Recurrent (universal) transformers from Dehghani et. al, 2018 perform slightly better when comparing models with equal parameter count due to reusing parameters, but slightly worse when comparing per FLOP (fig. 17).
  - Our trends must eventually level off, since natural language has non-zero entropy. Far beyond the model sizes we study empirically, we find a contradiction between our equations: the amount of data used by compute-efficient training grows slowly with the compute budget, the performance predicted by L(C_min) eventually hits a lower bound set by the L(D) power law (fig. 15). There is a maximum rate at which the dataset size can productively grow with compute (when we are only training for a single epoch, eq. 6.7), but it grows the dataset much more slowly than in eq. 6.6 (the optimal dataset size to keep overfitting under control). It appears to imply that compute-efficient training will eventually run into a problem with overfitting, even if the training process never re-uses any data! This point occurs nearly at 10^4 petaflop-days, 10^12 parameters, 10^12 tokens and loss 1.7 nats/token (though the numerical values are highly uncertain, varying by an order or magnitude). The most obvious interpretation is that our scaling laws break down at or before we reach this point. However, this may have a deeper meaning. If we cannot increase the model size even more without qualitatively different data requirements, perhaps this means that we have extracted all of the reliable information available in natural language data. We conjecture that this point provides an estimate of the point at which Transformer language models reach maximal performance.
  - This is not the first work which found power-law scalings between performance and dataset size. However, for example, "Deep learning scaling is predictable, empirically" (2017) found super-linear scaling of dataset size with model size, whereas we find a sub-linear scaling.

@article{Rosenfeld2019Sep,
	author = {Rosenfeld, Jonathan S. and Rosenfeld, Amir and Belinkov, Yonatan and Shavit, Nir},
	title = {{A Constructive Prediction of the Generalization Error Across Scales}},
	journal = {arXiv},
	year = {2019},
	month = sep,
	eprint = {1909.12673},
	doi = {10.48550/arXiv.1909.12673}
}
  - What is the functional relation betweeng generalization error and model and dataset sizes?
  - We use 6 datasets for image classification and 3 for language modeling.
  - For a given dataset size, scaling up the model results in an initial power-law decrease in test error, which then saturates to a level determined by the dataset size.
  - For a given model size, scaling up the dataset results in an initial power-law increase in performance, which then saturates to a level determined by the model size.

@article{Bahri2021Feb,
	author = {Bahri, Yasaman and Dyer, Ethan and Kaplan, Jared and Lee, Jaehoon and Sharma, Utkarsh},
	title = {{Explaining Neural Scaling Laws}},
	journal = {arXiv},
	year = {2021},
	month = feb,
	eprint = {2102.06701},
	doi = {10.1073/pnas.2311878121}
}
  - We are interested in how the average test loss L(D, P) depends on the dataset size D and the number of model parameters P.
  - We identify variance-limited and resolution-limited scaling behavior for both dataset and model size (for a total of four scaling regimes). They originate from different mechanisms.
  - The variance-limited scaling follows simply from the existence of a well-behaved infinite data or infinite width limit. We prove (theorem 1) that if we scale the dataset size D, the training loss will approach the population loss, where fluctuations scale as 1/D. Also recent work shows that wide networks have a smooth large P limit, when the predictions approach a limiting distribution equivalent to a linear model with random features (see NTK), and fluctuations scale as 1/width (or 1/√P), i.e. the loss will differ from its infinite width limit by a term proportional to 1/width.
  - The resolution-limited regime happens in overparameterized models with dataset size or underparameterized models with model size. For exmaple, if D ≫ P ≫ 1 (D is infinite, P grows), the model is underparametrized, and this regime can be viewed as the "ideal-world generalization error", where optimizers take steps on the population loss (see "The Deep Bootstrap Framework" paper). We prove (theorem 3) that if we construct a sufficiently smooth estimator for true target function F by interpolating among P randomly chosen points, then the loss will be bounded by O(P^(1/d)), and probably this is not only a bound, but an estimate. Same for P ≫ D ≫ 1 (P is infinite, D grows) in overparameterized model.
  - Linear predictors may serve as a test bed to study training dynamics. Furthermore, linear predictors constructed from random features provide an accurate description of DNNs in the large width limit (see NTK and "Wide neural networks of any depth evolve as linear models under gradient descent". We distil linear teacher with a lot of deep features into a student with much less count of deep featues. In this setting we demonstrate variance-limited scaling and resolution-limited scaling. both theoretically and empirically.
  - For CIFAR-100 superclassed to N classes, we find that the number of target classes does not have a visible effect on the scaling exponent (power law coefficient) in resolution-limited scaling. This suggests that DNNs may be largely learning properties of the input data manifold – akin to unsupervised learning – rather than significant task-specific structure.

@article{Wu2019Jan,
	author = {Wu, Felix and Fan, Angela and Baevski, Alexei and Dauphin, Yann N. and Auli, Michael},
	title = {{Pay Less Attention with Lightweight and Dynamic Convolutions}},
	journal = {arXiv},
	year = {2019},
	month = jan,
	eprint = {1901.10430},
	doi = {10.48550/arXiv.1901.10430}
}
  - We propose LightConv, that is a depthwise convolution: it works independently over every channel, that is, i-th output channel depends only on i-th input channel. The total number of parameters equals dk, where d is the count of input and ouptut channels, and k is the kernel_size. We also normalize convolutional weights with softmax across the temporal dimension. To further reduce weights, we select a hyperparameter H=16 and tie the parameters of every subsequent number of d/H channels.
  - For example, if d=1024 and k=7, a regular convolution has kd^2 = 7.3M weights, a depthwise convolution has kd = 7168 weights, and with weith sharing with H=16 it has only kH = 112 weights.
  - We also propose DynamicConv. It takes the same form as LightConv but uses a time-step dependent kernel, computed as a linear learnable function R^d -> R^(Hk). Our vast reduction in the number of convolution parameters is crucial to make this possible on current hardware memory requirements.
  - The weights of DynamicConv do not depend on the entire context, they are a function of the current time-step only. The computation scales linearly in the sequence length.
  - We apply GLU layer before LightConv and linear projection with d x d weight matrix after LightConv. We found DropConnect to be a good regularizer for the LightConv module: we drop every entry of the softmax-normalized weights probability p and divide it by 1 − p during training.
  - We describe a CUDA implementation of our method.
  - We experiment with an encoder-decoder architecture for seq2seq learning. In encoder and decoder we replace self-attention with either LightConv or DynamicConv. The cross-attention is the same as in regular transformer. To compare with transformer baseline, we increase the number of blocks to N=7 in LightConv and DynamicConv, since these layers use less parameters. Encoder blocks have kernel sizes 3, 7, 15, 31, 31, 31, 31.
  - LightConv can outperform a strong self-attention baseline on WMT’17 Chinese-English translation, IWSLT’14 German-English translation and CNNDailyMail summarization. This is surprising because the common belief is that content-based self-attention mechanisms are necessary to obtaining SOTA results in NLP applications. DynamicConv improve further and achieve a new state of the art on the test set of WMT’14 English-German.
  - Both LightConv and DynamicConv are 20% faster than self-attention.
  - IMO, if we take H=1, we average every k subsequent vectors with k weights, but this is different to windowed self-attention: the weights are not based on similarity, but are a function of the middle vector. H>1, together with input and output projections, can be seen as H=1 but multiple parallel calculations of (input proj + LightConv + output proj) and summing the results, when input proj projects from dimension d to dimension d/H.

@article{So2019Jan,
	author = {So, David R. and Liang, Chen and Le, Quoc V.},
	title = {{The Evolved Transformer}},
	journal = {arXiv},
	year = {2019},
	month = jan,
	eprint = {1901.11117},
	doi = {10.48550/arXiv.1901.11117}
}
  - We apply neural architecture search to the Transformer.
  - We use the same tournament selection algorithm as AmoebaNet-A and encourage the reader to view their in-depth description of the method. In Tournament selection, we first define a gene encoding that describes architecture, randomly sample an initial population, evaluate fitness (valiation loss on WMT 2014 En-De), produce subpopulations, perform mutations and so on.
  - Our encoding search space is inspired by the NASNet search space. We ensure that it can represent the Transformer, so that we could seed the initial population with it. A child model’s genetic encoding is expressed as: (left input, left normalization, left layer, left relative output dimension, left activation, right input, right normalization, right layer, right relative output dimension, right activation, combiner function) × 14 + (number of cells) × 2, with the first 6 blocks allocated to the encoder and the latter 8 allocated to the decoder.
  - To train a Transformer to peak performance on WMT’14 En-De requires ∼10 hours. In our preliminary experimentation we could not find a proxy task that gave adequate signal for how well each child model would perform on the full WMT’14 En-De task; we investigated using only a fraction of the data set and various forms of aggressive early stopping. To address this problem we formulated a method Progressive Dynamic Hurdles (PDH) which allows well performing models that are consistently performing well to train for more steps.
  - We obtain the Evolved Transformer (ET) (fig. 3) which demonstrates consistent improvement. The four most notable aspects are 1) wide depth-wise separable convolutions, 2) GLU, 3) branching structures and 4) swish activations. Both the ET encoder and decoder independently developed wide depth-wise separable convolutions in the lower layers. (IMO this resembles LightConv from "Pay Less Attention with Lightweight and Dynamic Convolutions").
  
@article{Wu2020Apr,
	author = {Wu, Zhanghao and Liu, Zhijian and Lin, Ji and Lin, Yujun and Han, Song},
	title = {{Lite Transformer with Long-Short Range Attention}},
	journal = {arXiv},
	year = {2020},
	month = apr,
	eprint = {2004.11886},
	doi = {10.48550/arXiv.2004.11886}
}
  - We focus on the efficient inference for mobile devices, where the total number of Mult-Adds is constrained below 500M. Mult-Adds are dominated by the FFN (given a small sequence length around 20-30 as usual in NMT).
  - We propose Lite Transformer with Long-Short Range Attention (LSRA). It introduces convolution in a parallel branch to capture local dependencies (fig. 3a). We split input into two parts (for two branches) along the channel dimension, which will be mixed by the following FFN layer, it reduces the overall computation by 2x. In convolutional branch we use LiteConv (see "Pay less attention with lightweight and dynamic convolutions"), consisting of linear layers and depth-wise convolution.
  - In LSRA, due to convolutional branch, attention is specialized for long-term relationships (fig. 3b, c).
  - We also flatten the FFN layer (IMO it is not clear what does this mean)
  - Lite Transformer outperform the basic transformer under the mobile settings (fig. 4) and outperforms Evolved Transformer. It indicates that AutoML is not a panacea.
  - Combined with pruning and quantization, Lite Transformer can achieve 18.2× model size compression.
  
@article{Dai2020Jun,
	author = {Dai, Zihang and Lai, Guokun and Yang, Yiming and Le, Quoc V.},
	title = {{Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing}},
	journal = {arXiv},
	year = {2020},
	month = jun,
	eprint = {2006.03236},
	doi = {10.48550/arXiv.2006.03236}
}
  - Intuitively, for many sequence-level NLP tasks such as text classification and ranking, the full-length sequence of hidden states may contain significant redundancy (information down to the token-level granularity). Linguistic prior encourages gradually merging nearby tokens (words) into larger semantic units (phrases), which leads to a shorter sequence of representations.
  - We propose Funnel-Transformer: between some transformer layers we insert strided mean pooling layers that reduce temporal dimension (fig. 1). Importantly, instead of directly feeding the pooled sequence into the first self-attention layer of the new block, we only use pooled sequence to construct the query vector (and the residual signal) of the self-attention, while the unpooled sequence serves that role of key and value vectors. We think that this makes compression operation more expressive. We also do not apply pooling to CLS token.
  - We also propose a "decoder" if the task requires token-level prediction. We employ a single up-sampling with a large expansion rate by repeating each hidden vector several times, and add the last-layer hidden states from the first block of the encoder (this forms a skip connection). In addition, we stack 2 more Transformer layers to achieve a better deep fusion of the low-level and high-level features.
  - For tasks like sequence classification, the decoder is discarded after pretraining and only the encoder is finetuned.
  - The capacity drop of a pooled layer could be well compensated by re-investing the saved FLOPs in stacking more layers or increasing the width of the model.
  - By ablation studies, we found that the performance of the mean and max pooling operation is similar. But they are significantly better than the idea of utilizing attention score (Top-Attn pooling).
  - The relative positional embeddings (from Transformer-XL) is key to the performance of Funnel-Transformer. We suspect that the pooling operation could destroy the positional information carried by the absolute position encoding. To achieve good result with Funnel-Transformer based on absolute positional embedding, one may inject the absolute positional embedding into each attention layer.
  - With comparable or even fewer FLOPs, Funnel-Transformer improves over the standard Transformer on text classification, language understanding, and reading comprehension.

@incollection{Goyal2020Nov,
	author = {Goyal, Saurabh and Choudhury, Anamitra Roy and Raje, Saurabh and Chakaravarthy, Venkatesan and Sabharwal, Yogish and Verma, Ashish},
	title = {{PoWER-BERT: Accelerating BERT Inference via Progressive Word-vector Elimination}},
	booktitle = {{International Conference on Machine Learning}},
	journal = {PMLR},
	pages = {3690--3699},
	year = {2020},
	month = nov,
	issn = {2640-3498},
	publisher = {PMLR},
	url = {https://proceedings.mlr.press/v119/goyal20a.html}
}
  - We study optimizing inference time for BERT.
  - We demonstrate that, due to the self-attention mechanism, there is diffusion of information: as the word-vectors pass through the transformer block pipeline, they start carrying similar information, resulting in redundancy. We demonstrate the phenomenon through cosine similarity measurements (fig. 2). Consequently, a significant fraction of the word-vectors can be eliminated.
  - We develop PoWER-BERT (Progressive Word-vector Elimination for inference time Reduction of BERT).
  - We identify a retention configuration: a monotonically decreasing sequence l_1, ..., l_12 that specifies the number of word-vectors l_j to retain at each transformer block j. For example, l = (80, 73, 70, 50, 50, 40, 33, 27, 20, 15, 13, 3).
  - Our intuition that the significance of a word-vector W can be estimated from the attention normalized scores (after softmax) imposed by W on the other word-vectors. For each W, we sum all the attention scores from it to other vectors. At inference time we retain the word-vectors with the topmost significance scores.
  - We also design soft-extract layer added in between the self-attention and FFN. It involves N learnable parameters (ranging from 0 to 1) per layer: they represent the extent to which the k-th sorted position in the current layer is retained. It would retain all the word-vectors, but multiplies each word-vector by the retention parameter corresponding to its sorted position. We also add L1 loss over the sum of the soft-extract layer parameters. Since the transformer blocks have varying influence on the classification output, we scale the loss over block index. We obtain the retention configuration from the learned parameters by taking non-zero learned parameters (since L1 loss often yields zero-values learned parameters).
  - Training a pre-trained PoWER-BERT involves 3 steps: (i) fine-tune on the dataset, (ii) insering soft-extract into the fine-tuned model and train to derive the retention configuration, (iii) fine-tune again (from the pre-trained checkpoint?) with fixed retention configuration found at the previous step.
  - Real-life examples of  word-vector elimination are shown in fig. 7.
  - PoWER-BERT achieves large reduction in inference time over BERT-base (fig. 6). We also observe that previously proposed Head-Prune method is not competitive: pruning a large fraction of the heads would obliterate the critical self-attention mechanism of BERT.
  - IMO, it is important not to use bias in K, V projections, so that multiplying vector by zero acts the same as removing it.
  - IMO, as for the importance of vectors, a vector may be important if it accumulates imformation from other vectors. In this case, the output attention weights can be small. The another possibility to select important vectors is to train a model to predict gradient norm of each vector by loss, and apply this prediction af inference time.

@article{Wang2017Nov,
	author = {Wang, Xiaolong and Girshick, Ross and Gupta, Abhinav and He, Kaiming},
	title = {{Non-local Neural Networks}},
	journal = {arXiv},
	year = {2017},
	month = nov,
	eprint = {1711.07971},
	doi = {10.48550/arXiv.1711.07971}
}
  - We present non-local operations for capturing long-range dependencies with DNNs.
  - A generic non-local operation computes the response at a position (in space, time, or spacetime) as a weighted sum of the features at all positions in the input feature maps: y_i = 1/C(x) sum_j f(x_i, x_j) g(x_j). It consists of the pairwise function f, the unary function g and the normalization factor C.
  - We only consider g in the form of a linear projection. (IMO such g is meaningless, since ot can be can be taken out of the formula and applied as 1x1 convolution after the non-local layer, so it is not related to the layer)
  - A natural choice of f is the Gaussian function f(x1, x2) = exp(x1^T x2), obtaining the self-attention module (up to the normalization coefficient). Euclidean distance as used in is also applicable.
  - Despite the relation to self-attention, we show that the attentional behavior (due to softmax) is not essential. To show this, we describe two alternative versions: dot product similarity f(x1, x2) = theta(x1)^T fi(x2), and even concatenation f(x1, x2) = ReLU(w [theta(x1), fi(x2)]), where W projects vector to a scalar (IMO, there are two linear ops in a row here, so this equals summing dot products (x_i, w1) and (x_j, w2), where w1 and w2 are learnable, following a ReLU).
  - Our nonlocal models are not sensitive to the choices of f and g, indicating that the generic non-local behavior is the main reason for the observed improvements. This indicate that the non-locality of the model, which is orthogonal to the ideas of attention, interaction or relation, is the key to empirical success.  Non-local modeling, a long-time crucial element of image processing, has been largely overlooked in recent NNs for CV.
  - IMO, non-locality may also promote shortcut learning

@article{Tay2020May,
	author = {Tay, Yi and Bahri, Dara and Metzler, Donald and Juan, Da-Cheng and Zhao, Zhe and Zheng, Che},
	title = {{Synthesizer: Rethinking Self-Attention in Transformer Models}},
	journal = {arXiv},
	year = {2020},
	month = may,
	eprint = {2005.00743},
	doi = {10.48550/arXiv.2005.00743}
}
  - We propose Synthesizer with Synthetic Attention, a new model that learns to synthesize the self-alignment matrix instead of manually computing pairwise dot products. We consider a fixed sequence length N: we define a maximum length and dynamically truncate to the actual length of each batch, in similar spirit to trainable positional encodings. We propose two variants (fig. 1 b, c):
  - 1) Dense Synthesizer. Let vectors have a dimension d. Each token predicts weights for each token in the input sequence with a two-layer FFN. (IMO, seems like there is incorrect notation in sec 3.1: should be B_{i,h,l} ∊ R^N, and then B_{h, l} ∊ R^(NxN)).
  - 2) Random Synthesizer. The attention weights are initialized to random values which can then either be trainable or kept fixed. We were not expecting this variation to work at all, but it turns out to be a strong baseline. This is a direct generalization of the recently proposed "Fixed Encoder Self-Attention Patterns in Transformer-Based Machine Translation".
  - The Dense Synthesizer adds d × N parameters, and the Random Synthesizer adds N × N parameters (in each layer and head?). We propose Factorized Dense Synthesizer based on tiling, and Factorized Random Synthesizer based on NxN matrix factorization to prevent overfitting.
  - On MLM on the C4 dataset and finetuning on SuperGLUE and GLUE benchmarks, random Synthesizers can outperform/match Lightweight Dynamic convolutions along with outperforming Transformers and Universal Transformers. On two encoding tasks, factorized random Synthesizers outperform low-rank Linformers.
  - For the LM task, Synthesizers are capable of learning a local window attention pattern.
  - Although, this work made it’s appearance first in May 2020, a year before the MLP-Mixer was proposed, we show that Random Synthesizers are a form of multi-headed MLP-Mixers, with one difference: we use a softmax normalization on the attention weights.

@article{Ramachandran2019,
	author = {Ramachandran, Prajit and Parmar, Niki and Vaswani, Ashish and Bello, Irwan and Levskaya, Anselm and Shlens, Jon},
	title = {{Stand-Alone Self-Attention in Vision Models}},
	journal = {Advances in Neural Information Processing Systems},
	volume = {32},
	year = {2019},
	url = {https://proceedings.neurips.cc/paper/2019/hash/3416a75f4cea9109507cacd8e2f2aefc-Abstract.html}
}
  - We develop a local self-attention layer to build a fully attentional vision model. We use a multi-head windowed convolution with k x k window (i. e. each vector aggregates information only from the k x k window around). While using small k, such as k = 3, has a large negative impact on performance, the improvements of using a larger k plateau around k = 11.
  - To encode positional information, we use 2D relative position embeddings. We calculate row and column offsets between each pair of vectors, and each offset is associted with a positional embedding (seems like they use the same embeddings for horizontal and vertical distance). We concatenate poth positional embeddings (for row offset and column offset). The attention logits are sum of Q^T K and Q^T R, where R is a matrix of the concatenated positional embeddings (eq. 3). Relative position encodings perform 2% better than absolute encodings.
  - Interestingly, removing the content-content Q^T K interactions and just using the content-relative Q^T R interactions drops the accuracy by only 0.5%. This suggests that future work may improve attention by exploring different parameterizations and usages of positional information.
  - IMO, an interesting question: what is the relation between Q^T R self-attention and convolution? Q^T R creates not context-dependent weights to average value-vectors around with depth-wise convolution with weights calculated from query; this works like DynConv from "Pay Less Attention with Lightweight and Dynamic Convolutions" with H=1.
  - A common ResNet bottleneck block consists of a 1x1 down-projection convolution, a 3x3 spatial convolution, and a 1x1 up-projection convolution, followed by a residual connection. In this block, we swaps the 3x3 spatial convolution with a self-attention layer. A 2x2 average pooling with stride 2 is applied whenever spatial downsampling is required.
  - The initial layers (stem) of ResNet is a 7x7 convolution with stride 2 followed by 3x3 max pooling with stride 2. Our experiments show that using self-attention in the stem underperforms compared to using the convolution stem of ResNet. To bridge the gap, we apply spatially-varying linear transformations (see sec. 3.2).
  - Our model outperforms the convolutional baseline for both image classification and object detection.

@article{Dosovitskiy2020Oct,
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	title = {{An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale}},
	journal = {arXiv},
	year = {2020},
	month = oct,
	eprint = {2010.11929},
	doi = {10.48550/arXiv.2010.11929}
}
  - In CV, some recent work replace the convolutions entirely with self-attention (see "Stand-alone self-attention in vision models" and "Axial-deeplab"). While theoretically efficient, they have not yet been scaled effectively on modern hardware accelerators due to the use of specialized attention patterns. Therefore, in large-scale image recognition, classic ResNet-like architectures are still SOTA.
  - We propose Vision Transformer (ViT): a standard Transformer applied directly to images, with the fewest possible modifications (fig. 1). We split an image into fixed-size patches, linearly embed each of them, add position embeddings, and feed the resulting sequence of vectors to a standard Transformer encoder. In order to perform classification, we use the standard approach of adding an extra learnable CLS to the sequence. We use standard learnable 1D position embeddings, since we have not observed significant performance gains from using more advanced 2D-aware position embeddings.
  - We train on image classification in supervised fashion. When trained on mid-sized datasets such as ImageNet without strong regularization, these models yield modest accuracies of a few percentage points below ResNets of comparable size. This may be expected: Transformers lack some of the inductive biases inherent to CNNs, such as translation equivariance and locality. However, if the models are trained on larger datasets (14M-300M images), ViT approaches or beats SOTA on multiple image recognition benchmarks.
  - It is often beneficial to fine-tune at higher resolution than pre-training. When feeding images of higher resolution, we keep the patch size the same, which results in a larger effective sequence length. In this case, the pre-trained position embeddings may no longer be meaningful. We therefore perform 2D interpolation of the pre-trained position embeddings, according to their location in the original image.
  - As an alternative to raw image patches, the input sequence for Transformer can be formed from feature maps of a CNN. Such hybrids slightly outperform ViT at small computational budgets, but the difference vanishes for larger models which is somewhat surprising.
  - Fig. 7a shows the top principal components of the the learned linear patch projection. We also visualize the mean attention distance across images, for all layers and heads (fig. 7c). This is analogous to receptive field size in CNNs. Some heads attend to most of the image already in the lowest layers, showing that the ability to integrate information globally is indeed used by the model. Globally, we find that the model attends to image regions that are semantically relevant for classification (fig. 6).

@article{Cordonnier2019Nov,
	author = {Cordonnier, Jean-Baptiste and Loukas, Andreas and Jaggi, Martin},
	title = {{On the Relationship between Self-Attention and Convolutional Layers}},
	journal = {arXiv},
	year = {2019},
	month = nov,
	eprint = {1911.03584},
	doi = {10.48550/arXiv.1911.03584}
}
  - We show that a single MHSA layer with H heads, head dimension D and output dimension D_out and a relative positional encoding of dimension Dp ≥ 3 can express any convolutional layer of kernel size sqrt(H) x sqrt(H) and min(D, D_out) output channels. The theorem is proven by selecting the parameters, such that each head attends only to a different relative shift.
  - In the relative positional encoding (from "Transformer-XL"), attention scores are a sum of 4 terms (eq. 8) that represent (i) Q_content-K_content, (ii) Q_content-K_pos, (iii) Q_pos-K_content and (iv) Q_pos-K_pos interactions accordingly. By setting W_q and W_k to zero, the authors zero out the first 3 terms, such that operations are no more dynamic: they depend only on positions, but not on content (features). The last term in relative positional encoding is a multiplication v W r_δ, where W is a trainable matrix, v is a trainable vector (for each head), and r_δ is a trainable vector for each spatial shift δ. The authors set W to identity, that results in attention scores A_{q,k} = v r_δ, where δ is a spatial shift between q and k. The authors select concrete values for v and r_δ (eq. 9) with only 3 dimensions, and refer to this as the "quadratic encoding". This gives the attention maps shown in fig. 1 (for each head).
  - We experiment with a fully attentional model consisting of six MHSA layers and compare to ResNet18 on CIFAR-10. In fig. 2, we compare 3 attention-based models:
  - 1) Standard attention performs the worse (grey line). 
  - 2) Training without content-based attention improves performance (violet line).
  - 3) Replacing learnable positional encodings with our proposed quadratic encoding (with learnable v parametrized as -α(1, -2_∆1, -2∆_2) and fixed r_δ) further improves performance (blue) and matches ResNet18 (green). The ResNet is faster to converge, but we cannot ascertain whether this corresponds to an inherent property of the architecture or an artifact of the adopted optimization procedures.
  - Fig. 3 shows centers of each attention head with learnable quadratic encoding. Our intuition that Self-Attention applied to images learns convolutional filters around the queried pixel is confirmed. In fig. 4, it can be seen that in the first few layers the heads tend to focus on local patterns.
  - Fig. 5 shows attention probabilities of each head at each layer (we zoomed on the 7x7 pixels around the query pixel) for learnable positional encodings without content-based attention.
  - Fig. 6 shows the attention probabilities, averaged over 100 test images, for a regular self-attention model. In practice, it was shown in "Attention augmented convolutional networks" that combining CNN and self-attention features outperforms each taken separately. Fig. 6 shows that such combination is learned when optimizing an unconstrained fully-attentional model.
  - We conclude that fully-attentional models seem to learn a generalization of CNNs where the kernel pattern is learned at the same time as the filters — similar to deformable convolutions. This suggests that localized convolution is the right inductive bias for the first few layers of an image classifying network.
  - From the remark after lemma 2, it seems like the exact realization of convolution requires setting alpha parameter in quadratic encoding to infinity, or at least to 46 to simulate infinity in float32 precision. IMO, this shows that convolution is only approximated, not exactly realized, since the attention patterns are not hard. This is in line with OpenReview discussion https://openreview.net/forum?id=HJlnC1rKPB&noteId=H1eGmNvsFr
  

@article{Tolstikhin2021Dec,
	author = {Tolstikhin, Ilya O. and Houlsby, Neil and Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Unterthiner, Thomas and Yung, Jessica and Steiner, Andreas and Keysers, Daniel and Uszkoreit, Jakob and Lucic, Mario and Dosovitskiy, Alexey},
	title = {{MLP-Mixer: An all-MLP Architecture for Vision}},
	journal = {Advances in Neural Information Processing Systems},
	volume = {34},
	pages = {24261--24272},
	year = {2021},
	month = dec,
	url = {https://proceedings.neurips.cc/paper/2021/hash/cba0a4ee5ccd02fda0fe3f9a3e7b89fe-Abstract.html}
}
  - We propose the MLP-Mixer for CV, that does not use convolutions or self-attention. Mixer relies only on matrix multiplication, reshapes, transpositions, and scalar nonlinearities.
  - Mixer accepts image of a fixed size and converts it into a sequence of linearly projected non-overlapping image patches (also referred to as tokens). This results in shape M = (n_patches, n_channels) (fig. 1). It contains channel-mixing MLPs and token-mixing MLPs. Each MLP has two layers with GELU intermediate activation. Also, Mixer uses skip-connections and layer normalization.
  - 1) The channel-mixing MLPs operate on each token independently: M_out[i, :] = MLP(M_in[i, :]), when we use the same parameters for each i. Tying the parameters of the channel-mixing MLPs is a natural choice — it provides positional invariance.
  - 2) The token-mixing MLP operate on each channel of all tokens: M_out[:, i] = MLP(M_in[:, i]). Therefore, the computational complexity is linear in the number of input patches, unlike ViT whose complexity is quadratic. We again use the same parameters for each i. Tying parameters across channels is much less common, but surprisingly, this choice does not affect the empirical performance, while leading to significant memory savings. Mixer does not use position embeddings because the token-mixing MLPs are sensitive to the order of the input tokens.
  - Fine-tuning at higher resolution has been shown to improve the transfer performance in CV. To do this, the token-mixing MLPs have to be adjusted to handle longer sequences. We experimented with several options. We assume that the image resolution is increased by an integer factor K, so the token-mixing MLP input, output size (and hidden size as well) is increased by a factor of K^2. To initialize the parameters, we split the input sequence into K^2 equal parts, and initialize the new MLP so that it processes all these parts independently in the same way as the pre-trained MLP.
  - In the extreme case, our architecture can be seen as a very special CNN, which uses 1×1 convolutions for channel mixing, and single-channel depth-wise convolutions of a full receptive field and parameter sharing for token mixing. However, the converse is not true as typical CNNs are not special cases of Mixer. Furthermore, a convolution is more complex than the plain matrix multiplication as it requires specialized implementation.
  - Despite its simplicity, Mixer attains competitive results. When pre-trained on large datasets (i.e., ∼100M images), it reaches near SOTA performance. When pre-trained on data of more modest scale (i.e., ∼1–10M images), coupled with modern regularization techniques (RandAugment, mixup, dropout, stochastic depth - this set of techniques was inspired by the timm library), Mixer also achieves strong performance. However, similar to ViT, it falls slightly short of specialized CNN architectures.
  - It appears that Mixer benefits from the growing dataset size even more than ViT. One could speculate and explain it with the difference in inductive biases: self-attention layers in ViT lead to certain properties of the learned functions that are less compatible with the true underlying distribution than those discovered with Mixer architecture.
  - It is commonly observed that the first layers of CNNs tend to learn Gabor-like detectors that act on pixels in local regions of the image. Fig. 5 shows hidden units of the first (left), second (center), and third (right) token-mixing MLPs of Mixer trained on JFT-300M (each unit in each MLP has 196 weights, one for each of the 14 × 14 incoming patches). Similar to CNNs, we observe many pairs of feature detectors with opposite phases.Deeper layers appear to have no clearly identifiable structure.
  - The work "How far can we go without convolution: Improving fully-connected networks" is closely related. It attains reasonable performance on CIFAR-10 using fully connected networks, heavy data augmentation, and pre-training with an auto-encoder.
  - IMO, token-mixing MLP learns to process each token position differently. However, in vision many decision rules are position-invariant. Do we need exponentially more data to compensate lack of inductive biases?

@incollection{Touvron2021Jul,
	author = {Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and Jegou, Herve},
	title = {{Training data-efficient image transformers {\&} distillation through attention}},
	booktitle = {{International Conference on Machine Learning}},
	journal = {PMLR},
	pages = {10347--10357},
	year = {2021},
	month = jul,
	issn = {2640-3498},
	publisher = {PMLR},
	url = {https://proceedings.mlr.press/v139/touvron21a}
}
  - Assume we have access to a strong image classifier as a teacher model. It could be a convnet, or a mixture of classifiers. How to learn a transformer by exploiting this teacher?
  - We propose Data-efficient image Transformers (DeiT⚗). We train a vision transformer on a single 8-GPU node in two to three days (53 hours of pre-training, and optionally 20 hours of fine-tuning) that is competitive with convnets, using Imagenet as the sole training set. Our two new models DeiT-S and DeiT-Ti have fewer parameters and can be seen as the counterpart of ResNet-50 and ResNet-18.
  - Our architecture design is identical to ViT.
  - Using a convnet teacher gives better performance than using a transformer. We use RegNetY-16GF with 84M parameters, that we trained with the same data and same data-augmentation as DeiT. After distillation, the distilled model (DeiT) outperforms its teacher in terms of the trade-off between accuracy and throughput.
  - 1) We show that hard-distillation (using only argmax from teacher) significantly outperforms the traditional soft-distillation with KL loss, while being parameter-free and conceptually simpler: the teacher prediction plays the same role as the true label. In our all experiments we use label smoothing with ε = 0.1 when we have access to the ground truth, but we do not smooth pseudo-labels provided by the teacher.
  - 2) We add a new token, the distillation token, to the initial embeddings. It plays the same role as the class token, except that it aims at reproducing the label estimated by the teacher (fig. 2). This allows to learn both from ground truth and pseudo-labels, and outperforms vanilla distillation by a significant margin. This improves the performance.
  - The learned class and distillation tokens converge towards different vectors: the average cosine similarity (cos) between these tokens equal to 0.06. The class and distillation embeddings computed at each layer gradually become more similar through the network, all the way through the last layer at which their similarity is 0.93. The embedding associated with the distillation token gives slightly better results than the class token.
  - We verified that our distillation token adds something to the model, compared to simply adding an additional class token associated with the same target label. Even if we initialize them randomly and independently, during training they converge towards the same vector (cos=0.999), and the output embedding are also quasi-identical. This does not bring anything to the classification performance.
  - At test time, we use the late fusion of these two separate heads, for which we add the softmax output by the two classifiers to make the prediction.

@article{Bapna2018Aug,
	author = {Bapna, Ankur and Chen, Mia Xu and Firat, Orhan and Cao, Yuan and Wu, Yonghui},
	title = {{Training Deeper Neural Machine Translation Models with Transparent Attention}},
	journal = {arXiv},
	year = {2018},
	month = aug,
	eprint = {1808.07561},
	doi = {10.48550/arXiv.1808.07561}
}
  - We train Transformer and RNMT+ on WMT’14 En→De dataset and notice that deeper Transformer encoders completely fail to train.
  - We keep track of the ratio r_t between gradient norm of the first layer and the gradient norm of the last layer, for each training step t, for two reasons:
  - 1) It indicates if training is suffering from exploding or vanishing gradients.
  - 2) When a network is properly trained the lowest layers usually converge quickly. We expect that, for a healthy training process, r_t is relatively large during the early stages of training when updates to lower layers are larger than upper layers. We observe this in most successful Transformer and RNMT+ training runs.
  - Shallow and deep RNMT+ models show the same r_t dynamics, but in Transformer deeper models show lower r_t (fig. 1).
  - Also, removing residual connections in encoder leads to disastrous results for the Transformer, but only to a slight degradation in RNMT+ with 6 layers (however, deeper versions of RNMT+ fail to train).
  - We propose Transparent attention for Transformer encoder-decoder and RNMT+. Let h_1, ..., h_N be the output feature map of i-th encoder layer, and h_0 be the encoder input embeddings. We let the decoder attend not only h_N, but all h_i. For each decoder layer, we add N + 1 additional learnable weights, normalize them with softmax, obtaining w_0, ..., w_N, and calculate the weighted sum h_0 w_0 + ... + h_N w_N. Then we apply a regular cross-attention to the result.
  - Transparent attention improves the Transformer performance, especially for deeper models. It allows to train encoders with up to 20 layers. The r_t dynamics now resembles what we expect to see. The weights for the top few layers remain comparable at convergence (fig. 4), suggesting that the observed gains in performance might also be partially associated with an ensembling effect of the encoder features.

@article{Britz2017Mar,
	author = {Britz, Denny and Goldie, Anna and Luong, Minh-Thang and Le, Quoc},
	title = {{Massive Exploration of Neural Machine Translation Architectures}},
	journal = {arXiv},
	year = {2017},
	month = mar,
	eprint = {1703.03906},
	doi = {10.48550/arXiv.1703.03906}
}
  - We present the first comprehensive analysis of architectural hyperparameters for NMT models consisting of bi-directional RNN encoder and RNN decoder with attention.
  - We train on WMT’15 English→German task consisting of 4.5M sentence pairs. To test for generality, we also ran a small number of experiments on English→French translation, and we found that the performance was highly correlated with that of English→German. We use a total of more than 250,000 GPU hours.
  - Our baseline is a 2-layer bidirectional encoder (1 layer in each direction), and a 2 layer decoder with a multiplicative attention. We use 512 embedding size, 512-unit GRU cells with dropout 0.2, Adam optimizer and 1e-4 lr without decay. In each of the experiments,only one hyperparameter is changed, and we perform additional experiments when we believe hyperparameter interactions are likely to occur.
  - Large 2048-dimensional embeddings yielded the overall best result, but only by a small margin. Even small 128-dimensional embeddings performed surprisingly well,while converging almost twice as quickly. We also did not observe overfitting with large embeddings.
  - LSTM cells consistently outperformed GRU cells. Vanilla RNN decoder is unable to learn nearly as well as the gated variant (GRU or LSTM). This suggests that the decoder indeed passes information in its own state throughout multiple time steps instead of relying solely on the attention mechanism and current input. It could also be the case that the gating mechanism is necessary to mask out irrelevant parts of the inputs.
  - We found no clear evidence that encoder depth beyond two layers is necessary, but found deeper models with residual connections to be significantly more likely to diverge during training. The best deep residual models achieved good results, but only one of four runs converged.
  - On the decoder side, deeper models outperformed shallower ones by a small margin. Without residual connections, it was impossible for us to train decoders with 8 or more layers. In decoder, dense residual connections (from each layer to each layer, as in DenseNet, but with summation rather than concantenation) consistently outperformed regular residual connections and converged much faster in terms of step count (fig. 2).
  - Bidirectional encoders generally outperform unidirectional encoders, but not by a large margin. The encoders with reversed source consistently outperform their non-reversed counterpart.
  - The parameterized additive attention (eq. 1) mechanism slightly but consistently outperformed the multiplicative one (eq. 2), with the attention dimensionality having little effect.
  - We experiment with using no attention mechanism by initializing the decoder state with the last encoder state (None-State), or concatenating the last decoder (IMO, maybe encoder?) state to each decoder input (None-Input). Both variants without attention perform poorly. Also, attention-based models exhibited significantly larger gradient updates to decoder (IMO, maybe encoder?) states throughout training. This suggests that the attention mechanism acts more like a "weighted skip connection" that optimizes gradient flow than like a "memory" that allows the encoder to access source states, as is commonly stated in the literature.
  - A well-tuned beam search is crucial to achieving good results. Similar to "Neural machine translation with reconstruction", we found that very large beams yield worse results and that there is a "sweet spot" of optimal beam width.
  
@article{Parikh2016Jun,
	author = {Parikh, Ankur P. and T{\ifmmode\ddot{a}\else\"{a}\fi}ckstr{\ifmmode\ddot{o}\else\"{o}\fi}m, Oscar and Das, Dipanjan and Uszkoreit, Jakob},
	title = {{A Decomposable Attention Model for Natural Language Inference}},
	journal = {arXiv},
	year = {2016},
	month = jun,
	eprint = {1606.01933},
	doi = {10.48550/arXiv.1606.01933}
}
  - We propose a simple neural architecture for NLI, based on attention and parallelizable across sentence length.
  - We consider a task of classifying sentence pairs. We use embedding layer to convert them into vector sequences a and b of lengths l_a and l_b. The core model consists of the following three components:
  - 1) Attend. Soft-align the elements of both sequences using dot product attention e_ij = F(a_i)^T F(b_j), where F is a FFN, obtaining an attention matrix of shape l_a x l_b (fig. 1, left). For each a_i, we normalize all attention weights using softmax over the second axis, and sum b_j with the normalized weights, giving vector β_i. Analogously, for each b_j, we normalize all attention weights using softmax over the first axis, and sum a_i with the normalized weights, giving vector α_i.
  - 2) Compare. For each i we concatenate a_i and β_i and process with FFN, obtaining v_{1,i}. Analogously, for each j we concatenate b_j and α_j and process with FFN, obtaining v_{2,i}.
  - 3) Aggregate. We sum v_{1,i} over i and sum v_{2,i} over i, obtaining v_1 and v_2. Then we concatenate them and feed the result through a final FFN classifier.
  - We can augment input word embeddings with intra-sentence attention to encode compositional relationships between words within each sentence. For the first sentence, we calculate attention score matrix f_ij = F_intra(a_i)^T F_intra(a_j), where F_intra is a FFN. Then we add distance-sensitive bias terms f_ij += d_{i-j}. Finally, for each i we softmax-normalize weights f_ij, and use them as weights to calculate weighted sum of a_j, obtaining a'_i. Bias terms provide a minimal amount of sequence information, while remaining parallelizable. Finally, we concatenate a_i and a'_i for each i. Same for b.
  - The approach outperforms considerably more complex neural methods. Results suggest that, at least for this task, pairwise comparisons are relatively more important than global sentence-level representations.
  - IMO, intra-sentence attention is actually the self-attention, but with attention weight biases instead of positional encodings. The Transformer paper and many subsequent papers cite this work. For both papers Jakob Uszkoreit is one of the authors, and Transformer paper says that Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. So, the transformer encoder building block, with few differences, was actually first described in this paper.

@article{Bradbury2016Nov,
	author = {Bradbury, James and Merity, Stephen and Xiong, Caiming and Socher, Richard},
	title = {{Quasi-Recurrent Neural Networks}},
	journal = {arXiv},
	year = {2016},
	month = nov,
	eprint = {1611.01576},
	doi = {10.48550/arXiv.1611.01576}
}
  - We present quasi-recurrent neural networks (QRNN). It allows for parallel computation across timestep, and allows the output to depend on the overall order of elements in the sequence. It consists of (fig. 1, right):
  - 1) Masked convolution: with filters of width k, output as time t depends only on input at times t-k+1, ..., t. This is implemented by padding the input to the left by the convolution’s filter size minus one (see also "Pixel recurrent neural networks"). We perform such a masked convolution 3 times with different weights to obtain 3 output feature maps, and process them with tanh, sigmoid and sigmoid accordingly (eq. 1), obtaining 3 outputs Z, F, O. Note that if the filter width is 2, these equations reduce to the LSTM-like. Larger filters effectively compute higher n-gram features at each timestep.
  - 2) We propose 3 variants of pooling: f-pooling, fo-pooling, and ifo-pooling (where f defontes forget gate, o denotes output gate and i denotes input gate, eq. 3, 4, 5). These operations act independently on each channel. F-pooling was earlier proposed as "dynamic average pooling" in "Strongly-Typed Recurrent Neural Networks". Although the recurrent parts of these functions must be calculated for each timestep in sequence, evaluating them over even long sequences requires a negligible amount of computation time, since they are simple and parallelizable along channel dimension.
  - Dropout’s lack efficacy when applied to recurrent connections. Variational inference–based dropout and zoneout were proposed instead. The first is not applicable in our case, because QRNNs lack recurrent weights. Zoneout stochastically chooses a new subset of channels to "zone out" at each timestep; for these channels the network copies states from one timestep to the next without modification. We extend zoneout to the QRNN architecture by modifying the pooling function to keep the previous pooling state for a stochastic subset of channels. We can implement this with a ordinary dropout (eq. 6), but  in this case it is important to remove automatic rescaling functionality from the implementation if it is present. We can also apply ordinary dropout between layers.
  - We found it helpful to use skip-connections between every QRNN layer, as in DenseNet.
  - We also design QRNN encoder–decoder architecture (fig. 2).
  - However, in the subsequent work "Convolutional Sequence to Sequence Learning" it was stated that QRNN did not improve over SOTA on large benchmark datasets.
  - IMO this is very interesting idea to combine heavy element-wise operations and fast recurrent operations to obtain recurrent but fast model. This seems to be orthogonal to approaches like RetNet or RWKV. However, ablation studies is needed: maybe, just a cumulative max pooling over time axis will be enough?

@article{Gehring2017May,
	author = {Gehring, Jonas and Auli, Michael and Grangier, David and Yarats, Denis and Dauphin, Yann N.},
	title = {{Convolutional Sequence to Sequence Learning}},
	journal = {arXiv},
	year = {2017},
	month = may,
	eprint = {1705.03122},
	doi = {10.48550/arXiv.1705.03122}
}
  - We propose ConvS2S, a seq2seq architecture based entirely on CNN with attention. CNN does not depend on the computations of the previous time step and therefore allow parallelization over every element in a sequence.
  - We start from embedding layer to convert the input sequence into a sequence of vectors. We also equip our model with a sense of order by using absolute positional embeddings (IMO, this is interesting because this work was done before or concurrently with transformers).
  - Each block contains a 1D convolution followed by a GLU non-linearity, with residual connection. For decoder, we use causal convolution. Each decoder layer computes a separate dot-product attention by using the current decoder layer output and the final encoder layer outputs.

@article{Chen2018Apr,
	author = {Chen, Mia Xu and Firat, Orhan and Bapna, Ankur and Johnson, Melvin and Macherey, Wolfgang and Foster, George and Jones, Llion and Parmar, Niki and Schuster, Mike and Chen, Zhifeng and Wu, Yonghui and Hughes, Macduff},
	title = {{The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation}},
	journal = {arXiv},
	year = {2018},
	month = apr,
	eprint = {1804.09849},
	doi = {10.48550/arXiv.1804.09849}
}
  - We study which techniques and methods contribute significantly to the success of ConvS2S and Transformer, and how these improvements are applicable to RNMT (encoder-decoder RNN-based model with attentiomn for NMT).
  - We train on the standard WMT’14 En→Fr and En→De datasets that comprise 36.3M and 4.5M sentence pairs, respectively.
  - We come up with RNMT+, an enhanced version of RNMT (fig. 1). The encoder network has 6 bidirectional LSTM layers. The decoder network has 8 unidirectional LSTM layers. After the first decoder layer, we apply multi-head cross-attention to tne ehcoder outputs, and feed the result to each of the subsequent decoder layers, via contenation with the previous layer output. It is then fed into the rest of the decoder layers as well as the softmax layer. Residual connections are added to the third layer and above for both the encoder and decoder. Inspired by the Transformer model, in LSTM we apply per-gate layer normalization, which greatly stabilizes training. We apply dropout to both embedding layers and each LSTM layer output. Attention dropout is also applied. We use uniform label smoothing with an uncertainty=0.1. We use the Adam optimizer with β1 = 0.9, β2 = 0.999. Learning rate (fig. 3) is constant until the decay start step, then exponentially decay until the decay end step, and keep it at 5x10^−5 after the decay ends. For the WMT’14 En→De task, as it is smaller, we apply L2 regularization to the weights with λ = 10^−5. We also abort a training step if the norm of the gradient exceeds four standard deviations of the moving average, which is usually an indication of an imminent gradient explosion.
  - RNMT+ is slightly better than the Transformer Big model in terms of its mean BLEU score, and both these models outperform GNMT and ConvS2S by about 2 BLEU points. RNMT+ also yields a much lower standard deviation, and hence we observed much less fluctuation in the training curve.
  - We evaluate the importance of four main techniques for both the RNMT+ and the Transformer Big models. We believe that these techniques are universally applicable across different model architectures, and should always be employed by NMT practitioners for best performance.
  - 1) Label Smoothing improves both models: +0.7 BLEU for RNMT+ and +0.2 BLEU for Transformer Big.
  - 2) Multiple attention heads result in +0.6 BLEU for RNMT+ and +0.9 BLEU for Transformer Big.
  - 3) Layer normalization: removing it results in unstable training runs for both models, especially when multi-head attention is used.
  - 4) Synchronous training (see "Revisiting distributed synchronous SGD"): removing it results in a significant quality drop for RMNT+, while for the Transformer Big model, it causes the model to become unstable. Synchronous training is only successful when coupled with LR warmup. For RNMT+, removing this warmup stage during synchronous training causes the model to become unstable.
  - We also tried to combine the encoder and decoder from different model families. ConvS2S encoder quality was not on par with the other models, and takes a significant amount of time to converge, so we focused only on Transformer encoder with RNMT+ decoder and RNMT+ encoder with Transform
  - We also study two mixing schemes in the encoder: Cascaded Encoder and Multi-Column Encoder (fig. 2). With RNMT+ decoder, both variants improves over the Transformer encoder. (IMO it is not clear why in table 6 RNMT+ encoder is better that Transformer encoder, while table 5 shows the opposite results; random seed matters? are the conclusions reliable?)

@article{Narang2021Feb,
	author = {Narang, Sharan and Chung, Hyung Won and Tay, Yi and Fedus, William and Fevry, Thibault and Matena, Michael and Malkan, Karishma and Fiedel, Noah and Shazeer, Noam and Lan, Zhenzhong and Zhou, Yanqi and Li, Wei and Ding, Nan and Marcus, Jake and Roberts, Adam and Raffel, Colin},
	title = {{Do Transformer Modifications Transfer Across Implementations and Applications?}},
	journal = {arXiv},
	year = {2021},
	month = feb,
	eprint = {2102.11972},
	doi = {10.48550/arXiv.2102.11972}
}
  - Our goal is to try to determine why most modifications proposed to the Transformer have not seen widespread adoption. Is the original Transformer near-perfect, or the modifications do not generalize across applications?
  - We reimplement and evaluate a wide variety of Transformer variants on a suite of tasks that Transformers are commonly applied to. We limit our study to the encoder-decoder architecture. We find that many Transformer modifications do not result in improved performance.
  - 1) We consider various activation functions: ReLU, GeLU, Swish, ELU, SeLU, GLU (sigmoid-GLU, ReGLU, GeGLU, LiGLU), sigmoid and softplus.
  - 2) We explored RMS norm (2019) as an alternative to Layer Norm, Rezero (2020) and Fixup (2019) initialization schemes, and combining Rezero with Layer Norm and RMS Norm.
  - 3) We explored the trade-offs between the width and depth of the FFN. In order to ensure fair comparison, we scale FFN dimension and the number of heads in order to keep the total number of parameters constant when changing the depth.
  - 4) We explore tying only encoder input and decoder input embeddings, tying only decoder input and output embeddings, and untying all the embeddings (2021). We also explored factorizing the embedding matrix into two smaller matrices (d_model, d_inner) x (d_inner, d_vocab) (2019) and Adaptive input embeddings (2019), where clusters of more frequent tokens have a larger embedding dimension.
  - 5) We explore sharing the parameters of the Transformer layers inspired by the ALBERT model (2020), together with factorized and, optionally, tied embeddings. We also experimented with applying the parameter sharing to the encoder and decoder separately.
  - 6) For the final probability distribution we experiment with Adaptive softmax (2017) and Mixture of Softmaxes (MoS, 2017).
  - 7) We experiment with Transparent attention (2018) that creates weighted residual connections along encoder depth to facilitate gradient flow, and additional attention variants.
  - 8) We try the Evolved Transformer (2019), factorized, dense, and random Synthesizer (2020), Funnel Transformer (2020), Lightweight and Dynamic convolution (2019), MoE Transformer (2018, 2020), Switch Transformer (2021), product key memory(2021), Universal Transformer (2018)
  - As our baseline model, we use the original Transformer with 12 layers in the encoder and decoder and 223 million parameters. We use Pre-LayerNorm (this change has been unanimously adopted by all current Transformer implementations) and relative attention with shared biases.
  - As tasks, we consider (i) "span corruption" MLM objective from T5 on the C4 dataset, after which we first compute the perplexity on a held-out set, and then fine-tune on SuperGLUE, XSum and WebQuestions; (ii) supervised training on the WMT’14 English to German translation task without any pre-training, where we report the BLEU score of the highest-scoring checkpoint on the validation set.
  - Each considered variant has approximately the same number of parameters or total operations as the baseline, with some reasonable exceptions (see sec. 3.1).
  - The results for all model variants are shown in table 1. The scores which outperform the vanilla Transformer are highlighted in boldface. Results:
  - 1) Several activation functions, especially SwiGLU and GeGLU improve performance over the ReLU activation in baseline.
  - 2) Replacing layer normalization with RMS normalization yields improvements while also improving training speed.
  - 3) Deeper models tend to outperform shallower ones with a fixed parameter count, but are more compute-intensive and therefore slower.
  - 4) Sharing of parameters across layers tends to hurt performance.
  - 5) Untying the encoder/decoder embeddings improve performance with only a modest increase in parameter count.
  - 6) Using mixture of softmaxes does improve performance but is almost 40% slower than the vanilla Transformer.
  - 7) Synthesizer improve performance comparing to the baseline, when dot product attention is additively combined with the synthetic attention, and a scalar (learnable?) weighting coefficient (denoted as "plus alpha" variant in table 1). This holds both for random attention pattern (trainable attention scores, as in MLP-mixer) and dense attention pattern (query-dependent attention scores).
  - 8) Switch Transformer, MoE, and product key memories all improve performance with significantly more parameters than the baseline model, but roughly the same FLOPs.
  - Overall, the modifications that led to significant improvements tended to fall into one of three buckets: relatively minor changes (i.e., activation functions, normalization and untying embedding matrices); those that increase parameter count (i.e., Switch Transformer, product key memory) or are slower (i.e., mixture of softmaxes, deeper models); or those that were originally invented in the Mesh TensorFlow codebase that we use for our experiments (i.e., mixture of experts, switch Transformer, synthesizer). Surprisingly, some other architecture variants generally performed poorly (Transparent attention, Dynamic convolution, Lightweight convolution, Universal Transformer, Funnel Transformer).
  - We present a case study of trying to improve one of the model variants by tuning its hyperparameters. We selected Universal Transformers (UT) because it was claimed to achieve better results than the vanilla Transformer, and the UT has a relatively large number of hyperparameters that we can adjust. We swept over 25 model configurations, but only 2 managed to outperform the initial results: reducing the number of recurrent steps (from 16 to 2) and slightly increasing the model size. We were ultimately unable to match the performance of the vanilla Transformer.
  - We investigate the correlation between perplexity and quality on each task. The performance on SuperGLUE (ρ = 0.87) and XSum (ρ = 0.80) seems to be highly correlated with the pre-training perplexity, whereas the performance on WebQuestions (ρ = 0.69) has a somewhat lower correlation.
  - There are various possible explanations why so few modifications produced a boost:
  - 1) The Mesh TensorFlow codebase and implementation are just so different than standard practice that most architectural modifications may not work.
  - 2) The tasks we consider are non-standard.
  - 3) Not tuning hyperparameters handicapped other methods. However, we argue that truly useful improvements to the Transformer should be reasonably hyperparameter-agnostic.
  - 4) Modifications to the Transfomer architecture often do not transfer across implementations and applications. We believe the final option is a plausible explanation.
  - Suggestions when proposing a new modification:
  - 1) Try it out in multiple completely disparate codebases.
  - 2) Apply it to a wide variety of downstream applications, including transfer learning, supervised learning, and language modeling – and, possibly, include domains beyond NLP too, e.g., computer vision.
  - 3) When evaluating performance in different implementations and on different tasks, keep hyperparameters fixed as much as possible, or at least attempt to measure the robustness of the modifications to changes in hyperparameters.
  - 4) Report mean and standard deviation across multiple trials, or at least avoid cherry-picking.
  
@article{Qin2022Oct,
	author = {Qin, Zhen and Han, XiaoDong and Sun, Weixuan and Li, Dongxu and Kong, Lingpeng and Barnes, Nick and Zhong, Yiran},
	title = {{The Devil in Linear Transformer}},
	journal = {arXiv},
	year = {2022},
	month = oct,
	eprint = {2210.10340},
	doi = {10.48550/arXiv.2210.10340}
}
  - Linear transformers (modifications that adopt kernel functions to decompose softmax attention) show degraded performance than the vanilla model. We identify two reasons for this:
  - 1) Unbounded gradients. In kernel-based linear attention, the partial derivative of attention probabilities by attention logits can be arbitrarily large in the absolute value, if attention logits are close to zero (eq. 11). In the softmax attention, in contrast, the derivatives are upped-bounded between -1/4 and 1/4. The unbounded gradients lead to less stable optimization and worse convergence results in our preliminary studies.
  - 2) Attention dilution. We introduce a metric called locally accumulated attention score. For given token position i, sequence length N and ratio r < 1 it is defined as sum of attention probabilities in a window of size rN centered in the position i. A higher score indicates the particular attention layer concentrates on the local neighbourhood, while a lower score tends to indicate the issue of attention dilution, where scores are distributed more evenly to local and distant tokens. Using this score, we provide evidence that the vanilla attention is more concentrated locally, while the linear transformer suffers from the issue of attention dilution (fig. 2a). It is a known property of vanilla attention to emphasize on neighbouring tokens.
  - We propose TransNormer, a new variant of linear transformer (fig. 3). It uses two types of attention: DiagAttention for the early stage of the model and NormAttention for the later stage.
  - 1) In DiagAttention, attention is only calculated inside the blocks to enforce neighbouring focus. It addresses the attention dilution issue. By properly reshaping the inputs, the diagonal attention can be efficiently computed in linear space-time.
  - 2) Directly removing the scaling operation in the linear transformers leads to critical performance drop since the attention map becomes unbounded in the forward pass. We propose NormAttention: we empirically find that we can apply an arbitrary normalization after applying attention weights to values: XNorm(Q(K^T V)), where the XNorm can be Layernorm or RMSNorm etc. (we use RMSNorm). The gradients of NormAttention are bounded.
  - We train our model for 50k iterations with RoBERTa architecture on the WikiText103. Performer and Linear Transformer (from "Transformers are RNNs") have substantially higher relative standard deviation of gradients compared to vanilla attention. The NormAttention produces more stable gradients.




  - TransNormer shows better performance than vanilla transformer on a wide range of tasks (fig. 1).